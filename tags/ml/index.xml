<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>ML on KiloBytes by KB</title><link>https://Kshitij-Banerjee.github.io/tags/ml/</link><description>Recent content in ML on KiloBytes by KB</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 18 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://Kshitij-Banerjee.github.io/tags/ml/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding GPT - A Journey from RNNs to Attention</title><link>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</guid><description>Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&amp;rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.</description></item><item><title>Loss Functions In ML</title><link>https://Kshitij-Banerjee.github.io/2023/02/17/loss-functions-in-ml/</link><pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/02/17/loss-functions-in-ml/</guid><description>Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on) All losses in keras defined here
Frequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.</description></item><item><title>Intro to ML</title><link>https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/</link><pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/</guid><description>Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.
I&amp;rsquo;ll update and make these more readable in the future (which possibly means never :D)
Lecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.
The notation “a := b” to denote an assignment operation</description></item></channel></rss>