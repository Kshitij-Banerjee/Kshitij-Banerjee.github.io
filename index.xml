<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>KiloBytes by KB</title><link>https://Kshitij-Banerjee.github.io/</link><description>Recent content on KiloBytes by KB</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 18 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://Kshitij-Banerjee.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding GPT history through papers</title><link>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-history-through-papers/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-history-through-papers/</guid><description>Introduction This post is inspired by Andrej Karpathy&amp;rsquo;s makemore series. Unfortunately, as of the time of this writing, he hasn&amp;rsquo;t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding.</description></item><item><title>Loss Functions In ML</title><link>https://Kshitij-Banerjee.github.io/2023/02/17/loss-functions-in-ml/</link><pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/02/17/loss-functions-in-ml/</guid><description>Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on) All losses in keras defined here
Frequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.</description></item><item><title>Intro to ML</title><link>https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/</link><pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/</guid><description>Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.
I&amp;rsquo;ll update and make these more readable in the future (which possibly means never :D)
Lecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.
The notation “a := b” to denote an assignment operation</description></item><item><title>Largest Area Under a Histogram (and related concepts/problems).</title><link>https://Kshitij-Banerjee.github.io/2017/01/30/largest-area-under-a-histogram-and-related-concepts/problems./</link><pubDate>Mon, 30 Jan 2017 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2017/01/30/largest-area-under-a-histogram-and-related-concepts/problems./</guid><description>A problem with a clever solution, with some insights to its construction</description></item><item><title>The Classic Laptop Dev Configuration - Linux/Mac (Tutorial)</title><link>https://Kshitij-Banerjee.github.io/2017/01/24/the-classic-laptop-dev-configuration-linux/mac-tutorial/</link><pubDate>Tue, 24 Jan 2017 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2017/01/24/the-classic-laptop-dev-configuration-linux/mac-tutorial/</guid><description>The classic list of dev tools and configurations that I can&amp;#39;t live without (and how to set them up).</description></item><item><title>Reverse Engineer data from raw database files.</title><link>https://Kshitij-Banerjee.github.io/1/01/01/reverse-engineer-data-from-raw-database-files./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/1/01/01/reverse-engineer-data-from-raw-database-files./</guid><description>How to recover data from raw .tokudb files! Corrupted you tokudb mysql instance ? This post can help you recover the data from just the tokudb files.</description></item></channel></rss>