<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>KiloBytes by KB</title><link>https://Kshitij-Banerjee.github.io/</link><description>Recent content on KiloBytes by KB</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 24 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://Kshitij-Banerjee.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Build - Tony Fadell</title><link>https://Kshitij-Banerjee.github.io/2024/02/24/build-tony-fadell/</link><pubDate>Sat, 24 Feb 2024 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2024/02/24/build-tony-fadell/</guid><description>Introduction Tony Fadell is CEO of nest (bought by google ), and instrumental in building products at Apple like the iPod and the iPhone.
The book is not about facts and science, but based on tony’s experience and deals with subjective concepts like how to build products, dealing with assholes, and how to hire etc.
Overall, 4/5 stars for me, and I recommend reading it. It covers one strong individuals strong opinions, about how to deal with matters one must deal with when building impactful products.</description></item><item><title>Getting Things Done with LogSeq</title><link>https://Kshitij-Banerjee.github.io/2024/02/16/getting-things-done-with-logseq/</link><pubDate>Fri, 16 Feb 2024 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2024/02/16/getting-things-done-with-logseq/</guid><description>Introduction I was first introduced to the concept of &amp;ldquo;second-brain&amp;rdquo; from Tobi Lutke, the founder of Shopify. The topic started because someone asked whether he still codes - now that he is a founder of such a large company. Tobi went on to explain that he spent the weekend writing some code to customise Logseq to his preferences, and that he&amp;rsquo;s an active member of the Logseq community.
The following weekend, I setup Logseq and learnt its weird ways of working, and have since been an ardent user and fan of the Logseq/Obsidian methodology of building a &amp;ldquo;second-brain&amp;rdquo;</description></item><item><title>Understanding GPT 1, 2 and 3</title><link>https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/</guid><description>Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered transformers via the original paper &amp;ldquo;Attention is all you need&amp;rdquo; that brought the innovation that made all this progress possible.
This post will focus on GPT-3 and its predecessors GPT-1 and 2.</description></item><item><title>Understanding GPT - Transformers</title><link>https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/</link><pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/</guid><description>Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don&amp;rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here.</description></item><item><title>Understanding GPT - A Journey from RNNs to Attention</title><link>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</link><pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</guid><description>Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&amp;rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.</description></item><item><title>Loss Functions in ML</title><link>https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/</link><pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/</guid><description>Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
All losses in keras defined here
Frequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.</description></item><item><title>Intro to ML</title><link>https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/</link><pubDate>Fri, 20 Jan 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/</guid><description>Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.
I&amp;rsquo;ll update and make these more readable in the future (which possibly means never :D)
Lecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.
The notation “a := b” to denote an assignment operation</description></item><item><title>Python Cheet Sheet</title><link>https://Kshitij-Banerjee.github.io/2023/01/02/python-cheet-sheet/</link><pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2023/01/02/python-cheet-sheet/</guid><description>A quick cheatsheet on python operations
Slice:
astring = &amp;#34;Hello World&amp;#34; print(astring[3:7]) # prints-&amp;gt; lo w print(astring[0:10:2]) # skips one character, prints -&amp;gt; Hlowr print(astring[::-1]) # reverse a string using step -1 Case
astring.upper() astring.lower() Slicing complete list performs a copy
spam_copy = spam[:] Zip to loop
furniture = [&amp;#39;table&amp;#39;, &amp;#39;chair&amp;#39;, &amp;#39;rack&amp;#39;, &amp;#39;shelf&amp;#39;] price = [100, 50, 80, 40] for item, amount in zip(furniture, price): print(f&amp;#39;The {item} costs ${amount}&amp;#39;) Multiple assignments</description></item><item><title>Grit - Angela Duckworth</title><link>https://Kshitij-Banerjee.github.io/2022/12/12/grit-angela-duckworth/</link><pubDate>Mon, 12 Dec 2022 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2022/12/12/grit-angela-duckworth/</guid><description>Book Summary - Grit by Angela Duckworth The post is a book summary of the main bullet points from the book &amp;ldquo;Grit&amp;rdquo; by &amp;ldquo;Angela Duckworth&amp;rdquo;
Components of Grit Angela breaks down grit in the following components:-
Interest: I love what I do
Practice: I will do what it takes to improve and become world-class
Purpose: What I do is important for everyone
Hope: I will keep going even when it’s difficult</description></item><item><title>Largest Area Under a Histogram (and related concepts/problems).</title><link>https://Kshitij-Banerjee.github.io/2017/01/30/largest-area-under-a-histogram-and-related-concepts/problems./</link><pubDate>Mon, 30 Jan 2017 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/2017/01/30/largest-area-under-a-histogram-and-related-concepts/problems./</guid><description>A problem with a clever solution, with some insights to its construction</description></item><item><title>Reverse Engineer data from raw database files.</title><link>https://Kshitij-Banerjee.github.io/1/01/01/reverse-engineer-data-from-raw-database-files./</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://Kshitij-Banerjee.github.io/1/01/01/reverse-engineer-data-from-raw-database-files./</guid><description>How to recover data from raw .tokudb files! Corrupted you tokudb mysql instance ? This post can help you recover the data from just the tokudb files.</description></item></channel></rss>