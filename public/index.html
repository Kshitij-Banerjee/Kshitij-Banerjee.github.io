<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.122.0"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>KiloBytes by KB</title>
<meta name=description content><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/><link crossorigin=anonymous href=/assets/css/stylesheet.197da216ae0376a6e25536c67c59479922e8fce0dcc011f79bf6dcf38cf18663.css integrity="sha256-GX2iFq4DdqbiVTbGfFlHmSLo/ODcwBH3m/bc84zxhmM=" rel="preload stylesheet" as=style><link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://Kshitij-Banerjee.github.io/index.xml><link rel=alternate type=application/json href=https://Kshitij-Banerjee.github.io/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-KFSE3K3EMC"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KFSE3K3EMC",{anonymize_ip:!1})}</script><meta property="og:title" content="KiloBytes by KB"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/"><meta name=twitter:card content="summary"><meta name=twitter:title content="KiloBytes by KB"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"KiloBytes by KB","url":"https://Kshitij-Banerjee.github.io/","description":"","thumbnailUrl":"https://Kshitij-Banerjee.github.io/favicon.ico","sameAs":["https://www.linkedin.com/in/kshitijbanerjee/","https://kilobytes.substack.com/","https://github.com/Kshitij-Banerjee","mailto:ksh.dce@gmail.com"]}</script></head><body class=list id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class="first-entry home-info"><div class=profile><div class=profile_inner><img draggable=false src=https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb0e0342-380c-4c11-94fa-4e2c586ac21e_1170x1168.jpeg alt="profile image" title height=150 width=150>
<span><strong>Kshitij Banerjee (KB)</strong></span>
<span><em>AI, Books, and Code LLM Research</em></span><div class=social-icons><a href=https://www.linkedin.com/in/kshitijbanerjee/ target=_blank rel="noopener noreferrer me" title=Linkedin><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M16 8a6 6 0 016 6v7h-4v-7a2 2 0 00-2-2 2 2 0 00-2 2v7h-4v-7a6 6 0 016-6z"/><rect x="2" y="9" width="4" height="12"/><circle cx="4" cy="4" r="2"/></svg>
</a><a href=https://kilobytes.substack.com/ target=_blank rel="noopener noreferrer me" title=Substack><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentcolor" stroke-width="2"><path d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54.0H1.46v2.836h21.08V0z"/></svg>
</a><a href=https://github.com/Kshitij-Banerjee target=_blank rel="noopener noreferrer me" title=Github><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg>
</a><a href=mailto:ksh.dce@gmail.com target=_blank rel="noopener noreferrer me" title=Email><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 21" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 4h16c1.1.0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1.0-2-.9-2-2V6c0-1.1.9-2 2-2z"/><polyline points="22,6 12,13 2,6"/></svg></a></div></div></div></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/react-benchmark-eval.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Evaluating LLM Benchmarks for React<div class=cover-tags>AI, Machine-Learning</div></h2></header><div class=entry-content><p>Introduction I previously wrote about writing react code with Deepseek-coder 33b model, and whether we could improve some of these shortcomings with the latest research in the LLM space
But to really measure and mark progress, it would require the build of a benchmark to test various hypothesis around it.
So in this post, I’m going to evaluate existing benchmarks that specifically measures LLM capabilities on coding capabilities.
My goal is to be able to build a benchmark that can test their React/Typescript coding capabilities....</p></div><footer class=entry-footer><span title='2024-05-04 00:00:00 +0000 UTC'>May 4, 2024</span></footer><a class=entry-link aria-label="post link to Evaluating LLM Benchmarks for React" href=https://Kshitij-Banerjee.github.io/2024/05/04/evaluating-llm-benchmarks-for-react/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/llm-better-code.jpeg alt></figure><header class=entry-header><h2 class=entry-hint-parent>Can LLM's produce better code?<div class=cover-tags>AI, Machine-Learning</div></h2></header><div class=entry-content><p>Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.
I used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-...</p></div><footer class=entry-footer><span title='2024-04-30 00:00:00 +0000 UTC'>April 30, 2024</span></footer><a class=entry-link aria-label="post link to Can LLM's produce better code?" href=https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/exploring_code_llms.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Deepseek coder - Can it code in React?<div class=cover-tags>AI</div></h2></header><div class=entry-content><p>Introduction The goal of this post is to deep-dive into LLMs that are specialized in code generation tasks and see if we can use them to write code.
Note: Unlike copilot, we’ll focus on locally running LLM’s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we’ll perform a few simple coding tasks, compare the various methods in achieving the desired results, and also show the shortcomings....</p></div><footer class=entry-footer><span title='2024-04-15 00:00:00 +0000 UTC'>April 15, 2024</span></footer><a class=entry-link aria-label="post link to Deepseek coder - Can it code in React?" href=https://Kshitij-Banerjee.github.io/2024/04/15/deepseek-coder-can-it-code-in-react/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://miro.medium.com/v2/resize:fit:516/1*Jlq_cyLvRdmp_K5jCd3LkA.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Exploring Code LLMs - Instruction fine-tuning, models and quantization<div class=cover-tags>AI</div></h2></header><div class=entry-content><p>Introduction The goal of this post is to deep-dive into LLM’s that are specialised in code generation tasks, and see if we can use them to write code.
Note: Unlike copilot, we’ll focus on locally running LLM’s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we’ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings....</p></div><footer class=entry-footer><span title='2024-04-14 00:00:00 +0000 UTC'>April 14, 2024</span></footer><a class=entry-link aria-label="post link to Exploring Code LLMs - Instruction fine-tuning, models and quantization" href=https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms-instruction-fine-tuning-models-and-quantization/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/build_banner.webp alt></figure><header class=entry-header><h2 class=entry-hint-parent>Build - Tony Fadell<div class=cover-tags>Book-Review</div></h2></header><div class=entry-content><p>Introduction Tony Fadell is CEO of nest (bought by google ), and instrumental in building products at Apple like the iPod and the iPhone.
The book is not about facts and science, but based on tony’s experience and deals with subjective concepts like how to build products, dealing with assholes, and how to hire etc.
Overall, 4/5 stars for me, and I recommend reading it. It covers one strong individuals strong opinions, about how to deal with matters one must deal with when building impactful products....</p></div><footer class=entry-footer><span title='2024-02-24 00:00:00 +0000 UTC'>February 24, 2024</span></footer><a class=entry-link aria-label="post link to Build - Tony Fadell" href=https://Kshitij-Banerjee.github.io/2024/02/24/build-tony-fadell/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/GTDLogseqBanner.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Getting Things Done with LogSeq<div class=cover-tags>Book Review</div></h2></header><div class=entry-content><p>Introduction I was first introduced to the concept of “second-brain” from Tobi Lutke, the founder of Shopify. The topic started because someone asked whether he still codes - now that he is a founder of such a large company. Tobi went on to explain that he spent the weekend writing some code to customise Logseq to his preferences, and that he’s an active member of the Logseq community.
The following weekend, I setup Logseq and learnt its weird ways of working, and have since been an ardent user and fan of the Logseq/Obsidian methodology of building a “second-brain”...</p></div><footer class=entry-footer><span title='2024-02-16 00:00:00 +0000 UTC'>February 16, 2024</span></footer><a class=entry-link aria-label="post link to Getting Things Done with LogSeq" href=https://Kshitij-Banerjee.github.io/2024/02/16/getting-things-done-with-logseq/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/GPT-3_banner.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Understanding GPT 1, 2 and 3<div class=cover-tags>Machine Learning</div></h2></header><div class=entry-content><p>Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered transformers via the original paper “Attention is all you need” that brought the innovation that made all this progress possible.
This post will focus on GPT-3 and its predecessors GPT-1 and 2....</p></div><footer class=entry-footer><span title='2023-10-01 00:00:00 +0000 UTC'>October 1, 2023</span></footer><a class=entry-link aria-label="post link to Understanding GPT 1, 2 and 3" href=https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/Transformers_banner_1689490231707_0.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Understanding GPT - Transformers<div class=cover-tags>Machine Learning</div></h2></header><div class=entry-content><p>Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don’t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here....</p></div><footer class=entry-footer><span title='2023-07-07 00:00:00 +0000 UTC'>July 7, 2023</span></footer><a class=entry-link aria-label="post link to Understanding GPT - Transformers" href=https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/UnderstandingGPTBanner.jpg alt></figure><header class=entry-header><h2 class=entry-hint-parent>Understanding GPT - A Journey from RNNs to Attention<div class=cover-tags>Machine Learning</div></h2></header><div class=entry-content><p>Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I’ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood....</p></div><footer class=entry-footer><span title='2023-06-18 00:00:00 +0000 UTC'>June 18, 2023</span></footer><a class=entry-link aria-label="post link to Understanding GPT - A Journey from RNNs to Attention" href=https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/></a></article><article class=post-entry><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/image_1676730500910_0.png alt></figure><header class=entry-header><h2 class=entry-hint-parent>Loss Functions in ML<div class=cover-tags>Machine Learning</div></h2></header><div class=entry-content><p>Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
All losses in keras defined here
But why is the loss function expressed as a negative loss? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more....</p></div><footer class=entry-footer><span title='2023-02-18 00:00:00 +0000 UTC'>February 18, 2023</span></footer><a class=entry-link aria-label="post link to Loss Functions in ML" href=https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://Kshitij-Banerjee.github.io/page/2/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://Kshitij-Banerjee.github.io>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>