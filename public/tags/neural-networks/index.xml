<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Neural-networks on KiloBytes by KB</title>
    <link>https://Kshitij-Banerjee.github.io/tags/neural-networks/</link>
    <description>Recent content in Neural-networks on KiloBytes by KB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 18 Jun 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://Kshitij-Banerjee.github.io/tags/neural-networks/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding GPT - A Journey from RNNs to Attention</title>
      <link>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</guid>
      <description>Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&amp;rsquo;ve always liked to ground myself with foundational knowledgeÂ on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.</description>
    </item>
  </channel>
</rss>
