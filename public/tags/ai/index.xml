<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>AI on KiloBytes by KB</title>
    <link>https://Kshitij-Banerjee.github.io/tags/ai/</link>
    <description>Recent content in AI on KiloBytes by KB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 04 May 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://Kshitij-Banerjee.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Evaluating LLM Benchmarks for React</title>
      <link>https://Kshitij-Banerjee.github.io/2024/05/04/evaluating-llm-benchmarks-for-react/</link>
      <pubDate>Sat, 04 May 2024 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2024/05/04/evaluating-llm-benchmarks-for-react/</guid>
      <description>Introduction I previously wrote about writing react code with Deepseek-coder 33b model, and whether we could improve some of these shortcomings with the latest research in the LLM space
But to really measure and mark progress, it would require the build of a benchmark to test various hypothesis around it.
So in this post, I&amp;rsquo;m going to evaluate existing benchmarks that specifically measures LLM capabilities on coding capabilities.
My goal is to be able to build a benchmark that can test their React/Typescript coding capabilities.</description>
    </item>
    <item>
      <title>Can LLM&#39;s produce better code?</title>
      <link>https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/</link>
      <pubDate>Tue, 30 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/</guid>
      <description>Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.
I used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-</description>
    </item>
    <item>
      <title>Deepseek coder - Can it code in React?</title>
      <link>https://Kshitij-Banerjee.github.io/2024/04/15/deepseek-coder-can-it-code-in-react/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2024/04/15/deepseek-coder-can-it-code-in-react/</guid>
      <description>Introduction The goal of this post is to deep-dive into LLMs that are specialized in code generation tasks and see if we can use them to write code.
Note: Unlike copilot, we&amp;rsquo;ll focus on locally running LLM&amp;rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we&amp;rsquo;ll perform a few simple coding tasks, compare the various methods in achieving the desired results, and also show the shortcomings.</description>
    </item>
    <item>
      <title>Exploring Code LLMs - Instruction fine-tuning, models and quantization</title>
      <link>https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms-instruction-fine-tuning-models-and-quantization/</link>
      <pubDate>Sun, 14 Apr 2024 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms-instruction-fine-tuning-models-and-quantization/</guid>
      <description>Introduction The goal of this post is to deep-dive into LLM&amp;rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.
Note: Unlike copilot, we&amp;rsquo;ll focus on locally running LLM&amp;rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we&amp;rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings.</description>
    </item>
    <item>
      <title>Understanding GPT 1, 2 and 3</title>
      <link>https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/</guid>
      <description>Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered transformers via the original paper &amp;ldquo;Attention is all you need&amp;rdquo; that brought the innovation that made all this progress possible.
This post will focus on GPT-3 and its predecessors GPT-1 and 2.</description>
    </item>
    <item>
      <title>Understanding GPT - Transformers</title>
      <link>https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/</link>
      <pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/</guid>
      <description>Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don&amp;rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here.</description>
    </item>
    <item>
      <title>Understanding GPT - A Journey from RNNs to Attention</title>
      <link>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</link>
      <pubDate>Sun, 18 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/</guid>
      <description>Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&amp;rsquo;ve always liked to ground myself with foundational knowledgeÂ on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.</description>
    </item>
    <item>
      <title>Loss Functions in ML</title>
      <link>https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/</link>
      <pubDate>Sat, 18 Feb 2023 00:00:00 +0000</pubDate>
      <guid>https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/</guid>
      <description>Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
All losses in keras defined here
But why is the loss function expressed as a negative loss? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.</description>
    </item>
  </channel>
</rss>
