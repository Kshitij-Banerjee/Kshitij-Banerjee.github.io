<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Can LLM's produce better code? | KiloBytes by KB</title>
<meta name=keywords content="Machine-Learning,AI"><meta name=description content="Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.
I used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-"><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/><link crossorigin=anonymous href=/assets/css/stylesheet.197da216ae0376a6e25536c67c59479922e8fce0dcc011f79bf6dcf38cf18663.css integrity="sha256-GX2iFq4DdqbiVTbGfFlHmSLo/ODcwBH3m/bc84zxhmM=" rel="preload stylesheet" as=style><link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-KFSE3K3EMC"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KFSE3K3EMC",{anonymize_ip:!1})}</script><meta property="og:title" content="Can LLM's produce better code?"><meta property="og:description" content="Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.
I used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-"><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/"><meta property="og:image" content="https://Kshitij-Banerjee.github.io/llm-better-code.jpeg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-30T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-30T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Kshitij-Banerjee.github.io/llm-better-code.jpeg"><meta name=twitter:title content="Can LLM's produce better code?"><meta name=twitter:description content="Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.
I used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Can LLM's produce better code?","item":"https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Can LLM's produce better code?","name":"Can LLM\u0027s produce better code?","description":"Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.\nI used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-","keywords":["Machine-Learning","AI"],"articleBody":"Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.\nI used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-\nThe randomness problem: LLMs are unable to produce correct code in the first attempt, however a few attempts (sometimes) leads to the correct code output.\nThe complexity problem: Smaller, more manageable problem with lesser constraints are more feasible, than complex multi-constraint problem.\nFor example, if I would ask it to code a component and gave both styling and logic constraints in the prompt, it would frequently solve the logic but miss the styling part of the solution.\n(Hunch) Out of training problem: I also noticed that it spectacularly fails in smaller sized problems for specific types. For example, while it can write react code pretty well. It’s ability of writing test cases was quite horrid, and will typically just write the test case name, and leave the implementation as a “TODO: Fill this implementation…”.\nSo I spent some time researching existing literature that could explain the reasoning, and potential solutions to these problems.\nHow the rest of the post is structured.\nHumanEval+ - A summary on this rigorous evaluation of CodeLLMs and how they fair in this extended benchmark. There are some interesting insights and learnings about LLM behavior here.\nThe effect of Pre-Planning in code LLMs: Insights from this paper on how pre-planning helps produce better code\nThe effect of using a planning-algorithm (Monte Carlo Tree Search) in the LLM decoding process: Insights from this paper, that suggest using a planning algorithm can improve the probability of producing “correct” code, while also improving efficiency (when compared to traditional beam search / greedy search).\nThe effect of using a higher-level planning algorithm (like MCTS) to solve more complex problems: Insights from this paper, on using LLMs to make common sense decisions to improve on a traditional MCTS planning algorithm.\nOverall - I believe using a combination of these concepts can be viable approach to solving complex coding problems, with higher accuracy than using vanilla implementation of current code LLMs. I’ll detail more insights and summarise the key findings in the end.\nHuman Eval+ Paper: HumanEval+\nCore Problem: Existing code LLM benchmarks are insufficient, and lead to wrong evaluation of models. The authors found, that by adding new test cases to the HumanEval benchmark, the rankings of some open source LLM’s (Phind, WizardCoder) overshot the scores for ChatGPT (GPT 3.5, not GPT4), which was previously incorrectly ranked higher than the others.\nHow they create tests cases Liu et.al augmented the existing HumanEval test suite by\ngenerating some seed inputs by prompting ChatGPT.\nUsing type-based mutations to generate more test inputs\nComparing the results on these additional inputs on the ground-truth solution to the LLM generated solutions\nAdding these new (minimal-set-of) inputs into a new benchmark.\nThe results Insights from the paper 1) Increasing K in pass@k , almost always leads to improved benchmark results. This proves that the correct solution does exist in the solution space of the LLM outputs most of the times, however it may not be the first one that the LLM spits out. Using a strategy that can guide the LLM towards the reward has the potential to lead to better outcomes.\n2) Choosing a temperature value When using a pass@1 (or single greedy output), choose a low temperate of 0.2 or below\nFor a larger number of passes, a higher temperature value of 0.6 -\u003e 0.8, will lead to good results.\n3) Practically, k=10 is a decent default to pick The improvement between k=1, and k=10 is pretty large. However this improvement, is not really extrapolated in the same degree when moving from k=10, to k=100.\n4) New code models are coming up Comparing the results from the paper, to the current eval board, its clear that the space is rapidly changing and new open source models are gaining traction. (Deepseek-coder wasn’t even present in the original paper)\n5) Llama3 is still behind This one was surprising to me, I thought the 70B LLama3-instruct model, being larger and also trained on 15T tokens, would perform quite well.\nHowever, the benchmark shows its still behind deepseek, wizard and other open source models\nPre-Planning in Code LLMs Paper: Self-Planning Code Generation with LLM\nCore Problem While chain-of-thought adds some limited reasoning abilities to LLMs, it does not work properly for code-outputs.\nTypically, CoT in code is done via creating sequences of comments interspersed with code output.\nThis is because, while mentally reasoning step-by-step works for problems that mimic human chain of though, coding requires more overall planning than simply step-by-step thinking.\nHow they solve it An obvious solution is to make the LLM think about a high level plan first, before it writes the code. This is precisely the subject of evaluation for this paper.\nTo create such a plan the authors use few-shot learning examples to create plans.\nWhat is a good plan ? The authors expect the plans to be in a specific fashion.\nNamely that it is a number list, and each item is a step that is executable as a subtask.\nThe plan should always conclude with a return statement.\nResults Adding a self planning step, that adds a high-level plan before the implementation starts-creates a 25% improvement in benchmark results.\nInterestingly, the authors also evaluate a multi-turn self planning step, and find it inferior.\nIn the multi-turn approach, the LM Takes iterative turns to create a final code output as opposed to producing the output in one-turn.\nThis seems counter-intuitive to me, given all the recent progress in Agentic LLMs.\nThey offer some clues:-\nThey find that the multi turn approach does not work as well as a one-shot approach because:-\nThis can be ascribed to two possible causes: 1) there is a lack of one-to-one correspondence between the code snippets and steps, with the implementation of a solution step possibly interspersed with multiple code snippets; 2) LLM faces challenges in determining the termination point for code generation with a sub-plan. Since the final goal or intent is specified at the outset, this often results in the model persistently generating the entire code without considering the indicated end of a step, making it difficult to determine where to truncate the code. When implemented as a one-phase process, the self-planning approach has been shown to yield slightly improved performance compared to the two-phase way.\nInsights and recommendations from the paper Considering limited LLM context windows.\nThe authors suggest a 2-phase plan + 8 shot examples approach produces best results\n(2 phase in this context, does not mean 2 turns. It simply means, the LLM is prompted to prepare the plan first, and then the plan is concatenated to produce the final output)\nwe generally recommend using either 8-shot or 4-shot for self-planning in LLMs.\nPlanning algorithms in LLM Decoder Paper: Planning with LLM for code gen\nProblem LLMs being probabilistic machines, they do not always create correct programs in a single run. However, if we sample the code outputs from an LLM enough times, usually the correct program lies somewhere in the sample set. The task of finding the correct output by sampling and filtering is costly. Intuitively, in sampling + filtering, we are not making use of any objectives to focus the search on the “correct” outputs, but merely hoping that the correct output lies somewhere in a large sample.\nCan we integrate a planning algorithm with a pre-trained code generation Transformer, achieving an algorithm that generates better programs than the conventional Transformer generation algorithms and the well-accepted sampling + filtering scheme in the literature?\nCore Idea The core idea here is that we can search for optimal code outputs from a transformer effectively by integrating a planning algorithm, like Monte Carlo tree search, into the decoding process as compared to a standard beam search algorithm that is typically used.\nCatch For this to work, we need to create a reward function with which to evaluate different code outputs produced during the search of each branch in the solution space. The reward function here is based on evaluating test-cases. So an explicit need for “testable” code is required for this approach to work.\nBut assuming we can create tests, by providing such an explicit reward - we can focus the tree search on finding higher pass-rate code outputs, instead of the typical beam search of finding high token probability code outputs.\nIntuitively, transformers are built to produce outputs that match previously seen completions - which may not be the same as a program that is correct and solves the overall problem.\nThe paper shows, that using a planning algorithm like MCTS can not only create better quality code outputs. But it is also more resource efficient as we do not have to create a large amount of samples to use for filtering.\nTo achieve this efficiency, a caching mechanism is implemented, that ensures the intermediate results of beam search and the planning MCTS do not compute the same output sequence multiple times.\nAnother interesting idea, is to use these planner-guided solutions to fine-tune the LLM to improve its future outputs\nLLM reasoning for MCTS Paper: LLM-MCTS - Zhao et.al\nThe Question The core concept of this paper intrigues me. In essence, the paper tries to answer the question - “Can the reasoning abilities of LLM models, be used to guide a Monte Carlo search in finding the optimal answer to a higher-level problem like object rearrangement in household\"\nThe 3 options The authors conduct comparison between 3 solutions\nA pure LLM solution (LLM-Policy)\nA planner guided solution called LLM-model (LLM guided MCTS), and\nA hybrid approach (LLM-MCTS) where the LLMs reasoning is not used as a hard action, but used as part of a heuristic to continue the search process.\nHow it works Insights from the paper 1. LLM-MCTS outperforms both the LLM-policy, and LLM-model 2. The improvement is much more noticeable when the problem is Complicated / Novel as opposed to simple. This quantifies the initial intuition of this post, that LLM’s are unable to solve more complex/novel problems but perform much better for smaller/simpler problems\nAn intuitive understanding of this, is best explained by the authors via an analogy to the multiplication example. i.e: Multiplying large numbers is hard, but using a algorithm that works on top of foundational concepts becomes a simpler solve.\nA decimal number is described as a sequence of n digits,(dn−1, dn−2, . . . , d0). There are two methods of implementing multiplication with an LLM. The first one corresponds to L-Policy. We represent the multiplication function as a table. Each row or column corresponds to a number. The table entry is the multiplication of two numbers, obtained by querying an LLM. Experimentally, GPT4 performs single-digit multiplication perfectly with 100% accuracy, 2-digit multiplication with 99% accuracy, 4-digit multiplication with merely 4% accuracy, and fails almost completely on 5-digit multiplication [ 10 ]. The second approach uses LLM-derived small single-digit multiplication tables, which GPT4 performs with 100% accuracy. To multiply multi-digit numbers, it applies the long multiplication algorithm with the single-digit table. This method corresponds to L-Model. While long multiplication differs from planning in algorithmic details, it plays the same role in L-Model and is highly efficient. Clearly, this second method achieves100% accuracy for arbitrarily large numbers, provided that the single-digit multiplication table is accurate. So, the L-Model outperforms L-Policy for the multiplication task, contrary to the finding for object rearrangement tasks\nMy take In scenarios where the problem is complex, it’s solution space is large, AND there exist some known planning algorithms that can solve the larger problem at play - its beneficial to let the planning algorithms take control of the overall process, but let the LLM guide the intelligent search process to utilise its inherent common sense and reasoning abilities.\nIn essence, when the problem becomes sufficiently complicated and large, it is hard for LLMs to solve the entire problem in one go. This is because, LLMs can’t stop, rethink and evaluate before they answer and lack any second-order thinking. Also, in complicated situations like path finding, a certain level of trial and error search is required to find the answer.\nSummarising some key points LLM’s can solve simpler problems well, but struggle with complex/novel problems Finding ways to break a complicated problem into smaller pieces, and using known algorithms to combine the pieces is a viable idea.\nTests can be used to focus the LLM decoding process to optimise for correctness without fine-tuning. In problem spaces where we can use tests to judge program correctness (eg: Test cases, Visual comparison, comparison with ground-truth solutions), we can guide the LLM outputs towards correctness more efficiently.\nFurther, a planner-guided LLM output can be used to fine-tune the base model to improve its base accuracy\nPre-Planning and Chain-of-thought are cost-effective default solutions to try It is intuitive, that making these models build a plan and working through the solutions step-by-step would increase correctness, without increasing computation cost much, and as such should probably be defaulted into most developer workflows.\nConclusion There are some promising ideas in the field of planner-augmented problem solving that can be applied to code generation.\nRigorous study and evaluation is needed to confirm these ideas, and conduct further research.\nIf there are other papers related to the same field, or you have some insights, I’d would love to know more. Please reach out, or comment to share them with me.\n","wordCount":"2292","inLanguage":"en","image":"https://Kshitij-Banerjee.github.io/llm-better-code.jpeg","datePublished":"2024-04-30T00:00:00Z","dateModified":"2024-04-30T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/"},"publisher":{"@type":"Organization","name":"KiloBytes by KB","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Can LLM's produce better code?</h1><div class=post-meta><span title='2024-04-30 00:00:00 +0000 UTC'>April 30, 2024</span></div></header><figure class=entry-cover><img loading=eager src=https://Kshitij-Banerjee.github.io/llm-better-code.jpeg alt></figure><div style=display:flex;justify-content:center;align-items:center><iframe src=https://kilobytes.substack.com/embed width=480 height=150 style="border:none;background:0 0" frameborder=0 scrolling=no></iframe></div><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>In my previous <a href=https://kshitij-banerjee.github.io/2024/04/15/deepseek-coder-can-it-code-in-react/>post</a>, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the <a href=https://arxiv.org/pdf/2305.01210.pdf>HumanEval+</a> benchmark <a href=https://evalplus.github.io/leaderboard.html>leaderboard</a> - DeepseekCoder:33b-instruct.</p><p>I used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-</p><p><strong>The randomness problem:</strong> LLMs are unable to produce correct code in the first attempt, however a few attempts (sometimes) leads to the correct code output.</p><p><strong>The complexity problem:</strong> Smaller, more manageable problem with lesser constraints are more feasible, than complex multi-constraint problem.</p><p>For example, if I would ask it to code a component and gave both styling and logic constraints in the prompt, it would frequently solve the logic but miss the styling part of the solution.</p><p><strong>(Hunch) Out of training problem</strong>: I also noticed that it spectacularly fails in smaller sized problems for specific types. For example, while it can write react code pretty well. It&rsquo;s ability of writing test cases was quite horrid, and will typically just write the test case name, and leave the implementation as a &ldquo;TODO: Fill this implementation&mldr;&rdquo;.</p><p>So I spent some time researching existing literature that could explain the reasoning, and potential solutions to these problems.</p><p>How the rest of the post is structured.</p><ol><li><p><a href=https://arxiv.org/pdf/2305.01210.pdf>HumanEval+</a> - A summary on this rigorous evaluation of CodeLLMs and how they fair in this extended benchmark. There are some interesting insights and learnings about LLM behavior here.</p></li><li><p>The effect of <strong>Pre-Planning in code LLMs</strong>: Insights from this <a href=https://arxiv.org/pdf/2303.06689.pdf>paper</a> on how pre-planning helps produce better code</p></li><li><p>The effect of using a planning-algorithm (Monte Carlo Tree Search) in the LLM decoding process: Insights from this <a href="https://openreview.net/pdf?id=Lr8cOOtYbfL.pdf">paper</a>, that suggest using a planning algorithm can improve the probability of producing &ldquo;correct&rdquo; code, while also improving efficiency (when compared to traditional beam search / greedy search).</p></li><li><p>The effect of using a higher-level planning algorithm (like MCTS) to solve more complex problems: Insights from this <a href=https://arxiv.org/pdf/2305.14078.pdf>paper</a>, on using LLMs to make common sense decisions to improve on a traditional MCTS planning algorithm.</p></li></ol><p>Overall - I believe using a combination of these concepts can be viable approach to solving complex coding problems, with higher accuracy than using vanilla implementation of current code LLMs. I&rsquo;ll detail more insights and summarise the key findings in the end.</p><h1 id=human-eval>Human Eval+<a hidden class=anchor aria-hidden=true href=#human-eval>#</a></h1><p><strong>Paper</strong>: <a href=https://arxiv.org/pdf/2305.01210.pdf>HumanEval+</a></p><h2 id=core-problem>Core Problem:<a hidden class=anchor aria-hidden=true href=#core-problem>#</a></h2><p>Existing code LLM benchmarks are insufficient, and lead to wrong evaluation of models. The authors found, that by adding new test cases to the HumanEval benchmark, the rankings of some open source LLM&rsquo;s (Phind, WizardCoder) overshot the scores for ChatGPT (GPT 3.5, not GPT4), which was previously incorrectly ranked higher than the others.</p><h2 id=how-they-create-tests-cases>How they create tests cases<a hidden class=anchor aria-hidden=true href=#how-they-create-tests-cases>#</a></h2><p>Liu et.al augmented the existing HumanEval test suite by</p><ol><li><p>generating some seed inputs by prompting ChatGPT.</p></li><li><p>Using type-based mutations to generate more test inputs</p></li><li><p>Comparing the results on these additional inputs on the ground-truth solution to the LLM generated solutions</p></li><li><p>Adding these new (minimal-set-of) inputs into a new benchmark.</p></li></ol><h1 id=the-results>The results<a hidden class=anchor aria-hidden=true href=#the-results>#</a></h1><p><img loading=lazy src=/2024-04-30-18-02-16.jpeg alt=2024-04-30-18-02-16.jpeg></p><h2 id=insights-from-the-paper>Insights from the paper<a hidden class=anchor aria-hidden=true href=#insights-from-the-paper>#</a></h2><h3 id=1-increasing-k-in-passk--almost-always-leads-to-improved-benchmark-results>1) Increasing K in pass@k , almost always leads to improved benchmark results.<a hidden class=anchor aria-hidden=true href=#1-increasing-k-in-passk--almost-always-leads-to-improved-benchmark-results>#</a></h3><p>This proves that the correct solution <em>does exist</em> in the solution space of the LLM outputs most of the times, however it may not be the first one that the LLM spits out. Using a strategy that can guide the LLM towards the reward has the potential to lead to better outcomes.</p><h3 id=2-choosing-a-temperature-value>2) Choosing a temperature value<a hidden class=anchor aria-hidden=true href=#2-choosing-a-temperature-value>#</a></h3><p>When using a pass@1 (or single greedy output), choose a low temperate of 0.2 or below</p><p>For a larger number of passes, a higher temperature value of 0.6 -> 0.8, will lead to good results.</p><h3 id=3-practically-k10-is-a-decent-default-to-pick>3) Practically, k=10 is a decent default to pick<a hidden class=anchor aria-hidden=true href=#3-practically-k10-is-a-decent-default-to-pick>#</a></h3><p>The improvement between k=1, and k=10 is pretty large. However this improvement, is not really extrapolated in the same degree when moving from k=10, to k=100.</p><h3 id=4-new-code-models-are-coming-up>4) New code models are coming up<a hidden class=anchor aria-hidden=true href=#4-new-code-models-are-coming-up>#</a></h3><p>Comparing the results from the paper, to the current eval board, its clear that the space is rapidly changing and new open source models are gaining traction. (Deepseek-coder wasn&rsquo;t even present in the original paper)</p><h3 id=5-llama3-is-still-behind>5) Llama3 is still behind<a hidden class=anchor aria-hidden=true href=#5-llama3-is-still-behind>#</a></h3><p>This one was surprising to me, I thought the 70B LLama3-instruct model, being larger and also trained on 15T tokens, would perform quite well.</p><p>However, the benchmark shows its still behind deepseek, wizard and other open source models</p><h1 id=pre-planning-in-code-llms>Pre-Planning in Code LLMs<a hidden class=anchor aria-hidden=true href=#pre-planning-in-code-llms>#</a></h1><p>Paper: <a href=https://arxiv.org/pdf/2303.06689.pdf>Self-Planning Code Generation with LLM</a></p><h2 id=core-problem-1>Core Problem<a hidden class=anchor aria-hidden=true href=#core-problem-1>#</a></h2><p>While <a href=https://arxiv.org/pdf/2201.11903.pdf>chain-of-thought</a> adds <em>some</em> limited reasoning abilities to LLMs, it does not work properly for code-outputs.</p><p>Typically, CoT in code is done via creating sequences of comments interspersed with code output.</p><p><img loading=lazy src=/image_1714484127955_0.png alt=image.png></p><p>This is because, while mentally reasoning step-by-step works for problems that mimic human chain of though, coding requires more overall planning than simply step-by-step thinking.</p><h2 id=how-they-solve-it>How they solve it<a hidden class=anchor aria-hidden=true href=#how-they-solve-it>#</a></h2><p>An obvious solution is to make the LLM think about a high level plan first, before it writes the code. This is precisely the subject of evaluation for this paper.</p><p>To create such a plan the authors use few-shot learning examples to create plans.</p><h3 id=what-is-a-good-plan->What is a good plan ?<a hidden class=anchor aria-hidden=true href=#what-is-a-good-plan->#</a></h3><p>The authors expect the plans to be in a specific fashion.</p><p>Namely that it is a number list, and each item is a step that is executable as a subtask.</p><p>The plan should <strong>always conclude with a return statement.</strong></p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>Adding a self planning step, that adds a high-level plan <em>before</em> the implementation starts-creates a 25% improvement in benchmark results.</p><p>Interestingly, the authors also evaluate a multi-turn self planning step, and find it <em>inferior</em>.</p><p>In the multi-turn approach, the LM Takes iterative turns to create a final code output as opposed to producing the output in one-turn.</p><p>This seems counter-intuitive to me, given all the recent progress in Agentic LLMs.</p><p>They offer some clues:-</p><p>They find that the multi turn approach does not work as well as a one-shot approach because:-</p><blockquote><p>This can be ascribed to two possible causes: 1) there is a lack of one-to-one correspondence between the code snippets and steps, with the implementation of a solution step possibly interspersed with multiple code snippets; 2) <strong>LLM faces challenges in determining the termination point for code generation with a sub-plan</strong>. Since the final goal or intent is specified at the outset, this often results in the model persistently generating the entire code without considering the indicated end of a step, making it difficult to determine where to truncate the code. When implemented as a one-phase process, the self-planning approach has been shown to yield slightly improved performance compared to the two-phase way.</p></blockquote><p><img loading=lazy src=/2024-04-28-23-01-32.jpeg alt=2024-04-28-23-01-32.jpeg></p><h3 id=insights-and-recommendations-from-the-paper>Insights and recommendations from the paper<a hidden class=anchor aria-hidden=true href=#insights-and-recommendations-from-the-paper>#</a></h3><p>Considering limited LLM context windows.</p><p>The authors suggest a 2-phase plan + 8 shot examples approach produces best results</p><p>(2 phase in this context, <em>does not</em> mean 2 turns. It simply means, the LLM is prompted to prepare the plan first, and then the plan is concatenated to produce the final output)</p><blockquote><p>we generally recommend using either 8-shot or 4-shot for self-planning in LLMs.</p></blockquote><h1 id=planning-algorithms-in-llm-decoder>Planning algorithms in LLM Decoder<a hidden class=anchor aria-hidden=true href=#planning-algorithms-in-llm-decoder>#</a></h1><p>Paper: <a href="https://openreview.net/pdf?id=Lr8cOOtYbfL">Planning with LLM for code gen</a></p><h2 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h2><p>LLMs being probabilistic machines, they do not always create correct programs in a single run. However, if we sample the code outputs from an LLM enough times, usually the correct program lies somewhere in the sample set. The task of finding the correct output by sampling and filtering is costly. Intuitively, in sampling + filtering, we are not making use of any objectives to focus the search on the “correct” outputs, but merely hoping that the correct output lies somewhere in a large sample.</p><blockquote><p>Can we integrate a planning algorithm with a pre-trained code generation Transformer, achieving an algorithm that generates better
programs than the conventional Transformer generation algorithms and the well-accepted sampling + filtering scheme in the literature?</p></blockquote><h2 id=core-idea>Core Idea<a hidden class=anchor aria-hidden=true href=#core-idea>#</a></h2><p>The core idea here is that we can search for optimal code outputs from a transformer effectively by integrating a planning algorithm, like Monte Carlo tree search, into the decoding process as compared to a standard beam search algorithm that is typically used.</p><p><img loading=lazy src=/image_1714484788934_0.png alt=image.png></p><h3 id=catch>Catch<a hidden class=anchor aria-hidden=true href=#catch>#</a></h3><p>For this to work, we need to create a reward function with which to evaluate different code outputs produced during the search of each branch in the solution space. The reward function here is based on evaluating test-cases. So an explicit need for &ldquo;testable&rdquo; code is required for this approach to work.</p><p>But assuming we can create tests, by providing such an explicit reward - we can focus the tree search on finding higher pass-rate code outputs, instead of the typical beam search of finding high token probability code outputs.</p><p>Intuitively, transformers are built to produce outputs that match previously seen completions - <em>which may not be the same as a program that is correct and solves the overall problem.</em></p><p>The paper shows, that using a planning algorithm like MCTS can not only create better quality code outputs. But it is also more resource efficient as we do not have to create a large amount of samples to use for filtering.</p><p>To achieve this efficiency, a caching mechanism is implemented, that ensures the intermediate results of beam search and the planning MCTS do not compute the same output sequence multiple times.</p><p><em>Another interesting idea, is to use these planner-guided solutions to fine-tune the LLM to improve its future outputs</em></p><h1 id=llm-reasoning-for-mcts>LLM reasoning for MCTS<a hidden class=anchor aria-hidden=true href=#llm-reasoning-for-mcts>#</a></h1><p>Paper: <a href=https://arxiv.org/pdf/2305.14078.pdf>LLM-MCTS</a> - Zhao et.al</p><h2 id=the-question>The Question<a hidden class=anchor aria-hidden=true href=#the-question>#</a></h2><p>The core concept of this paper intrigues me. In essence, the paper tries to answer the question - “Can the reasoning abilities of LLM models, be used to guide a Monte Carlo search in finding the optimal answer to a higher-level problem like object rearrangement in household"</p><h2 id=the-3-options>The 3 options<a hidden class=anchor aria-hidden=true href=#the-3-options>#</a></h2><p>The authors conduct comparison between 3 solutions</p><ol><li><p>A pure LLM solution (LLM-Policy)</p></li><li><p>A planner guided solution called LLM-model (LLM guided MCTS), and</p></li><li><p>A hybrid approach (LLM-MCTS) where the LLMs reasoning is not used as a hard action, but used as part of a heuristic to continue the search process.</p></li></ol><h2 id=how-it-works>How it works<a hidden class=anchor aria-hidden=true href=#how-it-works>#</a></h2><p><img loading=lazy src=/image_1714485267082_0.png alt=image.png></p><h2 id=insights-from-the-paper-1>Insights from the paper<a hidden class=anchor aria-hidden=true href=#insights-from-the-paper-1>#</a></h2><h4 id=1-llm-mcts-outperforms-both-the-llm-policy-and-llm-model>1. LLM-MCTS outperforms both the LLM-policy, and LLM-model<a hidden class=anchor aria-hidden=true href=#1-llm-mcts-outperforms-both-the-llm-policy-and-llm-model>#</a></h4><p><img loading=lazy src=/image_1714485471917_0.png alt=image.png></p><h4 id=2-the-improvement-is-much-more-noticeable-when-the-problem-is-complicated--novel-as-opposed-to-simple>2. The improvement is much more noticeable when the problem is Complicated / Novel as opposed to simple.<a hidden class=anchor aria-hidden=true href=#2-the-improvement-is-much-more-noticeable-when-the-problem-is-complicated--novel-as-opposed-to-simple>#</a></h4><p><em>This quantifies the initial intuition of this post, that LLM&rsquo;s are unable to solve more complex/novel problems but perform much better for smaller/simpler problems</em></p><p>An intuitive understanding of this, is best explained by the authors via an analogy to the multiplication example. i.e: Multiplying large numbers is hard, but using a algorithm that works on top of foundational concepts becomes a simpler solve.</p><blockquote><p>A decimal number is described as a sequence of n digits,(dn−1, dn−2, . . . , d0).
There are two methods of implementing multiplication with an LLM. The first one corresponds to L-Policy. We represent the multiplication function as a table. Each row or column corresponds to a number. The table entry is the multiplication of two numbers, obtained by querying an LLM. Experimentally, GPT4 performs single-digit multiplication perfectly with 100% accuracy, 2-digit multiplication with 99% accuracy, 4-digit multiplication with merely 4% accuracy, and fails almost completely on 5-digit multiplication [ 10 ]. The second approach uses LLM-derived small single-digit multiplication tables, which GPT4 performs with 100% accuracy. To multiply multi-digit numbers, it applies the long multiplication algorithm with the single-digit table. This method corresponds to L-Model. While long multiplication differs from planning in algorithmic details, it plays the same role in L-Model and is highly efficient. Clearly, this second method achieves100% accuracy for arbitrarily large numbers, provided that the single-digit multiplication table is accurate. So, the L-Model outperforms L-Policy for the multiplication task, contrary to the finding for object rearrangement tasks</p></blockquote><h2 id=my-take>My take<a hidden class=anchor aria-hidden=true href=#my-take>#</a></h2><p>In scenarios where the problem is complex, it’s solution space is large, AND there exist some known planning algorithms that can solve the larger problem at play - its beneficial to let the planning algorithms take control of the overall process, but let the LLM guide the intelligent search process to utilise its inherent common sense and reasoning abilities.</p><p>In essence, when the problem becomes sufficiently complicated and large, it is hard for LLMs to solve the entire problem in one go. This is because, LLMs can’t stop, rethink and evaluate before they answer and lack any second-order thinking. Also, in complicated situations like path finding, a certain level of trial and error search is required to find the answer.</p><h1 id=summarising-some-key-points>Summarising some key points<a hidden class=anchor aria-hidden=true href=#summarising-some-key-points>#</a></h1><h3 id=llms-can-solve-simpler-problems-well-but-struggle-with-complexnovel-problems>LLM&rsquo;s can solve simpler problems well, but struggle with complex/novel problems<a hidden class=anchor aria-hidden=true href=#llms-can-solve-simpler-problems-well-but-struggle-with-complexnovel-problems>#</a></h3><p>Finding ways to break a complicated problem into smaller pieces, and using known algorithms to combine the pieces is a viable idea.</p><h3 id=tests-can-be-used-to-focus-the-llm-decoding-process-to-optimise-for-correctness-without-fine-tuning>Tests can be used to focus the LLM decoding process to optimise for correctness without fine-tuning.<a hidden class=anchor aria-hidden=true href=#tests-can-be-used-to-focus-the-llm-decoding-process-to-optimise-for-correctness-without-fine-tuning>#</a></h3><p>In problem spaces where we can use tests to judge program correctness (eg: Test cases, Visual comparison, comparison with ground-truth solutions), we can guide the LLM outputs towards correctness more efficiently.</p><p>Further, a planner-guided LLM output can be used to fine-tune the base model to improve its base accuracy</p><h3 id=pre-planning-and-chain-of-thought-are-cost-effective-default-solutions-to-try>Pre-Planning and Chain-of-thought are cost-effective default solutions to try<a hidden class=anchor aria-hidden=true href=#pre-planning-and-chain-of-thought-are-cost-effective-default-solutions-to-try>#</a></h3><p>It is intuitive, that making these models build a plan and working through the solutions step-by-step would increase correctness, without increasing computation cost much, and as such should probably be defaulted into most developer workflows.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>There are some promising ideas in the field of planner-augmented problem solving that can be applied to code generation.</p><p>Rigorous study and evaluation is needed to confirm these ideas, and conduct further research.</p><p>If there are other papers related to the same field, or you have some insights, I&rsquo;d would love to know more. Please reach out, or comment to share them with me.</p></div><footer class=post-footer><div style=display:flex;justify-content:center;align-items:center><iframe src=https://kilobytes.substack.com/embed width=480 height=320 style="border:1px solid #eee;background:#fff" frameborder=0 scrolling=no></iframe></div><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/machine-learning/>machine-learning</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li></ul></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kshitij-banerjee.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a>
<script src=https://giscus.app/client.js data-repo=Kshitij-Banerjee/Kshitij-Banerjee.github.io data-repo-id=R_kgDOI1NsBg data-category=General data-category-id=DIC_kwDOI1NsBs4Cdf1h data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://Kshitij-Banerjee.github.io>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>