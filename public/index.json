[{"content":"Introduction I previously wrote about writing react code with Deepseek-coder 33b model, and whether we could improve some of these shortcomings with the latest research in the LLM space\nBut to really measure and mark progress, it would require the build of a benchmark to test various hypothesis around it.\nSo in this post, I\u0026rsquo;m going to evaluate existing benchmarks that specifically measures LLM capabilities on coding capabilities.\nMy goal is to be able to build a benchmark that can test their React/Typescript coding capabilities.\nWhat we need Unit Test Evaluations In this method, we\u0026rsquo;ll require that the LLM write the code, and then we will run unit tests to measure the outcome.\nWe then will evaluate pass@1, pass@k, and strict-accuracy metrics.\nVisual verification In this method, we want to test style replication and ask the LLM to produce a component with some given specifications.\nWe\u0026rsquo;ll then verify it\u0026rsquo;s output against a known ground-truth of correct visual output.\nEase of writing, and similarity to real-life I\u0026rsquo;d also want this to be similar to how we write code practically.\nA file where some code is written, and a corresponding .test file that imports the code and runs a set of evaluations.\nHow the rest of the post is structured Review of existing benchmarks and how they are setup OpenAI Evals\nAPPS benchmark\nHumanEval\nCanAiCode\nMultiPL-E\nRepoBench\nIn a future post, I intent to cover Details on Test based method\nDetails on Visual verification\nBenchmark Results for 3 open source LLM models.\n1) OpenAI Evals This is probably the most renowned of all evaluation frameworks. https://github.com/openai/evals\nHowever, they don\u0026rsquo;t accept \u0026ldquo;Custom code\u0026rdquo; Evals. Meaning, only simple matches (Exact, Includes, Fuzzy Match) are possible test evaluations to run.\nEven though OpenAI doesn\u0026rsquo;t accept these evals. It\u0026rsquo;s worth noting that we can simply fork the repo and write our own custom evals\nThe framework allows to build a custom eval, as well as a custom completion function. It also comes with a nice cookbook tutorial.\nPros Mature framework.\nA ton of existing sample benchmarks. Once this is set up, it will allow one to find results on other interesting benchmarks.\nEnables custom evals and custom completions\nCons Doesn\u0026rsquo;t accept new custom evals.\nIt\u0026rsquo;s a bit heavy to setup, with git LFS and lots of dependencies that are added over time\nDoesn\u0026rsquo;t have many code related benchmarks\nVerdict üëç - This could work for building a react benchmark. It might be a bit hard to get off the ground though, and may limit customization.\n2) APPS Paper: Measuring Coding Challenge Competence With APPS\nRepository: https://github.com/hendrycks/apps\n10,000 code generation problems of varying difficulties. Covers simple introductory problems, interview-level problems, and coding competition challenges\nPros Simple code base. See evaluation guide here\nA ton of Coding specific evaluations, with multiple difficulty levels.\nCons Most of the code benchmarks are python. So it may not work too well for other languages.\nIsn\u0026rsquo;t written with extensibility in mind, and mostly coded for testing python codebases.\nVerdict üëé - Not something to use for custom real world \u0026ldquo;app\u0026rdquo; related benchmarking 3) HumanEval From OpenAI again, hand-written set of evaluations\nRepo: https://github.com/openai/human-eval\nPaper: Evaluating LLMs\nWe evaluate functional correctness on a set of 164 handwritten programming problems, which we call the HumanEval dataset. Each problem includes a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem\nPros Pretty simple codebase, and good examples Cons Mostly python evaluations Verdict If not testing python, this one is a üëé\n4) CanAiCode Repo: https://github.com/the-crypt-keeper/can-ai-code/blob/main/prompts/codellama-input-v2.txt\nLeaderboard: https://huggingface.co/spaces/mike-ravkine/can-ai-code-results\nPros Supports Javascript, and not just python test cases.\nTemplate based generation of test cases. See template prompt for starcoder\n{% if language == \u0026#34;python\u0026#34; %}\u0026lt;fim_prefix\u0026gt;def {{Signature}}: \u0026#39;\u0026#39;\u0026#39;a function {{Input}} that returns {{Output}}{% if Fact %} given {{Fact}}{% endif %}\u0026#39;\u0026#39;\u0026#39; \u0026lt;fim_suffix\u0026gt; # another function{% endif %} {% if language == \u0026#34;javascript\u0026#34; %}\u0026lt;fim_prefix\u0026gt;// a function {{Input}} that returns {{Output}}{% if Fact %} given {{Fact}}{% endif %} function {{Signature}} { \u0026lt;fim_suffix\u0026gt; } // another function{% endif %}\u0026lt;fim_middle\u0026gt; Combined with yaml for tests .Checks: \u0026amp;Checks FactorialZeroShot: Signature: \u0026#34;factorial(n)\u0026#34; Input: \u0026#34;with input n\u0026#34; Output: \u0026#34;the factorial of n using iteration\u0026#34; Description: \u0026#34;See if the model can implement a well known function\u0026#34; Checks: one_argument: assert: \u0026#34;len(f.args)\u0026#34; eq: 1 returns_list: assert: \u0026#34;isinstance(f.call(1),int)\u0026#34; eq: true value_0: assert: \u0026#34;f.call(1)\u0026#34; eq: 1 value_5: assert: \u0026#34;f.call(5)\u0026#34; eq: 120 Cons 1 - Unfortunately, it is not customizable beyond simple input-output testing.\n5) MultiPL-E Meant to tests code LLMs on multiple programming languages.\nA system for translating unit test-driven neural code generation benchmarks to new languages. We have used MultiPL-E to translate two popular Python benchmarks (HumanEval and MBPP) to 18 other programming languages.\nExamples shown here: https://nuprl.github.io/MultiPL-E/\nPaper: MultiPL-E\nRepo: https://github.com/nuprl/MultiPL-E\nPros Examples on running JS tests: https://github.com/nuprl/MultiPL-E/blob/main/prompts/humaneval-js-keep.jsonl\nEnables writing tests as a function, so not just simple input output comparisons.\nAdding new tests seems simple: See https://nuprl.github.io/MultiPL-E/new_benchmark.html\nCons While the tutorial makes it sound like writing the test cases is really simple. This doesn\u0026rsquo;t seem to be the case\nhttps://github.com/nuprl/MultiPL-E/blob/main/prompts/humaneval-r-remove.jsonl\nEach of the test case needs to be decoded in a particular jsonl format, with escape characters fixed etc.\nVerdict üëç - This could work for building a react benchmark. But may not be easy to add new test cases to.\n6) RepoBench Paper: RepoBench\nRepo: https://github.com/Leolty/repobench\nValidates LLM on 3 tasks\n1 - Retrieval Task: Ability to retrieve the right contextual files.\n2 - Completion Task: Ability to complete next line, given the context files.\n3 - Combined Task: Retrieval + Completion\nSome interesting points noted in the paper:\nPython Retrieval Shows Higher Accuracy Than Java: The language-specific results show that Python tasks typically show higher accuracy than Java across all retrieval methods. This discrepancy might be attributed to Python‚Äôs simpler syntax and less verbose nature, potentially reducing the variability of similar code snippets.\nPronounced Performance Differences in Java for RepoBenchC-2k: The evaluation on Java showcases a marked differentiation in model performance: Codex notably stands out as the superior model, followed by StarCoder, while CodeGen largely lags behind.\nWhile there are some intuitive reasons cited, this clearly shows that benchmarks on Python may not directly apply to React / Typescript codebases.\nInteresting bits The project is easy to read and some interesting files are\n1 - Metrics: ExactMatch, Similarity, and Accuracy@K https://github.com/Leolty/repobench/blob/main/evaluation/metrics.py. Note: their accuracy@k is not a probabilistic calculation like the pass@k metric introduced in HumanEval, and refers to the number of accurate codes retrieved out of correct codes.\n2 - Retriever: https://github.com/Leolty/repobench/blob/main/retriever/retriever.py\n3 - Similarity (Jaccard, Edit, Cosine): https://github.com/Leolty/repobench/blob/main/retriever/similarity.py\n4 - Promp constructor: https://github.com/Leolty/repobench/blob/main/data/utils.py\nPros 1 - Easy to understand\n2 - Repo level context understanding.\n3 - Usage of Google drive for dataset.\n4 - Multiple languages supported with various similarity metrics on next line.\nCons The question this benchmark is trying to answer is different from what we need.\nWe require unit-test and visual accuracy, assuming the right context is already given.\nVerdict Not applicable.\nConclusion So far, the only ones that meet what I\u0026rsquo;m looking for are the open-ai evals, and the MultiPL-E benchmark.\nIdeally, if these benchmarks were easier to prepare and mimicked the way we actually write code / test cases, then it would be much easier to extend.\nSo after this research, I believe the best answer is to build a new \u0026ldquo;ReactBench\u0026rdquo; - a benchmark that mimics how React code is structured and is geared towards accuracy on Typescript / React with unit-testing and snapshotting.\n","permalink":"https://Kshitij-Banerjee.github.io/2024/05/04/evaluating-llm-benchmarks-for-react/","summary":"Introduction I previously wrote about writing react code with Deepseek-coder 33b model, and whether we could improve some of these shortcomings with the latest research in the LLM space\nBut to really measure and mark progress, it would require the build of a benchmark to test various hypothesis around it.\nSo in this post, I\u0026rsquo;m going to evaluate existing benchmarks that specifically measures LLM capabilities on coding capabilities.\nMy goal is to be able to build a benchmark that can test their React/Typescript coding capabilities.","title":"Evaluating LLM Benchmarks for React"},{"content":"Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.\nI used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-\nThe randomness problem: LLMs are unable to produce correct code in the first attempt, however a few attempts (sometimes) leads to the correct code output.\nThe complexity problem: Smaller, more manageable problem with lesser constraints are more feasible, than complex multi-constraint problem.\nFor example, if I would ask it to code a component and gave both styling and logic constraints in the prompt, it would frequently solve the logic but miss the styling part of the solution.\n(Hunch) Out of training problem: I also noticed that it spectacularly fails in smaller sized problems for specific types. For example, while it can write react code pretty well. It\u0026rsquo;s ability of writing test cases was quite horrid, and will typically just write the test case name, and leave the implementation as a \u0026ldquo;TODO: Fill this implementation\u0026hellip;\u0026rdquo;.\nSo I spent some time researching existing literature that could explain the reasoning, and potential solutions to these problems.\nHow the rest of the post is structured.\nHumanEval+ - A summary on this rigorous evaluation of CodeLLMs and how they fair in this extended benchmark. There are some interesting insights and learnings about LLM behavior here.\nThe effect of Pre-Planning in code LLMs: Insights from this paper on how pre-planning helps produce better code\nThe effect of using a planning-algorithm (Monte Carlo Tree Search) in the LLM decoding process: Insights from this paper, that suggest using a planning algorithm can improve the probability of producing \u0026ldquo;correct\u0026rdquo; code, while also improving efficiency (when compared to traditional beam search / greedy search).\nThe effect of using a higher-level planning algorithm (like MCTS) to solve more complex problems: Insights from this paper, on using LLMs to make common sense decisions to improve on a traditional MCTS planning algorithm.\nOverall - I believe using a combination of these concepts can be viable approach to solving complex coding problems, with higher accuracy than using vanilla implementation of current code LLMs. I\u0026rsquo;ll detail more insights and summarise the key findings in the end.\nHuman Eval+ Paper: HumanEval+\nCore Problem: Existing code LLM benchmarks are insufficient, and lead to wrong evaluation of models. The authors found, that by adding new test cases to the HumanEval benchmark, the rankings of some open source LLM\u0026rsquo;s (Phind, WizardCoder) overshot the scores for ChatGPT (GPT 3.5, not GPT4), which was previously incorrectly ranked higher than the others.\nHow they create tests cases Liu et.al augmented the existing HumanEval test suite by\ngenerating some seed inputs by prompting ChatGPT.\nUsing type-based mutations to generate more test inputs\nComparing the results on these additional inputs on the ground-truth solution to the LLM generated solutions\nAdding these new (minimal-set-of) inputs into a new benchmark.\nThe results Insights from the paper 1) Increasing K in pass@k , almost always leads to improved benchmark results. This proves that the correct solution does exist in the solution space of the LLM outputs most of the times, however it may not be the first one that the LLM spits out. Using a strategy that can guide the LLM towards the reward has the potential to lead to better outcomes.\n2) Choosing a temperature value When using a pass@1 (or single greedy output), choose a low temperate of 0.2 or below\nFor a larger number of passes, a higher temperature value of 0.6 -\u0026gt; 0.8, will lead to good results.\n3) Practically, k=10 is a decent default to pick The improvement between k=1, and k=10 is pretty large. However this improvement, is not really extrapolated in the same degree when moving from k=10, to k=100.\n4) New code models are coming up Comparing the results from the paper, to the current eval board, its clear that the space is rapidly changing and new open source models are gaining traction. (Deepseek-coder wasn\u0026rsquo;t even present in the original paper)\n5) Llama3 is still behind This one was surprising to me, I thought the 70B LLama3-instruct model, being larger and also trained on 15T tokens, would perform quite well.\nHowever, the benchmark shows its still behind deepseek, wizard and other open source models\nPre-Planning in Code LLMs Paper: Self-Planning Code Generation with LLM\nCore Problem While chain-of-thought adds some limited reasoning abilities to LLMs, it does not work properly for code-outputs.\nTypically, CoT in code is done via creating sequences of comments interspersed with code output.\nThis is because, while mentally reasoning step-by-step works for problems that mimic human chain of though, coding requires more overall planning than simply step-by-step thinking.\nHow they solve it An obvious solution is to make the LLM think about a high level plan first, before it writes the code. This is precisely the subject of evaluation for this paper.\nTo create such a plan the authors use few-shot learning examples to create plans.\nWhat is a good plan ? The authors expect the plans to be in a specific fashion.\nNamely that it is a number list, and each item is a step that is executable as a subtask.\nThe plan should always conclude with a return statement.\nResults Adding a self planning step, that adds a high-level plan before the implementation starts-creates a 25% improvement in benchmark results.\nInterestingly, the authors also evaluate a multi-turn self planning step, and find it inferior.\nIn the multi-turn approach, the LM Takes iterative turns to create a final code output as opposed to producing the output in one-turn.\nThis seems counter-intuitive to me, given all the recent progress in Agentic LLMs.\nThey offer some clues:-\nThey find that the multi turn approach does not work as well as a one-shot approach because:-\nThis can be ascribed to two possible causes: 1) there is a lack of one-to-one correspondence between the code snippets and steps, with the implementation of a solution step possibly interspersed with multiple code snippets; 2) LLM faces challenges in determining the termination point for code generation with a sub-plan. Since the final goal or intent is specified at the outset, this often results in the model persistently generating the entire code without considering the indicated end of a step, making it difficult to determine where to truncate the code. When implemented as a one-phase process, the self-planning approach has been shown to yield slightly improved performance compared to the two-phase way.\nInsights and recommendations from the paper Considering limited LLM context windows.\nThe authors suggest a 2-phase plan + 8 shot examples approach produces best results\n(2 phase in this context, does not mean 2 turns. It simply means, the LLM is prompted to prepare the plan first, and then the plan is concatenated to produce the final output)\nwe generally recommend using either 8-shot or 4-shot for self-planning in LLMs.\nPlanning algorithms in LLM Decoder Paper: Planning with LLM for code gen\nProblem LLMs being probabilistic machines, they do not always create correct programs in a single run. However, if we sample the code outputs from an LLM enough times, usually the correct program lies somewhere in the sample set. The task of finding the correct output by sampling and filtering is costly. Intuitively, in sampling + filtering, we are not making use of any objectives to focus the search on the ‚Äúcorrect‚Äù outputs, but merely hoping that the correct output lies somewhere in a large sample.\nCan we integrate a planning algorithm with a pre-trained code generation Transformer, achieving an algorithm that generates better programs than the conventional Transformer generation algorithms and the well-accepted sampling + filtering scheme in the literature?\nCore Idea The core idea here is that we can search for optimal code outputs from a transformer effectively by integrating a planning algorithm, like Monte Carlo tree search, into the decoding process as compared to a standard beam search algorithm that is typically used.\nCatch For this to work, we need to create a reward function with which to evaluate different code outputs produced during the search of each branch in the solution space. The reward function here is based on evaluating test-cases. So an explicit need for \u0026ldquo;testable\u0026rdquo; code is required for this approach to work.\nBut assuming we can create tests, by providing such an explicit reward - we can focus the tree search on finding higher pass-rate code outputs, instead of the typical beam search of finding high token probability code outputs.\nIntuitively, transformers are built to produce outputs that match previously seen completions - which may not be the same as a program that is correct and solves the overall problem.\nThe paper shows, that using a planning algorithm like MCTS can not only create better quality code outputs. But it is also more resource efficient as we do not have to create a large amount of samples to use for filtering.\nTo achieve this efficiency, a caching mechanism is implemented, that ensures the intermediate results of beam search and the planning MCTS do not compute the same output sequence multiple times.\nAnother interesting idea, is to use these planner-guided solutions to fine-tune the LLM to improve its future outputs\nLLM reasoning for MCTS Paper: LLM-MCTS - Zhao et.al\nThe Question The core concept of this paper intrigues me. In essence, the paper tries to answer the question - ‚ÄúCan the reasoning abilities of LLM models, be used to guide a Monte Carlo search in finding the optimal answer to a higher-level problem like object rearrangement in household\u0026quot;\nThe 3 options The authors conduct comparison between 3 solutions\nA pure LLM solution (LLM-Policy)\nA planner guided solution called LLM-model (LLM guided MCTS), and\nA hybrid approach (LLM-MCTS) where the LLMs reasoning is not used as a hard action, but used as part of a heuristic to continue the search process.\nHow it works Insights from the paper 1. LLM-MCTS outperforms both the LLM-policy, and LLM-model 2. The improvement is much more noticeable when the problem is Complicated / Novel as opposed to simple. This quantifies the initial intuition of this post, that LLM\u0026rsquo;s are unable to solve more complex/novel problems but perform much better for smaller/simpler problems\nAn intuitive understanding of this, is best explained by the authors via an analogy to the multiplication example. i.e: Multiplying large numbers is hard, but using a algorithm that works on top of foundational concepts becomes a simpler solve.\nA decimal number is described as a sequence of n digits,(dn‚àí1, dn‚àí2, . . . , d0). There are two methods of implementing multiplication with an LLM. The first one corresponds to L-Policy. We represent the multiplication function as a table. Each row or column corresponds to a number. The table entry is the multiplication of two numbers, obtained by querying an LLM. Experimentally, GPT4 performs single-digit multiplication perfectly with 100% accuracy, 2-digit multiplication with 99% accuracy, 4-digit multiplication with merely 4% accuracy, and fails almost completely on 5-digit multiplication [ 10 ]. The second approach uses LLM-derived small single-digit multiplication tables, which GPT4 performs with 100% accuracy. To multiply multi-digit numbers, it applies the long multiplication algorithm with the single-digit table. This method corresponds to L-Model. While long multiplication differs from planning in algorithmic details, it plays the same role in L-Model and is highly efficient. Clearly, this second method achieves100% accuracy for arbitrarily large numbers, provided that the single-digit multiplication table is accurate. So, the L-Model outperforms L-Policy for the multiplication task, contrary to the finding for object rearrangement tasks\nMy take In scenarios where the problem is complex, it‚Äôs solution space is large, AND there exist some known planning algorithms that can solve the larger problem at play - its beneficial to let the planning algorithms take control of the overall process, but let the LLM guide the intelligent search process to utilise its inherent common sense and reasoning abilities.\nIn essence, when the problem becomes sufficiently complicated and large, it is hard for LLMs to solve the entire problem in one go. This is because, LLMs can‚Äôt stop, rethink and evaluate before they answer and lack any second-order thinking. Also, in complicated situations like path finding, a certain level of trial and error search is required to find the answer.\nSummarising some key points LLM\u0026rsquo;s can solve simpler problems well, but struggle with complex/novel problems Finding ways to break a complicated problem into smaller pieces, and using known algorithms to combine the pieces is a viable idea.\nTests can be used to focus the LLM decoding process to optimise for correctness without fine-tuning. In problem spaces where we can use tests to judge program correctness (eg: Test cases, Visual comparison, comparison with ground-truth solutions), we can guide the LLM outputs towards correctness more efficiently.\nFurther, a planner-guided LLM output can be used to fine-tune the base model to improve its base accuracy\nPre-Planning and Chain-of-thought are cost-effective default solutions to try It is intuitive, that making these models build a plan and working through the solutions step-by-step would increase correctness, without increasing computation cost much, and as such should probably be defaulted into most developer workflows.\nConclusion There are some promising ideas in the field of planner-augmented problem solving that can be applied to code generation.\nRigorous study and evaluation is needed to confirm these ideas, and conduct further research.\nIf there are other papers related to the same field, or you have some insights, I\u0026rsquo;d would love to know more. Please reach out, or comment to share them with me.\n","permalink":"https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/","summary":"Introduction In my previous post, I tested a coding LLM on its ability to write React code. Specifically, I tried the currently leading open source model in the HumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.\nI used this model in development for a few weeks, and published a subset of examples in the post. Even though I tried this on a relatively small problem size, there were some obvious issues that were recognisable to me, namely:-","title":"Can LLM's produce better code?"},{"content":"Introduction The goal of this post is to deep-dive into LLMs that are specialized in code generation tasks and see if we can use them to write code.\nNote: Unlike copilot, we\u0026rsquo;ll focus on locally running LLM\u0026rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.\nTo test our understanding, we\u0026rsquo;ll perform a few simple coding tasks, compare the various methods in achieving the desired results, and also show the shortcomings.\nIn Part-1, I covered some papers around instruction fine-tuning, GQA and Model Quantization - All of which make running LLM\u0026rsquo;s locally possible.\nIn this Part-2 of the series, I\u0026rsquo;d like to focus on how to setup an M1 to run deepseek-coder, and the verdict on its coding capabilities in react on a few tests\nThe tasks Test 1: Generate a higher-order-component / decorator that enables logging on a react component\nTest 2: Write a test plan, and implement the test cases\nTest 3: Parse an uploaded excel file in the browser.\nSetting Up the Environment: Ollama on M1 Option 1: Hosting the model To host the models, I chose the ollama project: https://ollama.com/\nOllama is essentially, docker for LLM models and allows us to quickly run various LLM\u0026rsquo;s and host them over standard completion APIs locally.\nThe website and documentation is pretty self-explanatory, so I wont go into the details of setting it up.\nOption 2: My machine is not strong enough, but I\u0026rsquo;d like to experiment If your machine doesn\u0026rsquo;t support these LLM\u0026rsquo;s well (unless you have an M1 and above, you\u0026rsquo;re in this category), then there is the following alternative solution I\u0026rsquo;ve found.\nYou can rent machines relatively cheaply (~0.4$ / hour) for inference methods, using vast.ai\nOnce you\u0026rsquo;ve setup an account, added your billing methods, and have copied your API key from settings.\nClone the llm-deploy repo, and follow the instructions.\nThis repo figures out the cheapest available machine and hosts the ollama model as a docker image on it.\nFrom 1 and 2, you should now have a hosted LLM model running. Now we need VSCode to call into these models and produce code.\nVSCode Extension Calling into the Model [UPDATE]: I\u0026rsquo;ve recently found an open source plugin works well. If I was starting from scratch, I\u0026rsquo;d try https://www.continue.dev/.\nGiven the above best practices on how to provide the model its context, and the prompt engineering techniques that the authors suggested have positive outcomes on result. I created a VSCode plugin that implements these techniques, and is able to interact with Ollama running locally.\nThe source code for this plugin is available here:\nhttps://github.com/Kshitij-Banerjee/kb-ollama-coder\nThis plugin achieves the following:-\nIt provides the LLM context on project/repository relevant files.\nThe plugin not only pulls the current file, but also loads all the currently open files in Vscode into the LLM context.\nIt then trims the context to the last 16000/24000 characters (configurable)\nThis is an approximation, as deepseek coder enables 16K tokens, and approximate that each token is 1.5 tokens. In practice, I believe this can be much higher - so setting a higher value in the configuration should also work.\nIt adds a header prompt, based on the guidance from the paper. (Configurable) Example:-\n\u0026ldquo;You need to first write a step-by-step outline and then write the code. The following is a complete {LANG} file named {FILE_NAME} in the project {PROJECT_NAME}. Anything NOT code is written as a CODE COMMENT.\u0026rdquo;\nSource code The source code for this plugin is available here:\nhttps://github.com/Kshitij-Banerjee/kb-ollama-coder\nFor completeness, there are other VSCode plugins that might also solve a similar problem - and I am yet to try them\nhttps://github.com/rjmacarthy/twinny\nhttps://github.com/smallcloudai/refact\nhttps://github.com/TabbyML/tabby\nTesting DeepseekCoder General Observations For the most part, the 7b instruct model was quite useless and produces mostly error and incomplete responses.\nThe 33b models can do quite a few things correctly. However, it\u0026rsquo;s probably at the brink of usefulness\nThe model doesn\u0026rsquo;t really understand writing test cases at all.\nTest Setting For all the tests, I used a temperature setting of 0, as the guideline is to have lower temperatures for tasks like coding, instead of creative generation tasks (I find coding a pretty creative activity though, but just going with the guidelines for now)\nTest 1: Mount/Unmount logging using hooks and HOC In this task, I wanted to test its ability to understand basic react concepts of useEffect and hooks / HOC\n1.1 Writing custom hook with mount/unmount logging PROMPT\n// A react custom hook that logs the component name on mount and unmount CODE OUTPUT:\nimport { useEffect } from \u0026#39;react\u0026#39;; export default function withLoggingHook(componentName: string) { return () =\u0026gt; { useEffect(() =\u0026gt; { console.log(`${componentName} mounted`); return () =\u0026gt; { console.log(`${componentName} unmounted`); }; }, []); }; } Things to note:\nIt doesn\u0026rsquo;t know when to stop, and kind of keep spitting code at times - so I had to manually stop it\nVerdict üëç - Works as expected 1.2 Writing a HOC that augments the logging instead of using the hook. PROMPT:\n// A react HOC, that wraps a given component and logs the component name // on mount and unmount. CODE OUTPUT (Attempt 1):\nimport React from \u0026#39;react\u0026#39;; export function withLoggingHook\u0026lt;T extends {}\u0026gt;( WrappedComponent: React.ComponentType\u0026lt;T\u0026gt;, ) { return class WithLoggingHoc extends React.PureComponent\u0026lt;T, {}\u0026gt; { constructor(props: T) { super(props); console.log(\u0026#39;Mounted \u0026#39; + WrappedComponent.name); } componentWillUnmount() { console.log(\u0026#39;Unmounted \u0026#39; + WrappedComponent.name); } render() { return \u0026lt;WrappedComponent {...this.props} /\u0026gt;; } }; } // ./src/generated/withLoggingHook.test.tsx import React from \u0026#39;react\u0026#39;; import { render, fireEvent } from \u0026#39;@testing-library/react\u0026#39;; import { withLoggingHook } from \u0026#39;./withLoggingHook\u0026#39;; const TestComponent = () =\u0026gt; \u0026lt;div\u0026gt;Test Component\u0026lt;/div\u0026gt;; describe(\u0026#39;withLoggingHook\u0026#39;, () =\u0026gt; { beforeEach(() =\u0026gt; { jest.spyOn(console, \u0026#39;log\u0026#39;); }); afterEach(() =\u0026gt; { jest.resetAllMocks(); }); it(\u0026#39;should log when mounted and unmounted\u0026#39;, () =\u0026gt; { const ComponentWithLogging = withLoggingHook(TestComponent); const { rerender, unmount } = render(\u0026lt;ComponentWithLogging /\u0026gt;); expect(console.log).toHaveBeenCalledWith(\u0026#39;Mounted TestComponent\u0026#39;); rerender(\u0026lt;div\u0026gt;\u0026lt;/div\u0026gt;); expect(console.log).not.toHaveBeenLastCalledWith(\u0026#39;Unmounted TestComponent\u0026#39;); unmount(); expect(console.log).toHaveBeenLastCalledWith(\u0026#39;Unmounted TestComponent\u0026#39;); }); }); Mistakes: It used a constructor, instead of the componentDidMount method.\nadded a fireEvent import, but didn\u0026rsquo;t use it later.\nRetry: I retried a couple more times. In the next attempt, it jumbled the output and got things completely wrong.\nAttempt 3 is did pretty well though\nCODE OUTPUT (Attempt 3):\n// A react HOC, that wraps a given component and logs the component name // on mount and unmount. import React from \u0026#39;react\u0026#39;; import { useEffect } from \u0026#39;react\u0026#39;; import { ComponentType } from \u0026#39;react\u0026#39;; export function withLoggingHook\u0026lt;TProps\u0026gt;( WrappedComponent: ComponentType\u0026lt;TProps\u0026gt;, componentName: string, ) { const WithLogging = (props: TProps) =\u0026gt; { useEffect(() =\u0026gt; { console.log(`${componentName} is mounted`); return () =\u0026gt; { console.log(`${componentName} is unmounted`); }; }, []); return \u0026lt;WrappedComponent {...props} /\u0026gt;; }; WithLogging.displayName = `WithLogging(${getDisplayName(WrappedComponent)})`; return WithLogging; } function getDisplayName\u0026lt;TProps\u0026gt;(WrappedComponent: ComponentType\u0026lt;TProps\u0026gt;) { return WrappedComponent.displayName || WrappedComponent.name || \u0026#39;Component\u0026#39;; } I also noticed, that asking it specifically what I want, returned the right output in the first attempt\nCODE OUTPUT (Attempt 4): asking explicitly what I want\n// A react HOC, that wraps a given component and logs the component name // on mount and unmount. Use effects, instead of componentDidMount / unmount import React from \u0026#39;react\u0026#39;; export function withLoggingHook\u0026lt;P extends object\u0026gt;( Component: React.ComponentType\u0026lt;P\u0026gt;, ) { return (props: P): JSX.Element =\u0026gt; { console.log(\u0026#39;[withLoggingHook]\u0026#39;, Component.name, \u0026#39;mounted\u0026#39;); React.useEffect(() =\u0026gt; { return () =\u0026gt; console.log(\u0026#39;[withLoggingHook]\u0026#39;, Component.name, \u0026#39;un-mounted\u0026#39;); }, []); return \u0026lt;Component {...props} /\u0026gt;; }; } Verdict: üëç It\u0026rsquo;s able to write basic hooks, but makes some errors with HOCs\nSometimes, it automatically wrote the test case for the code it generated, which was a pleasant surprise!\nRetrying a few times leads to automatically producing a better answer.\nComponent.name, is not defined, so that seems like a bug.\nPotential future experiments Experimenting with n, top_p ,and top_k parameters, and enabling the LLM to try multiple completions\nUsing a \u0026lsquo;Verifier\u0026rsquo; agent, that checks the completion for compilation and validity and retries\nGiving it concrete examples, that it can follow.\nTest 2: Write a test plan, and implement the test cases Next, let\u0026rsquo;s test its test case writing abilities\nTest 2.1 - Writing tests for withLoggingHook In the previous test, it wrote a withLoggingHook, let\u0026rsquo;s use that as its context\nAs mentioned before, I used the vscode plugin I created previously, to give it context of the withLoggingHooks code\nPROMPT:\n// [FILE-NAME] ./src/generated/withLoggingHook.tsx // A react HOC, that wraps a given component and logs the component name // on mount and unmount. Use effects, instead of componentDidMount / unmount ... code from withLogginHook\u0026lt;P\u0026gt; ... // [FILE-NAME] generatedApps/DoctorFinderApp/doctor-finder-app/src/generated/withLoggingHook.spec.tsx // Test cases for withLoggingHook CODE OUTPUT:\nimport { render } from \u0026#39;@testing-library/react\u0026#39;; import { withLoggingHook } from \u0026#39;./withLoggingHook\u0026#39;; describe(\u0026#39;withLoggingHook\u0026#39;, () =\u0026gt; { beforeEach(() =\u0026gt; { // Mock console.log to prevent unwanted output during testing jest.spyOn(console, \u0026#39;log\u0026#39;).mockImplementation(() =\u0026gt; {}); }); afterEach(() =\u0026gt; { // Restore original console.log after each test (console.log as jest.Mock).mockRestore(); }); it(\u0026#39;should log the component name on mount\u0026#39;, () =\u0026gt; { const TestComponent = withLoggingHook((): null =\u0026gt; null); render(\u0026lt;TestComponent /\u0026gt;); expect(console.log).toHaveBeenCalledWith( \u0026#39;[withLoggingHook]\u0026#39;, \u0026#39;TestComponent\u0026#39;, \u0026#39;mounted\u0026#39;, ); }); it(\u0026#39;should log the component name on unmount\u0026#39;, () =\u0026gt; { const TestComponent = withLoggingHook((): null =\u0026gt; null); const { unmount } = render(\u0026lt;TestComponent /\u0026gt;); unmount(); expect(console.log).toHaveBeenCalledWith( \u0026#39;[withLoggingHook]\u0026#39;, \u0026#39;TestComponent\u0026#39;, \u0026#39;un-mounted\u0026#39;, ); }); }); Mistakes: The test case in itself is fine, the issue with the withLoggingHook causes the tests to fail\nHowever, I did realise that multiple attempts on the same test case did not always lead to promising results.\nVerdict: üëé For simple test cases, it works quite well, but just barely.\nAnything more complex, it kinda makes too many bugs to be productively useful.\nTask 3: Excel Upload Component Let\u0026rsquo;s ask it to make a slightly more complicated component with some functionality\nPROMPT\n// A react component, that allows the user to upload excel files. // The excel file has one sheet, and that sheet has two columns // Columns headers are foo, and bar, and both should have number values. // When the file is uploaded, it is parsed using XLSX. // The final parsed values of foo and bar, are given back as an array to the callback. CODE OUTPUT:\nimport React from \u0026#39;react\u0026#39;; import XLSX from \u0026#39;xlsx\u0026#39;; interface ExcelUploadComponentProps { onFileLoad: (data: { foo: number[]; bar: number[] }) =\u0026gt; void; } const ExcelUploadComponent: React.FC\u0026lt;ExcelUploadComponentProps\u0026gt; = ({ onFileLoad, }: ExcelUploadComponentProps) =\u0026gt; { const handleChange = (e: React.ChangeEvent\u0026lt;HTMLInputElement\u0026gt;) =\u0026gt; { if (!e.target.files || e.target.files.length === 0) return; const file = e.target.files[0]; const reader = new FileReader(); reader.onload = (event: ProgressEvent\u0026lt;FileReader\u0026gt;) =\u0026gt; { if (!event.target || !event.target.result) return; const data = event.target.result as string; const workbook = XLSX.read(data, { type: \u0026#39;binary\u0026#39; }); const sheetName = workbook.SheetNames[0]; const worksheet = workbook.Sheets[sheetName]; // Convert the excel data to JSON const jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1 }); // Extract the foo and bar values from the JSON data const fooValues = []; const barValues = []; for (let i = 0; i \u0026lt; jsonData.length; i++) { if (!jsonData[i]) continue; const row = jsonData[i]; fooValues.push(row[0]); barValues.push(row[1]); } onFileLoad({ foo: fooValues, bar: barValues }); }; reader.readAsBinaryString(file); }; return \u0026lt;input type=\u0026#34;file\u0026#34; accept=\u0026#34;.xls,.xlsx,.csv\u0026#34; onChange={handleChange} /\u0026gt;; }; export default ExcelUploadComponent; Verdict: üëç üëç Now that, was pretty good. I have no idea how to use the xlsx utility, and have not looked at its documentation\nEven though, I had to correct some typos and some other minor edits - this gave me a component that does exactly what I needed.\nI\u0026rsquo;d say this save me atleast 10-15 minutes of time googling for the api documentation and fumbling till I got it right.\nConclusion All in all, I think having locally running LLMs that can help us with code is possibly very near\nThese current models, while don\u0026rsquo;t really get things correct always, do provide a pretty handy tool and in situations where new territory / new apps are being made, I think they can make significant progress.\nSomething to note, is that once I provide more longer contexts, the model seems to make a lot more errors. This is potentially only model specific, so future experimentation is needed here.\nWhat\u0026rsquo;s next There were quite a few things I didn\u0026rsquo;t explore here. I will cover those in future posts.\nHere\u0026rsquo;s a list of a few things I\u0026rsquo;m going to experiment next\nProviding more examples of good code, instead of trying to explicitly mention every detail we want\nComparing other models on similar exercises. Possibly making a benchmark test suite to compare them against.\nTrying multi-agent setups. I having another LLM that can correct the first ones mistakes, or enter into a dialogue where two minds reach a better outcome is totally possible.\n\u0026ndash; A hint on this, is that once it gets something wrong, and I add the mistake to the prompt - the next iteration of the output is usually much better.\n","permalink":"https://Kshitij-Banerjee.github.io/2024/04/15/deepseek-coder-can-it-code-in-react/","summary":"Introduction The goal of this post is to deep-dive into LLMs that are specialized in code generation tasks and see if we can use them to write code.\nNote: Unlike copilot, we\u0026rsquo;ll focus on locally running LLM\u0026rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.\nTo test our understanding, we\u0026rsquo;ll perform a few simple coding tasks, compare the various methods in achieving the desired results, and also show the shortcomings.","title":"Deepseek coder - Can it code in React?"},{"content":"Introduction The goal of this post is to deep-dive into LLM\u0026rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.\nNote: Unlike copilot, we\u0026rsquo;ll focus on locally running LLM\u0026rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.\nTo test our understanding, we\u0026rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings.\nThe goal - A few simple coding task Test 1: Generate a higher-order-component / decorator that enables logging on a react component\nTest 2: Write a test plan, and implement the test cases\nTest 3: Parse an uploaded excel file in the browser.\nHow the rest of the post is structured We\u0026rsquo;re going to cover some theory, explain how to setup a locally running LLM model, and then finally conclude with the test results.\nPart 1: Quick theory\nInstead of explaining the concepts in painful detail, I\u0026rsquo;ll refer to papers and quote specific interesting points that provide a summary. For a detailed reading, refer to the papers and links I\u0026rsquo;ve attached.\nInstruction Fine-tuning: Why instruction fine-tuning leads to much smaller models that can perform quite well on specific tasks, compared to much larger models\nOpen source models available: A quick intro on mistral, and deepseek-coder and their comparison.\nModel Quantization: How we can significantly improve model inference costs, by improving memory footprint via using less precision weights.\nIf you know all of the above, you may want to skip to Part 2\nPart 2: Local LLM Setup\nUsing Ollama and setting up my VSCode extension\nVSCode Extension available here: https://github.com/Kshitij-Banerjee/kb-ollama-coder\nPart 3: Test Results\nShowing results on all 3 tasks outlines above.\n[Part 1] Understanding Instruction Finetuning Before we venture into our evaluation of coding efficient LLMs. Let\u0026rsquo;s quickly discuss what \u0026ldquo;Instruction Fine-tuning\u0026rdquo; really means.\nWe refer to this paper: Training language models to follow instructions with human feedback Why instruction fine-tuning ? predicting the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù\nPerformance Implications In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer paramete\nHow they did it? SpeciÔ¨Åcally, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to Ô¨Åne-tune GPT-3 to follow a broad class of written instructions. This technique uses human preferences as a reward signal to Ô¨Åne-tune our models. We Ô¨Årst hire a team of 40 contractors to label our data, based on their performance on a screening tes We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API3 and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and Ô¨Åne-tune our supervised learning baseline to maximize this reward using the PPO algorithm\nPaper Results We call the resulting models InstructGPT. {:height 636, :width 1038}\nOn the TruthfulQA benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3 During RLHF Ô¨Åne-tuning, we observe performance regressions compared to GPT-3 We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores. InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises$$\nSome notes on RLHF Step 1: Supervised Fine Tuning: We Ô¨Åne-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2\nStep 2 : Reward model Starting from the SFT model with the Ô¨Ånal unembedding layer removed, we trained a model to take in a prompt and response, and output a scalar reward The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. These reward models are themselves pretty huge. 6B parameters in Open AI case\nStep 3: Fine-tuning with RL using PPO PPO : Proximal Policy Optimization Given the prompt and response, it produces a reward determined by the reward model and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models ‚ÄúPPO.‚Äù\nFrom : https://huggingface.co/blog/rlhf \u0026ldquo;Let\u0026rsquo;s first formulate this fine-tuning task as a RL problem. First, the¬†policy¬†is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The¬†action space¬†of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the¬†observation space¬†is the distribution of possible input token sequences, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary ^ length of the input token sequence). The¬†reward function¬†is a combination of the preference model and a constraint on policy shift.\u0026rdquo; Concatenated with the original prompt, that text is passed to the preference model, which returns a scalar notion of ‚Äúpreferability‚Äù,¬†rŒ∏‚Äã. In addition, per-token probability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback‚ÄìLeibler¬†(KL) divergence¬†between these sequences of distributions over tokens,¬†r_kl. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Finally, the¬†update rule¬†is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process.\nHelpful schematic showing the RL fine-tune process Final Thoughts InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints deÔ¨Åned in the instruction (e.g. ‚ÄúWrite your answer in 2 paragraphs or less.‚Äù),\nAre less likely to fail to follow the correct instruction entirely,\nAre less likely to make up facts (‚Äòhallucinate‚Äô) less often in closed-domain tasks. These results suggest that InstructGPT models are more reliable and easier to control than GPT-3\nComparison of GPT vs Instruct GPT [Part 1] Deep dive into Mistral Models Brief introduction to Mistral models, their architecture, and key features We refer to the paper: Mistral 7b Objective: The search for balanced models delivering both high-level performance and efficiency\nKey Results: Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25 ]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20 ], without sacrificing performance on non-code related benchmarks\nKey Insights: Mistral 7B leverages grouped-query attention (GQA) [ 1 ], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost\nSliding Window Attention Why? The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most W tokens from the previous layer Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after k attention layers, information can move forward by up to k √ó W tokens SWA exploits the stacked layers of a transformer to attend information beyond the window size W . The hidden state in position i of the layer k, hi, attends to all hidden states from the previous layer with positions between i ‚àí W and i. Recursively, hi can access tokens from the input layer at a distance of up to W √ó k tokens, as illustrated in Figure 1. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately131K tokens. In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [ 11 ] and xFormers [18 ] yield a 2x speed improvement over a vanilla attention baseline. This fixed attention span, means we can implement a rolling buffer cache. After W size, the cache starts overwriting the from the beginning. This also allows some pre-filling based optimizations.\nComparison with Llama Instruction Finetuning To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B ‚Äì Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance.\nSystem prompt We introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2. The prompt: \u0026ldquo;Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.\u0026rdquo;\nHow to select various model sizes, a Thumbrule from Mistral AI. Refer to this article: Model Selection\nTL;DR\nSmall Tasks (Custom support, classification) =\u0026gt; use Mistral Small\nMedium Tasks (Data Extraction, Summarizing Documents, Writing emails.. ) =\u0026gt; Mistral medium\nComplex Tasks (Code Generation, RAG) =\u0026gt; Mistral Large\nBenchmark on coding :\nModel MMLU hellaswag (10-shot) winograde (5-shot) arc challenge (25-shot) TriviaQA (5-shot) TruthfulQA Mistral 7B 62.5% 83.1% 78.0% 78.1% 68.8% 42.35% Mixtral 8x7B 70.6% 86.7% 81.2% 85.8% 78.38% 47.5% Mistral Small 72.2% 86.9% 84.7% 86.9% 79.5% 51.7% Mistral Medium 75.3% 88.0% 88% 89.9% 81.1% 47% Mistral Large 81.2% 89.2% 86.7% 94.0% 82.7% 50.6% [Part 1] Deepseek Coder, an upgrade? Overview of Eval metrics Before we understand and compare deepseeks performance, here\u0026rsquo;s a quick overview on how models are measured on code specific tasks.\nLeaderboard is provided here : https://evalplus.github.io/leaderboard.html\nWhat is HumanEval ? https://github.com/openai/human-eval\nHumanEval consists of 164 hand-written Python problems that are validated using test cases to assess the code generated by a Code LLM in a zero-shot setting,\nWhat is MBPP ? https://huggingface.co/datasets/mbpp\nWhile the MBPP benchmark includes 500 problems in a few-shot setting.\nDS-1000: More practical programming tasks, compared to Human Eval DS-1000 benchmark, as introduced in the work by Lai et al. (2023), offers a comprehensive collection of 1,000 practical and realistic data science workflows across seven different libraries\nDeepseek coder Summary Each model in the series has been trained from scratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive understanding of coding languages and syntax. Refer to the paper from DeepSeek coder: DeepSeek Code Useful links:\nhttps://deepseekcoder.github.io/\nhttps://ollama.com/library/deepseek-coder/tags\nHow it\u0026rsquo;s built Repository Context in Pre-training Besides, we attempt to organize the pretraining data at the repository level to enhance the pre-trained model‚Äôs understanding capability within the context of cross-files within a repository They do this, by doing a topological sort on the dependent files and appending them into the context window of the LLM. More details below.\nWe find that it can significantly boost the capability of cross-file code generation\nNext token prediction + Fill-in-the middle (like BERT) In addition to employing the next token prediction loss during pre-training, we have also incorporated the Fill-In-Middle (FIM) approach.\n16K context window (Mistral models have 4K sliding window attention) To meet the requirements of handling longer code inputs, we have extended the context length to 16K. This adjustment allows our models to handle more complex and extensive coding tasks, thereby increasing their versatility and applicability in various coding scenarios\nData Preparation Filtering Rule: TL;DR: Remove non-code related, or data heavy files\nRemove files with avg line length \u0026gt; 100, OR, maximum line length \u0026gt; 1000 characters.\nRemove files with fewer than 25% alphabetic characters\nremove \u0026lt;?xml version files\nJSON/YAML files - keep fields that have character counts ranging from 50 -\u0026gt; 5000 . This removes data-heavy files.\nDependency Parsing Instead of simply passing in the current file, the dependent files within repository are parsed.\nParse Dependency between files, then arrange files in order that ensures context of each file is before the code of the current file. By aligning files based on dependencies, it accurately represents real coding practices and structures.\nThis enhanced alignment not only makes our dataset more relevant but also potentially increases the practicality and applicability of the model in handling project-level code scenarios It‚Äôs worth noting that we only consider the invocation relationships between files and use regular expressions to extract them, such as\u0026quot;import\u0026quot; in Python, \u0026ldquo;using\u0026rdquo; in C#, and \u0026ldquo;include\u0026rdquo; in C. A topological sort algorithm for doing this is provided in the paper.\nTo incorporate file path information, a comment indicating the file‚Äôs path is added at the beginning of each file.\nModel Architecture Each model is a decoder-only Transformer, incorporating Rotary Position Embedding (RoPE) Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) as described by Su et al. (2023), with a group size of 8, enhancing both training and inference efficiency. Additionally, we employ FlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism we use AdamW (Loshchilov and Hutter, 2019) as the optimizer with ùõΩ1 and ùõΩ2 values of 0.9 and 0.95. he learning rate at each stage is scaled down to‚àöÔ∏É 110 of the preceding stage‚Äôs rate Context Length:\nTheoretically, these modifications enable our model to process up to 64K tokens in context. However, empirical observations suggest that the model delivers its most reliable outputs within a 16K token range.\\\nInstruction Tuning This data comprises helpful and impartial human instructions, structured by the Alpaca Instruction format. To demarcate each dialogue turn, we employed a unique delimiter token \u0026lt;|EOT|\u0026gt;\nPerformance Surpasses GPT3.5, and within reach of GPT4\nTo evaluate the model‚Äôs multilingual capabilities, we expanded the Python problems of Humaneval Benchmark to seven additional commonly used programming languages, namely C++, Java, PHP, TypeScript (TS), C#, Bash, and JavaScript (JS) (Cassano et al.,2023). For both benchmarks, We adopted a greedy search approach and re-implemented the baseline results using the same script and environment for fair comparison. Interesting Notes Chain of thought prompting\nOur analysis indicates that the implementation of Chain-of-Thought (CoT) prompting notably enhances the capabilities of DeepSeek-Coder-Instruct models. This improvement becomes particularly evident in the more challenging subsets of tasks. By adding the directive, \u0026ldquo;You need first to write a step-by-step outline and then write the code.\u0026rdquo; following the initial prompt, we have observed enhancements in performance. This observation leads us to believe that the process of first crafting detailed code descriptions assists the model in more effectively understanding and addressing the intricacies of logic and dependencies in coding tasks, particularly those of higher complexity. Therefore, we strongly recommend employing CoT prompting strategies when utilizing DeepSeek-Coder-Instruct models for complex coding challenges.\n[Part 1] Model Quantization Along with instruction fine-tuning, another neat technique that makes LLM\u0026rsquo;s more performant (in terms of memory and resources), is model quantization\nModel quantization enables one to reduce the memory footprint, and improve inference speed - with a tradeoff against the accuracy.\nIn short, Quantization is a process from moving the weights of the model, from a high-information type like fp32 to a low-information but performant data-type like int8\nReference: Huggingface guide on quantization - https://huggingface.co/docs/optimum/en/concept_guides/quantization\nThe two most common quantization cases are¬†float32 -\u0026gt; float16¬†and¬†float32 -\u0026gt; int8.\nSome schematics that explain the concept.\n{:height 362, :width 719}\nQuantization to Int8 Let‚Äôs consider a float¬†x¬†in¬†[a, b], then we can write the following quantization scheme, also called the¬†affine quantization scheme:\nx = S * (x_q - Z) x_q¬†is the quantized¬†int8¬†value associated to¬†x\nS¬†is the scale, and is a positive¬†float32\nZ¬†is called the zero-point, it is the¬†int8¬†value corresponding to the value¬†0¬†in the¬†float32¬†realm.\nx_q = round(x/S + Z) x_q = clip(round(x/S + Z), round(a/S + Z), round(b/S + Z)) In effect, this means that we clip the ends, and perform a scaling computation in the middle. The clip-off obviously will lose to accuracy of information, and so will the rounding.\nCalibration An example, explaining calibration to optimise clipping vs rounding error\n{:height 187, :width 314}\nTo ensure that we have a good balance of clipping vs rounding errors, based on the range [a, b] that we select. Some techniques are available\nUse per-channel granularity for weights and per-tensor for activations\nQuantize residual connections separately by replacing blocks\nIdentify sensitive layers and skip them from quantization\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\nModel quantization + instruct = Quite Good results\nGood reference reading on the topic: https://deci.ai/quantization-and-quantization-aware-training\nWhat\u0026rsquo;s next This post was more around understanding some fundamental concepts, I\u0026rsquo;ll not take this learning for a spin and try out deepseek-coder model. I\u0026rsquo;m primarily interested on its coding capabilities, and what can be done to improve it.\nPart-2 of this post is available here\n","permalink":"https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms-instruction-fine-tuning-models-and-quantization/","summary":"Introduction The goal of this post is to deep-dive into LLM\u0026rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.\nNote: Unlike copilot, we\u0026rsquo;ll focus on locally running LLM\u0026rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.\nTo test our understanding, we\u0026rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings.","title":"Exploring Code LLMs - Instruction fine-tuning, models and quantization"},{"content":"Introduction Tony Fadell is CEO of nest (bought by google ), and instrumental in building products at Apple like the iPod and the iPhone.\nThe book is not about facts and science, but based on tony‚Äôs experience and deals with subjective concepts like how to build products, dealing with assholes, and how to hire etc.\nOverall, 4/5 stars for me, and I recommend reading it. It covers one strong individuals strong opinions, about how to deal with matters one must deal with when building impactful products. He certainly has some of the biggest products in the world under his belt, So i‚Äôm definitely taking notes on what this guy has to teach me.\nI\u0026rsquo;ve read enough books on entrepreneurship, business and startups - but I think this one has more specific advice than the typical 0-1 book. I\u0026rsquo;m going to mention the ones that I thought were noteworthy for me / unique views. Do read the full book if you think these resonate.\nFair warning: The summary is a bit larger than usual. That\u0026rsquo;s because the topics in each chapter are completely different, so the book packs a lot of interesting quotes and wisdom that I end up taking a note of. Feel free to skip to the topics that most interest you.\nSummary in 210 kilobytes What to work on If you‚Äôre passionate about something‚Äîsomething that could be solving a huge problem one day‚Äîthen stick with it.\nOne day, if you are truly solving a real problem, when the world is ready to want it, you‚Äôll already be there.\nHow to work with your heroes If you have a hero you want to work with, follow them, bring something interesting to them, ask interesting questions. Don\u0026rsquo;t ask for a job / meeting, but persistence and being genuinely interested and helpful will work.\nAnd if that seems impossible‚Äîif you follow your heroes on Twitter but can‚Äôt imagine they‚Äôll ever pay any attention to you‚ÄîI‚Äôm excited to tell you that that is complete bullshit. I doubt I‚Äôm anyone‚Äôs hero, but I‚Äôm an experienced, well-connected product designer who‚Äôs been lucky enough to make some famous technology. Most people assume I won‚Äôt pay any attention to people who randomly DM me on Twitter or send me unsolicited emails out of the blue. But sometimes I do. Not when people are just asking for a job. Or angling for funding. But I‚Äôll notice people who come with something interesting to share. Something smart. Especially if they keep coming. If they sent me something cool last week and something cool this week and they keep bringing fascinating news or tech or ideas and they‚Äôre persistent, then I‚Äôll start to recognize them. I‚Äôll start to remember them, and respond. And that can turn into an introduction or a friendship or a referral or, potentially, a job at one of our portfolio companies. The key is persistence and being helpful. Not just asking for something, but offering something.\nHow IC\u0026rsquo;s should grow 20 percent of the time, individual contributors need to look up. And they need to look around. The sooner they start, the faster and higher they‚Äôll advance in their career. Looking up - means looking forward in the future of your work, to make sure you\u0026rsquo;re going in the right direction\nLooking around - means making sure that other functions that you need help from, are also on track and you\u0026rsquo;re not losing rhythm with them\n\u0026ldquo;Your job isn‚Äôt just doing your job. It‚Äôs also to think like your manager or CEO. You need to understand the ultimate goal, even if it‚Äôs so far away that you‚Äôre not really sure what it‚Äôll look like when you get there\u0026rdquo;\nBeing a great manager Remember that once you become a manager, you‚Äôll stop doing the thing that made you successful in the first place. You‚Äôll no longer be doing the things you do really well‚Äîinstead you‚Äôll be digging into how others do them, helping them improve. Your job will now be communication, communication, communication, recruiting, hiring and firing, setting budgets, reviews, one-on-one meetings (1:1s), meetings with your team and other teams and leadership, representing your team in those meetings, setting goals and keeping people on track, conflict resolution, helping to find creative solutions to intractable problems, blocking and tackling political BS, mentoring your team, and asking ‚Äúhow can I help you?‚Äù all the time.\nBecoming a manager is a discipline. Management is a learned skill, not a talent. You‚Äôre not born with it.\nBeing exacting and expecting great work is not micromanagement. Your job is to make sure the team produces high-quality work.\nDon‚Äôt worry that your team will outshine you. In fact, it‚Äôs your goal. You should always be training someone on your team to do your job. The better they are, the easier it is for you to move up and even start managing managers.\nOne of the hardest parts of management is letting go. Not doing the work yourself. You have to temper your fear that becoming more hands-off will cause the product to suffer or the project to fail. You have to trust your team‚Äîgive them breathing room to be creative and opportunities to shine.\n1:1s with individuals should have an agenda, a clear purpose, and should be beneficial to both sides. You should get the info you need about product development and your team members should get insight into how they‚Äôre doing. Try to see the situation from their point of view‚Äîtalk about their fears and your own concerns out loud, reframe your thoughts so they can hear the feedback, understand the goals, clear up ambiguities or concerns.\nDealing with Assholes Types of assholes\nPolitical assholes: The people who master the art of corporate politics, but then do nothing but take credit for everyone else‚Äôs work. These assholes are typically incredibly risk averse‚Äîthey‚Äôre focused exclusively on surviving and pushing others down so they can reach the top.\nControlling assholes: Micromanagers who systematically strangle the creativity and joy out of their team. These assholes can never be reasoned with. They resent any good idea that didn‚Äôt come from them and are extremely threatened by anyone on their team who is more talented than they are.\nAsshole assholes: They suck at work and everything else. These are the mean, jealous, insecure jerks who you‚Äôd avoid at a party, but who inevitably sit immediately next to you at the office. They cannot deliver, are deeply unproductive\nMission-driven ‚Äúassholes‚Äù: The people who are crazy passionate‚Äîand a little crazy. They speak most frankly, trampling the politics of the modern office, and steamroll right over the delicate social order of ‚Äúhow things are done around here.‚Äù Much like true assholes, they are neither easygoing nor easy to work with. Unlike true assholes, they care. They give a damn. They listen. They work incredibly hard and push their team to be better‚Äîoften against their will.\nHow to deal with them\nKill ‚Äôem with kindness.\nIgnore them.\nTry to get around them.\nQuit.\nIn that order\nMost people aren‚Äôt assholes. And even if they are, they‚Äôre also human. So don‚Äôt walk into a job trying to get anyone fired. Start with kindness. Try to make peace. Assume the best. and if that doesn‚Äôt work, then remember that what goes around comes around. Although it never comes fast enough\nWhen to quit You‚Äôre no longer passionate about the mission. If you‚Äôre staying for the paycheck or to get the title you want, but every hour at your desk feels like an eternity, then save yourself.\nYou‚Äôve tried everything. You‚Äôre still passionate about the mission but the company is letting you down. So you‚Äôve talked to your manager, to other teams, to HR, and to senior leadership. You‚Äôve tried to understand the roadblocks and pitched solutions and options. But still, your project is going nowhere or your manager is impossible or the company is falling apart. In that case, you should leave that job but stick to the mission and find another team on a similar journey.\nBuilding great experiences When building 0-\u0026gt;1 products, most think about prototypes.\nBut don‚Äôt just make a prototype of your product and think you‚Äôre done. Prototype as much of the full customer experience as possible. Make the intangible tangible so you can‚Äôt overlook the less showy but incredibly important parts of the journey. You should be able to map out and visualize exactly how a customer discovers, considers, installs, uses, fixes, and even returns your product. It all matters.\nYour product isn‚Äôt only your product. It‚Äôs the whole user experience‚Äîa chain that begins when someone learns about your brand for the first time and ends when your product disappears from their life, returned or thrown away, sold to a friend or deleted in a burst of electrons.\nBuilders sometimes think only about the product, but prototyping and imagining the entire customer experience is what many miss\nStorytelling Appeal to the pain that users have to go through. Appeal to the emotional feelings aroused when you talk about the struggles they have to face to solve this particular problem.\nBy the time you describe the solution, the audience should already be engaged and physically annoyed with the problem.\nThe story of your product, your company, and your vision should drive everything you do\nWhen you describe the solution, you need to boil down the complicated concept into simple words.\nAnalogies can be such a useful tool in storytelling. They create a shorthand for complicated concepts‚Äîa bridge directly to a common experience.\nA great analogy allows a customer to instantly grasp a difficult feature and then describe that feature to others. That‚Äôs why ‚Äú1,000 songs in your pocket‚Äù was so powerful.\nEvolution vs Disruption Your version one (V1) product should be disruptive, not evolutionary.\nAssuming V1 was at least a critical success, the second version of your product is typically an evolution of your first. Refine what you made in V1 using data and insights from actual customers and double down on your original disruption. The execution should step up a notch‚Äî\nDon\u0026rsquo;t overshoot. Don\u0026rsquo;t try to disrupt too many things at once. Focus on solving one thing first, and when you\u0026rsquo;ve solved that go for another\nYou focus on making one amazing thing but forget that it has to be part of a single, fluid experience.\nHeartbeats Don\u0026rsquo;t have too many external launches, even if you can. You need to let your customers gain balance, before you throw another ball at them to catch\nHaving a well known heartbeat, both external, project and team-wise is important - because deadlines are important.\nPeople will do their most creative and constructive work, when they have a sense of timelines and urgency.\nThe chasm - Custom adoption curve Geoffrey Moore first mapped out when different people are open to new products in his book Crossing the Chasm.\nTo/read Crossing the chasm\n(Not) Killing yourself Steve jobs was obsessed with Apple. But not everyone should be Steve. Having a balance, and knowing what balance is right for you is good.\nTo manage his work and keep things in order, interestingly - he used the same GTD concept to manage work. I wrote a blog article on this recently, so do check that out if interested.\nAdditionally he made his list-of-things and projects completely public to his teams. That way everyone knew what he was focussing on. Eventually people followed and started doing the same.\nAssistants are good way for senior executives to delegate machinery work like booking meetings/emails so that the executive can stay productive . Assistants can be shared executives so save cost\nHow to handle a crisis Everyone goes through a moment of crisis and breakfppints\nKeep your focus on fixing the problem and not who to blame, that can be solved later\nGet in the weeds. Don‚Äôt worry about micromanagement . Release the pressure and let people do work once things are in control\nGet advice from mentors and investors\nConstant communication - externally and internally\nAccept responsibility as a leader and listen to feedback.\nHiring The best teams are multi generational. Experienced people have wisdom, but it takes energy to build, so you need younger people too.\nThe 3 crown strategy - Your hiring committee should have 1) The hiring manager, 2) manager¬†of eventual custom of candidate. Example: for UX designer customer would be engineers.\nWhat makes a good recruiter: People who are actually excited about the product, and can create and tell a good story\nImportant questions to ask: Why you left the previous company, and why you\u0026rsquo;d like to join this one ? It needs to be a compelling and flowing story.\nAdvise on onboarding new people - They need a lot of help from the manager. Show them how to work in the environment and what the culture is - so they don‚Äôt feel lonely and feel scared.\nYou‚Äôll also need to fire people. Don‚Äôt be scared of it, but don‚Äôt be callous, either. Give people plenty of warning and opportunity to course correct, follow the letter of the law, then bite the bullet and help them find a better opportunity.\nBreakpoints Breakpoints are intentional and planned points in a companies journey, where the company must pause and change things about the way they work.\nTypically these arise as more and more people are joining and the company is growing. The old ways of working don\u0026rsquo;t work anymore and change is needed.\nA communication strategy becomes very important as the team and company scales. You need effective ways to articulate leadership vision throughout.\nAs your company increased, some individuals and teams will have to focus instead of go at breadth. This will be hard to do but is important to keep functions working great .\nMeetings They become too much and overwhelming . But important .\nManagers have to keep track of how many and how much time is spent and optimise it\nCulture Is the hardest thing to pinpoint and the hardest to preserve. Even\nSo to preserve what you love, have your team write down the things they value most and build a plan to continue them.\nCulture arises organically but then needs to be codified to be maintained.\nAlways remember that change is growth and growth is opportunity. Your company is an organism; its cells need to divide to multiply, they need to differentiate to become something new. Don‚Äôt worry about what you‚Äôre going to lose‚Äîthink about what you‚Äôre going to become.\nDesign Thinking Design is not just a profession. A customer is not only a person who buys something. A product is not just a physical object or software that you sell. You can employ design thinking for everything you do.\nDesign thinking forces you to really understand the problem you‚Äôre trying to solve. In this case, the problem isn‚Äôt ‚ÄúI need a car to get to work.‚Äù It‚Äôs actually much broader: ‚ÄúHow do I want to get around?‚Äù The product you‚Äôre designing is a mobility strategy for your life.\nLiterally the only way to make a really good product is to dig in, analyze your customer‚Äôs needs, and explore all the possible options (including the unexpected ones: maybe I can work from home, maybe I can move closer to work). There are no perfect designs. There are always constraints. But you choose the best of all the options‚Äîaesthetically, functionally, and at the necessary price point. That‚Äôs a design process.\nAvoid Habituation:\nMost people are so habituated to the problems in their home lives or work that they no longer realize they‚Äôre problems. You can‚Äôt solve interesting problems if you don‚Äôt notice they‚Äôre there.\nProduct Marketing Method Marketing cannot just be figured out at the very end. Use marketing to prototype your product narrative from the beginning.\nThe creative team can help you make the product narrative tangible. This should happen in parallel with product development‚Äîone should feed the other.\nThe best marketing is just telling the truth. The ultimate job of marketing is to find the very best way to tell the true story of your product.\nMessaging Architecture: To make the story feel real, to make it tangible, you need to visualize it.\nIt‚Äôs essential for product development: Product management and marketing work on the messaging architecture from day one.\nIt\u0026rsquo;s a living and shared document.\nThe process of convincing someone to buy and use your product needs to respect the customer, needs to understand their needs at different points of the user experience. You\nExample from Nest:\nTop-line billboards would just introduce the idea of a new kind of thermostat. The packaging would highlight the top six features and how the product connects to your phone. The website would emphasize energy savings and showcase how Nest fits into your daily life. The usage guide inside the packaging would provide more detail about how to train the learning algorithm and tips for saving energy. The support site would go deeper with exact instructions and thorough explanations of all the features.\nThe point of PM\u0026rsquo;s In many organisations PM\u0026rsquo;s starting becoming project managers, and in some they start focusing on the marketing/sales aspect more. These are typically anti-patterns\nThe point of PM\u0026rsquo;s is to be an architect of the product. They are someone that sees across the Sales, marketing, product development and support aspects and is able to guide the team on what needs to be done next.\nThe product manager has to be a master negotiator and communicator. They have to influence people without managing them. They have to ask questions and listen and use their superpower‚Äîempathy for the customer, empathy for the team‚Äîto build bridges and mend road maps. They have to escalate if someone needs to play bad cop, but know they can‚Äôt play that card too often. They have to know what to fight for and which battles should be saved for another day. They have to pop up in meetings all over the company where teams are representing their own interests‚Äîtheir schedules, their needs, their issues‚Äîand stand alone, advocating for the customer. Amazing product managers usually emerge from other roles. They start in marketing or engineering or support, but because they care so deeply about the customer, they start fixing the product and working to redefine it, rather than just executing someone else‚Äôs spec or messaging. And their focus on the customer doesn‚Äôt cloud their understanding that ultimately this is a business‚Äîso they also dive into sales and ops, try to understand unit economics and pricing.\nSales Culture Don\u0026rsquo;t do the typical model of giving sales people commission once the sale is closed. That creates short-term incentives for the sales team. Treat them with long term stock options and good salary\nBeing A CEO It\u0026rsquo;s a poison to think that all good ideas only come from you. You need to seek ways to bring the best ideas to life.\nIt\u0026rsquo;s also a trap to get into the \u0026ldquo;only things invented here\u0026rdquo; syndrome.\nSteve jobs rejected android before google bought it. Had Steve jobs bought android, our lives might be very different.\nIf you aren\u0026rsquo;t failing = you aren\u0026rsquo;t making progress. So embrace it\nConclusion Only two things matter in the end - \u0026ldquo;What you build\u0026rdquo;, and \u0026ldquo;Who you build it with\u0026rdquo;\nYou might get lucky, and the thing you\u0026rsquo;re building succeeds. But luck does play a big role, so some failure is inevitable\nWhen you\u0026rsquo;re building, don\u0026rsquo;t just aim to make your users life better - aim to give them superpowers.\nMentoring, is probably the most enriching and meaningful thing you will do. In fact, you will end up learning much more from your mentees, than you will teach them.\n","permalink":"https://Kshitij-Banerjee.github.io/2024/02/24/build-tony-fadell/","summary":"Introduction Tony Fadell is CEO of nest (bought by google ), and instrumental in building products at Apple like the iPod and the iPhone.\nThe book is not about facts and science, but based on tony‚Äôs experience and deals with subjective concepts like how to build products, dealing with assholes, and how to hire etc.\nOverall, 4/5 stars for me, and I recommend reading it. It covers one strong individuals strong opinions, about how to deal with matters one must deal with when building impactful products.","title":"Build - Tony Fadell"},{"content":"Introduction I was first introduced to the concept of \u0026ldquo;second-brain\u0026rdquo; from Tobi Lutke, the founder of Shopify. The topic started because someone asked whether he still codes - now that he is a founder of such a large company. Tobi went on to explain that he spent the weekend writing some code to customise Logseq to his preferences, and that he\u0026rsquo;s an active member of the Logseq community.\nThe following weekend, I setup Logseq and learnt its weird ways of working, and have since been an ardent user and fan of the Logseq/Obsidian methodology of building a \u0026ldquo;second-brain\u0026rdquo;\nWhile his description of the second brain was brief, the essence of it struck a cord with me. Its one of those concepts that sounds obvious after you hear it, but we all overestimate our capabilities, and usually fail to consistently apply what we already know.\nOver the last 3 years, Logseq has become a key part of my personal workflow, and has helped me structure my thoughts, access old information that I had completely forgotten about, and is also a place where I manage all my tasks and to-do lists. I\u0026rsquo;ve found surprising usages of it, and I\u0026rsquo;m thankful to Tobi for introducing it to everyone at Shopify.\nHere\u0026rsquo;s logseqs render of my second brain that its keeping up to date for me.\nSecond Brain - The core concept Our brains are creative, imaginative and full of amazing ideas. Ideas come to us all the time - When we\u0026rsquo;re in the shower, while having our morning coffee, reading the news, or while talking to friends.\nBut, our brains aren\u0026rsquo;t very good at retaining these ideas - and inevitably we forget about things that we once read / thought / learnt.\nLogseq, Obsidian and Roam Research - are all great tools that are built to facilitate easily capturing our thoughts and ideas into a digital second-brain.\nAs opposed to our human brains, this second brain isn\u0026rsquo;t very creative or imaginative (atleast yet), but its great at collecting and organising our information in structured storage, so that our creative brains have a way to access information that we once read/thought/learnt.\nAdditionally - If we\u0026rsquo;re able to classify your thoughts consistently, certain patterns and similarities start emerging from them as well.\nLogseq makes it really easy to capture thoughts quickly, into a running journal that flips to a new page daily Just like hash-tags, any idea that you tag with a hashtag - becomes easily searchable, and every hash-tag gets its own page to store linked and unlinked information.\nI\u0026rsquo;ve now been using Logseq for almost 3 years, and have found myself building my own tagging strategies over the years. I\u0026rsquo;d say my tagging/classification strategy is rudimentary, where I essentially create a new hash-tag whenever some new concept has emerged in life. This is great, but lacks structure. This is where the book \u0026ldquo;Getting things done\u0026rdquo; connected the dots for me and I\u0026rsquo;ve found a better way to structure, classify and process information in my second brain. I\u0026rsquo;ll talk about how to apply that methodology soon.\nThe curse of the never ending TODO list Another great feature of logseq, is that its super easy to track items as TODO / DOING / DONE, so Logseq also starts serving as the defacto tool where all my TODO items go.\nThe problem though, is that at times the TODO list just becomes a dumpyard of items and I never even go through all the TODO items in my backlog.\nGetting Things Shit Done - The art of stress free productivity I\u0026rsquo;m always reading books, and never miss a chance to ask friends and colleagues for book recommendations when I can.\nSome people vehemently dislike books in the category of \u0026ldquo;Self Improvement\u0026rdquo;, because they tend to say obvious things and keep repeating the same concepts. While I agree that that is quite often the case, but I also think that we\u0026rsquo;re wired to \u0026ldquo;know-but-not-do\u0026rdquo; things in general, and these books serve as reminders, and occasionally the examples end up firming up the idea in us.\nThe book \u0026ldquo;Getting things done - the art of stress free productivity\u0026rdquo; by David Allen, was one of these recommendations that piqued my curiosity when they described that the book is about bringing structure to our never ending list of todo items.\nThe books promise: Being able to work clearly and constructively all while being in control - even when there are a ton of things to do which might otherwise be overwhelming.\nWhile I had heard of this book before, I never actually read the book - and giving a quick glance at its good-read rating it passed my book-filter (Good reads rating \u0026gt;= 4). I was also introduced to the Libby app and that the Singapore public library has this book as an audio book available. Since this was a discussion on a work trip, I spent the rest of the long flight listening to the book.\nLogseq - For Getting Shit Done ? What struck me from the book is that the core idea of the author (which is repeated and dragged IMO for 50% of the book), is that we have too many \u0026ldquo;things to do\u0026rdquo; that we end up retaining in our brain. Without a good structure in place, our minds have to deal with the stress of \u0026ldquo;having too much to do\u0026rdquo;.\nThis constant feeling of, I have a lot to do, and many things are pending stresses us out internally and distracts us from actually doing the productive work.\nAdditionally, since we have limited storage capacity in our brains - we will inevitably forget some things, and this will further increase stress and panic.\nIf I had to boil this book down to 3 key principles, they are\n#1 - Maintain a list of \u0026ldquo;Projects\u0026rdquo;.\nThese can be personal, or professional - but its important to keep a list of running \u0026ldquo;Projects\u0026rdquo;, each with its clear and independent purpose.\n#2 - Every project must have a clear \u0026ldquo;Next Action\u0026rdquo;\nEvery project needs to have a clear and well thought through - \u0026ldquo;Immediate Next Action\u0026rdquo;\nWhen we\u0026rsquo;re working, the key is to be able to see ALL your \u0026ldquo;Next actions\u0026rdquo; across projects, so you can pick from them and be productive\nProjects need to be broken down into workable actions, but there is only one \u0026ldquo;Immediate next action\u0026rdquo;. At times you may have a couple, but being clear on exactly what next to do - is key.\n#3 - Review - Reflect - Regularly\nA constant cadence of reviewing, reflecting and updating items regularly will keep our \u0026ldquo;Projects\u0026rdquo; and \u0026ldquo;Next Actions\u0026rdquo; list in order.\nBy doing these 3 things, our mind is freed from retaining and juggling around all these open-loops, and can focus on picking up the next available action and focussing on completing them.\nDoesn\u0026rsquo;t this sound very similar to the idea of \u0026ldquo;Second Brain\u0026rdquo; ? A place where you keep all your ideas and thoughts neatly structured and tagged against their own hash-tags ? I was curious how I could implement the strategy with Logseq.\nSome great summary points from the book Before we dig into my Logseq implementation, let me give you a few quick points that I thought were great from the book.\nHow to think about \u0026ldquo;Project\u0026rdquo;: A project must have a clear and concise\ni. Purpose (The Why)\nii. A vision of end state (The what, possibly visual),\niii. a set of goals, and\niv. a clear set of next actions to achieve them.\nA project can be personal, or professional. Everything that has the capacity to cause stress to your brain, should be stored in a Project. The purpose is to declutter your brain.\nThe 2 minute rule: (everyone knows it, but needs a quick reminder). How to decide on the \u0026ldquo;Action\u0026rdquo; for each item that comes in:\nIf it takes less than 2 mins: Do it right now\nIf it is better done by someone else: Delegate it\nIf it takes more than 2 minutes, but needs to be done: Defer and add it to the \u0026ldquo;IN\u0026rdquo; bucket of a \u0026ldquo;Project\u0026rdquo;\nIf something needs to wait, or can only be done later - Your calendar, is an equally important place to keep your scheduled items\nIt\u0026rsquo;s important to have a clear and dedicated space, for your brain to focus on work mode. Never shy in setting up a clear and clean workdesk, both at home and work\nThe process in visuals\nIn Summary\nWhen a new task comes in: Add it to the \u0026ldquo;IN\u0026rdquo; of a project\nWhen you\u0026rsquo;re working: Review all the \u0026ldquo;Next Actions\u0026rdquo; across projects\nOn a weekly basis, review all \u0026ldquo;IN\u0026rdquo; items and add them to your \u0026ldquo;Next actions\u0026rdquo; of the project.\nImplementing it with Logseq The great thing about logseq, is that it easily allows you to create new tasks, and \u0026ldquo;Tag\u0026rdquo; them to specific projects\nGiven that you\u0026rsquo;re journaling the items on a regular basis, you don\u0026rsquo;t have to find where the project is every time. You can simply write it down in your daily journal, and the act of adding a \u0026ldquo;Tag\u0026rdquo;, with the double [ syntax, automatically links the item to your Project\nHowever the task of scanning through your currently active \u0026ldquo;Next actions\u0026rdquo; is what isn\u0026rsquo;t trivial to achieve.\nThankfully, Logseq has a concept of \u0026ldquo;default-queries\u0026rdquo; that we can use, along with the task schema\nThe end result, we\u0026rsquo;re achieving, is that you can simply tag action items into your daily journal - and also see an overview of all the \u0026ldquo;next\u0026rdquo; actions in your logseq home page as well.\nHere\u0026rsquo;s how to achieve this.\nChange your default config.edn to follow the TODO / DONE schema, instead of the LATER / NOW / DONE format To achieve this, go to Settings \u0026gt; General \u0026gt; Custom configuration | and click on \u0026ldquo;Edit config.edn\u0026rdquo;\nChange the preferred workflow to :todo :preferred-workflow :todo Change the default-queries section in your config.edn, to the following :default-queries {:journals [{ :title \u0026#34;üî®Working On\u0026#34; :query (and (page-tags [[project]]) [[Up Next]] ) :collapsed? true } ]} (optional) Create a \u0026ldquo;Projects\u0026rdquo; page, and list down all the projects in your life.\nFor each projects page, tag the project page as \u0026ldquo;Projects\u0026rdquo;. You do this, by adding a line at the to in each logseq page\ntags:: project For each Project page, create a \u0026ldquo;IN\u0026rdquo; section, and an \u0026ldquo;Up Next\u0026rdquo; section.\nIn your reviews (recommended weekly), for each project, place your next actions in the \u0026ldquo;Up Next\u0026rdquo; section\nIf you did all the above correctly, you should see a \u0026ldquo;Working on\u0026rdquo; section in your home page and you can expand it to see all the \u0026ldquo;Next actions\u0026rdquo; in one go.\nConclusion Building a second brain, can also help you Get-things-done. The book provides a way to structure your tasks, to offload things from your brain, and maintain a consistent productive flow.\nLogseq is a great tool, to capture all the open-loops in our life, and provide a way to query and process them when we\u0026rsquo;re feeling productive\nIf you\u0026rsquo;re not convinced about how great a tool Logseq is yet, here\u0026rsquo;s something interesting\nI wrote this blog completely in Logseq, and have a script that exports it into Hugo and hosts it via github automatically.\n(Item that goes into IN of Project BLOG) - TODO: Write a post on how to use Logseq to Blog with Hugo.\nLogseq gives you wings. Try it: https://logseq.com/\nAdditional Resources I found a video online from logseqmastery.com on a similar topic. I think it has some great ideas\n{{video(https://www.youtube.com/watch?v=nieOiG8LGa0)}}\n","permalink":"https://Kshitij-Banerjee.github.io/2024/02/16/getting-things-done-with-logseq/","summary":"Introduction I was first introduced to the concept of \u0026ldquo;second-brain\u0026rdquo; from Tobi Lutke, the founder of Shopify. The topic started because someone asked whether he still codes - now that he is a founder of such a large company. Tobi went on to explain that he spent the weekend writing some code to customise Logseq to his preferences, and that he\u0026rsquo;s an active member of the Logseq community.\nThe following weekend, I setup Logseq and learnt its weird ways of working, and have since been an ardent user and fan of the Logseq/Obsidian methodology of building a \u0026ldquo;second-brain\u0026rdquo;","title":"Getting Things Done with LogSeq"},{"content":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered transformers via the original paper \u0026ldquo;Attention is all you need\u0026rdquo; that brought the innovation that made all this progress possible.\nThis post will focus on GPT-3 and its predecessors GPT-1 and 2. The progression from GPT 1,2 and finally to 3, explains how the authors found a way to generalise the transformer architecture to task-agnostic workloads, and what led to the discovery of the GPT-3 175B parameter model.\nI intend for this to be summary of the original papers, and do refer to the detailed results sections in the paper itself\nPapers previously covered Previously Covered Transformers, following¬†Vaswani et al. 2017 Google¬†Attention is all you need Papers covered in this post GPT-1, following Radford et al. 2018 Improving Language Understanding by Generative Pre-Training GPT-2, following Radford et al. 2018 Language Models are Unsupervised Multitask Learners GPT-3: Few shot learners, 2020 OpenAI Language Models are Few-Shot Learners GPT-1 \u0026amp; 2 While transformers found an effective way to utilise raw textual content to solve tasks like machine translation, there was still no consensus on the most effective way to transfer these learned representations to any other target task.\nExisting techniques involved a combination of task-specific model architectures, or using intricate learning schemes.\nGPT-1 paper, explores a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and a supervised fine-training.\nThe primary goal, is to find a universal representation that transfers with little adaptation to a wide range of tasks.\nPre-training The pre-training phase is quite similar to what we previously covered in the transformers paper\nThe difference, being that the authors use a decoder only architecture (as there is no translation involved here, only predicting the next token)\nFine Tuning This is also quite similar to the previous stage, with some key differences.\nThe core motivation is, that instead of predicting the next token, the model is supposed to predict the output Y.\nSo, a pre-labeled dataset C is chosen, where given the input in the form of tokens X_1 \u0026hellip; X_n, the model is evaluated to predict y\nTo do this, an additional linear output layer with parameters W_y is used to predict the final y\nAuxilary objectives Additionally, they add an auxiliary objective (language modelling), to the mix, as this helps with generalisation.\nTo do this, they combine the Loss functions of the pre-training with the auxiliary task\nTask Specific Input Transformations As described from the paper, with some modifications to the input they are able to use the same model for different tasks.\nwe convert structured inputs into an ordered sequence that our pre-trained model can process.\nTo explain with an example, consider how they change the inputs for sentence similarity tasks\nFor similarity tasks, there is no inherent ordering of the two sentences being compared. To reÔ¨Çect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations hm l which are added element-wise before being fed into the linear output layer\nGPT-2 Primarily it is a larger model (1.5B) with a few additional modificaitons\nLayer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the Ô¨Ånal selfattention block. A modiÔ¨Åed initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/‚àöN where N is the number of residual layers.\nThe vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used\nGPT-3 - Language Models are Few-Shot Learners The motivation A major limitation to this approach is that while the architecture (GPT-1/2) is task-agnostic, there is still a need for task-speciÔ¨Åc datasets and task-speciÔ¨Åc Ô¨Åne-tuning: to achieve strong performance on a desired task typically requires Ô¨Åne-tuning on a dataset of thousands to hundreds of thousands of examples speciÔ¨Åc to that task. Removing this limitation would be desirable, for several reasons\nHumans do not require large supervised datasets to learn most language tasks ‚Äì a brief directive in natural language (e.g. ‚Äúplease tell me if this sentence describes something happy or something sad‚Äù) or at most a tiny number of demonstrations (e.g. ‚Äúhere are two examples of people acting brave; please give a third example of bravery‚Äù) is often enough to produce satisfactory results.\nCan machines learn like humans do ? With few-shots ? What if we give few examples to the model, to come with the answer like humans learn. Will that work?\n‚ÄúIn-context learning‚Äù, uses the text input of a pretrained language model as a form of task speciÔ¨Åcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\nWhile initially, the results were not at par with the fine-tuning approach. The authors believed there is hope, as they see a linear trend of improvement with increased model sizes. The authors hence hope that increased model size, would also help with the \u0026ldquo;in-context\u0026rdquo; learning capabilities, to bring them at par.\nSince in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale\nTo test this, the authors built a 175B parameters model, and measured its in-context learning abilities.\nThey test in the following conditions\n(a) ‚Äúfew-shot learning‚Äù, or in-context learning where we allow as many demonstrations as will Ô¨Åt into the model‚Äôs context window (typically 10 to 100), (b) ‚Äúone-shot learning‚Äù, where we allow only one demonstration, and (c) ‚Äúzero-shot‚Äù learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditionalÔ¨Åne-tuning setting, but we leave this to future work\nModel architecture We use the same model and architecture as GPT-2 [ RWC+19 ], including the modiÔ¨Åed initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ].\nResults The authors then show a variety of results on various tasks like news article generation, story closure, Translation, Common sense reasoning, Reading comprehension. The results are fascinating, and are best read from the paper directly than me repeating them here.\nConclusion The success of ChatGPT is something we see now, but clearly it was years in the making and required ruthless process of discovery and experimentaion.\nMy personal takeaways are\nThe authors formulated a varied set of \u0026ldquo;tests\u0026rdquo; along with a large pool of training data. They evaluated the output across translation, comprehension, question answering etc.\nThe authors followed a clear hypothesis, tests, result method - and for the most part tested a limited set of parameters in each test. In the GPT-3 example, they are majorly testing size of the model compared with performance.\nThe ability to question ones own achievements. Even though they achieved great results from their GPT 1 and 2 models, they found issues in the fine-tuning approach and were able to pivot.\nI\u0026rsquo;m next interested in, models that take visual inputs, and how the world is reaching GPT-4V (visual) multi-modal techniques.\n","permalink":"https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/","summary":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered transformers via the original paper \u0026ldquo;Attention is all you need\u0026rdquo; that brought the innovation that made all this progress possible.\nThis post will focus on GPT-3 and its predecessors GPT-1 and 2.","title":"Understanding GPT 1, 2 and 3"},{"content":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don\u0026rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here.\nThis post will focus on the \u0026ldquo;Attention is all you need\u0026rdquo; paper that introduced the ground-breaking transformer architecture to the world and has since started a cascading and exponential affect on the AI landscape.\nPapers to be covered in this series [THIS POST] Transformers, following¬†Vaswani et al. 2017 Google¬†Attention is all you need GPT-1, following Radford et al. 2018 Improving Language Understanding by Generative Pre-Training GPT-2, following Radford et al. 2018 Language Models are Unsupervised Multitask Learners BERT, following Devlin et.al. 2019 Google Pre-training of Deep Bidirectional Transformers for Language Understanding RoBERTa, following Liu et. al. A Robustly Optimized BERT Pretraining Approach GPT-3: Few shot learners, 2020 OpenAI Language Models are Few-Shot Learners PaLM: following Chowdhery et al. 2022 Scaling Language Modeling with Pathways Maybe: MACAW-LLM, following Lyu et al. 2023 MULTI-MODAL LANGUAGE MODELING Transformers Paper Transformers, following¬†Vaswani et al. 2017 Google¬†Attention is all you need The problem its solving This inherently sequential nature (of RNNs) precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples\nIntention In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\nArchitecture Main Points Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nIts helpful to see the additive attention that was described previously in the paper by Dzmitry Refer to the diagram above from the paper by Dzmitry . In it, attention is realised by creating a context vector C that is generated via an alignment model. The model has weights alpha[tx,ty] that act as weighted sum on the states h[j] of the encoded sentence. This helps provide an \u0026ldquo;attention\u0026rdquo; mechanism.\nI believe the authors are summarising this behaviour by explaining attention as a query + key-value pairs =\u0026gt; output.\nThe query in this case, is the alpha vector that understands which parts of the X[t] to query. The values are the hidden-states h[j], and the keys are the time/positions that relate to that value h.\nIn affect, the attention mechanism is a way for the decoder network to query the positionally encoded hidden states, based on the current state s[t-1]\nLater in the paper, they also mention the following:- In \u0026ldquo;encoder-decoder attention\u0026rdquo; layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models\nBut note that the previous paper relies on an RNN to create the context of time, which the current papers want to get rid of. So how do we create the keys then?\nAnother helpful visualisation of attention, is found in the paper Massive Exploration of Neural Machine Translation Architectures\nScaled Dot-Product Attention The authors hint that they prefers the multiplicative attention mechanism, due to its computational effeciencies - even though historically the additive attention has proven to work better.\nAdditive vs Multiplicative Attention While the transformers paper doesn\u0026rsquo;t explain the difference between the additive and multiplicative versions. The referenced paper can be expanded to understand them.\nEquation 6 is the additive version, and 7 is the multiplicative version\nThe multiplicative attention is introduced here by Luong et al.\nThe authors hypothize that the multiplicative attention has underperformed as it moves the logits into extreme ends where the gradients are close to 0. So they choose to scale down the logits before passing them to the softmax.\nWe compute the dot products of the query with all keys, divide each by ‚àödk, and apply a softmax function to obtain the weights on the values\nMathematically This is similar to doing the weighted sum on the values, where the weights are a softmax outputs from aligning the queries and the keys via a dot-product Visually Multi-Head Attention Further, the authors propose to do multi-head attention. This is essentially a way to parallelise the attention process on multiple heads instead of a single head.\nSo instead of doing a single attention with d_model dimensions. They, parallely run N attention models with d_model/N dimensions each.\nThe reason for doing this?\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMathematically: Self Attention Self attention, is essentially where the attention is given to itself, rather than a separate encoder model.\nThey use this in the encoder. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nPositional Encoding Since the authors completely got rid of the recurrence, or convolutional parts in the network - they need to provide the model with the positional information to compensate for this missing and crucial context.\nTo that effect, they chose to create positional embeddings (with the same dim size as the text embeddings).\nBut, they chose to not make them learnable parameters - and that makes sense to me.\nThey create the positional embeddings with the following logic\nWe\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nThe d2l.ai book has the best explanation to this that I could find.\nIf we plot different columns, we can see that one can easily be transformed into the other, via linear transformations.\nEven after this though, I don\u0026rsquo;t think I fully understand this part well. For now, I\u0026rsquo;ve marked this as a TODO, and will come back to it later.\nWhy Self-Attention The core of this goes back to the original intention described towards the beginning of the paper.\nAs a reminder\nThis inherently sequential nature (of RNNs) precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples\nThe authors detail this by comparing the complexity of different layers, and also how traversing the path for finding long-range dependencies is easy with attention, but relatively complex in other forms.\nBelow is the tabular version for comparison\nFew key points Comparison with convolutions\nIn convolutions, long range dependencies would require a stack of N/k convolutional layers. Traversing such a path, hence takes Log_k(n). They are generally more expensive then recurrent layers by a factor of k in terms of complexity Comparison with recurrence\nThe core win here, is that recurrent connections require n sequential operations, which becomes O(1) with self attention Attention is also more interpretable\nThe authors are able to build attention distributions on the model, to realise that the model is relatively easier to reason about the relationship between positions and tokens. Conclusion The paper packs a ton of things into it. Its brilliant, but also probably takes a few iterations to absorb all the content well.\nI intend to dive into the code that is available in tensor2tensor and update this post with more understanding and learnings from the code.\nIn the next post, I intend to cover GPT-1 and 2 and work our way towards the GPT-3 and other state-of-the-art model architectures and additions.\n","permalink":"https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/","summary":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don\u0026rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here.","title":"Understanding GPT - Transformers"},{"content":"Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I\u0026rsquo;ve always liked to ground myself with foundational knowledge¬†on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.\nInspired by Andrej Karpathy\u0026rsquo;s enlightening \u0026lsquo;makemore\u0026rsquo; series, this post aims to dive deep into the key academic papers that shaped our current landscape of language models. From Recurrent Neural Networks (RNNs) to Transformers, let\u0026rsquo;s demystify these complex concepts together.\nAs of the time of this writing, Andrej hasn\u0026rsquo;t updated his series in the last 6 months. This leaves a gap in our comprehension as the series jumps from WaveNets to Transformers and GPT. Hence, I\u0026rsquo;d like this blog to act as a bridge, filling the void for anyone on a similar journey of understanding. Rest assured, when Andrej completes his series, it will serve as a comprehensive resource. Meanwhile, let me summarise as best as I can.\nPapers NOT be going through The following papers are ones, that andrew has explained in a lot of detail in his make more lecture series on YouTube , and I would recommend anyone to go through the series - as its the best explanation I\u0026rsquo;ve seen so far.\nBigram (one character predicts the next one with a lookup table of counts)\nMLP, following¬†Bengio\nCNN, following¬†DeepMind WaveNet 2016\nPapers intend to deep-dive into: RNN , following¬†Mikolov et al. 2010 Recurrent neural network based language model Bidirectional RNN, following Mike et al 1997 paper Backpropagation through time , followed in Mikael Bod ÃÅen 2001 BPTT LSTM , following¬†Graves et al. 2014 Generating Sequences With Recurrent Neural Networks GRU , following¬†Kyunghyun Cho et al. 2014 On the Properties of Neural Machine Translation: Encoder‚ÄìDecoder Batch Normalisation, following Sergey Ioffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Layer Normalization, following Jimmy Lei Ba, 2016 Layer Normalization Attention, following Dzmitry Bahdanau, 2015 Dzmitry Bahdanau, 2015 Transformers , following¬†Vaswani et al. 2017 Attention Is All You Need Let\u0026rsquo;s get started RNN - Recurrent Neural Networks Paper:¬†Mikolov et al. 2010\nSummary The primary challenge that this paper addresses is sequence prediction: given X tokens of a sequence, predict the X+1th token. While the bigram and MLP papers solved this by feeding some fixed context-length to predict the next token, they had their shortcomings - namely fixed and manually set context lengths. To overcome these, the authors propose how a recurrent neural network can \u0026ldquo;figure-out\u0026rdquo; the context length instead of manually setting it.\nThe proposed RNNs, can \u0026ldquo;build a context\u0026rdquo; of information from the past and incorporate it into their predictions. This feature allows RNNs to capture dependencies between elements in a sequence, making them especially suited for tasks involving sequential data.\nThe problem, in the words of the author:- It is well known that humans can exploit longer context with great success. Also, cache models provide comple-\nmentary information to neural network models, so it is natural to think about a model that would encode temporal information\nimplicitly for contexts with arbitrary lengths\nThe solution The authors then explain how a simple recurrent neural network works\nWhere\nw(t) is the input word at t\ns(t-1) is the state previously generated by the RNN in its last time-step\nOutput layer y(t) represents probability distribution of next word given previous word w(t) and context. Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.\nBack-propagation through time (BPTT) algorithm is used. (This is covered next)\nHow do you back-propagate through the loop ? Backpropagation through time , followed in Mikael Bod ÃÅen 2001 BPTT The key insight is around how to back-propagate through the recursion caused loop\nThe solution is to \u0026ldquo;unroll\u0026rdquo; the model T times, and then follow normal backpropation\nInstead, of keeping separate weight matrix for each time-step, the weight matrix is instead shared across the unfolded layers.\nNote, how weights V and U , remain the same through the unfolding process\nImportant quotes from the paper:\nIt is important to note, however, that after error deltas have been calculated, weights are folded back adding up to one big change for each weight. Obviously there is a greater memory requirement (both past errors and activations need to be stored away), the larger œÑ we choose.\nIn practice, a large œÑ is quite useless due to a ‚Äúvanishing gradient effect‚Äù (see e.g.\n(Bengio et al., 1994)). For each layer the error is backpropagated through the error\ngets smaller and smaller until it diminishes completely. Some have also pointed out that the instability caused by possibly ambiguous deltas (e.g. (Pollack, 1991)) may disrupt convergence. An opposing result has been put forward for certain learning tasks (Bod ÃÅen et al., 1999).\nNote: Batch normalization and layer normalization were probably not present at this time.\nNotable lines from the paper:-\nBased on our experiments, size of hidden layer should reflect amount of training data - for large amounts of data, large hidden layer is needed\nConvergence is usually achieved after 10-20 epochs.\nregularization of networks to penalize large weights did not provide any significant improvements.\nPyTorch\nCode Doc\nInput:\n(N,L,H**in‚Äã)¬†when¬†batch_first=True\nN = Batch Size\nL = Sequence Length\nH_in = Hidden Layer\nBidirectional RNN Paper: Mike et al 1997 paper Future input information coming up later than is usually also useful for prediction. With an RNN, this can be partially\nachieved by delaying the output by a certain number of time frames to include future information. While delaying the output by some frames has been used successfully to improve results in a practical speech recogni-\ntion system [12], which was also confirmed by the experiments conducted here, the optimal delay is task dependent and has to be found by the ‚Äútrial and error‚Äù error method on a validation test set.\nTo overcome the limitations of a regular RNN outlined in the previous section, we propose a bidirectional recurrent\nneural network (BRNN) that can be trained using all available input information in the past and future of a specific time frame.\nLSTM - Long Short-term Memory Paper: Graves et al. 2014 Generating Sequences With Recurrent Neural Networks\nSummary\nQuoting the paper to best describe the problem they are addressing\nIn practice however, standard RNNs are unable to\nstore information about past inputs for very long [15]. As well as diminishing\ntheir ability to model long-range structure, this ‚Äòamnesia‚Äô makes them prone to\ninstability when generating sequences. The problem (common to all conditional\ngenerative models) is that if the network‚Äôs predictions are only based on the last\nfew inputs, and these inputs were themselves predicted by the network, it has\nlittle opportunity to recover from past mistakes. Having a longer memory has\na stabilising effect, because even if the network cannot make sense of its recent\nhistory, it can look further back in the past to formulate its predictions.\nIn my words, I understand it as follows\nThe forget gate, tries to find how much to forget in the next iteration. The network learns weights, such that for certain inputs x, at hidden states h and a previous cell state c[t-1] - it predicts how to forget in the next iteration\nSimlarly, the input gate learns how much to store in the new cell state at t.\nCombining both, the new c[t] is a Fc[t-1] + I(WX+Wh+b)\nThe paper has some great examples of text generation and handwriting prediction using LSTMs, that I would encourage going through.\nPaper 2 by Google: hasim et al. 2014 hasim et all The recurrent connections in the\nLSTM layer are directly from the cell output units to the cell input\nunits, input gates, output gates and forget gates. The cell output units\nare connected to the output layer of the network.\nPyTorch:\nReference: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\nGRU - Gated Recurrent Neural Networks Paper:\nCho et. al 2014 Learning Phrase representations using Encoder-Decoder On the Properties of Neural Machine Translation: On the Properties of Neural Machine Translation: Encoder‚ÄìDecoder paper From the Papers:\nIn addition to a novel model architecture, we also\npropose a new type of hidden unit (f in Eq. (1))\nthat has been motivated by the LSTM unit but is\nmuch simpler to compute and implement.1 Fig. 2\nshows the graphical depiction of the proposed hidden unit.\nWe show that the neural machine translation performs\nrelatively well on short sentences without unknown words,\nbut its performance de-grades rapidly as the length of the sentence\nand the number of unknown words increase.\nFurthermore, we find that the pro-posed gated recursive convolutional net-\nwork learns a grammatical structure of a sentence automatically.\nIn my words, they simplified the task to an update gate and a reset gate, instead of the complicated interactions between multiple gates in LSTMs.\nBoth the reset gates and update gates are a function of the input, and the hidden state at t-1\nand the next h state is calculated as\nAs each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales.\nThose units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active.\nBatch Normalisation Paper: [[Batch Normalisation]], following Sergey Ioffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift The problem\nTraining is complicated by the fact that the inputs to each layer\nare affected by the parameters of all preceding layers ‚Äì so\nthat small changes to the network parameters amplify as\nthe network becomes deeper.\nChange in the distributions of layers‚Äô inputs presents a problem because the layers need to continu-\nously adapt to the new distribution.\nInput distribution properties that make training more efficient ‚Äì such as having the same distribution\nbetween the training and test data ‚Äì apply to training the sub-network as well. As such it is advantageous for the\ndistribution of x to remain fixed over time.\nConsider a layer with a sigmoid activation function z = g(W u + b) where u is the layer input,\nthe weight matrix W and bias vector b are the layer parameters to be learned, and g(x) = 1/ 1+exp(‚àíx) . As |x|\nincreases, g‚Ä≤(x) tends to zero. This means that for all dimensions of x = W u+b except those with small absolute values, the gradient flowing down to u will vanish and the model will train slowly.\nIf, however, we could ensure that the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime,and the training would accelerate.\nSo they propose a new solution\nThe Solution: Introduce a normalization step that fixes the means and variances of every layer inputs\nWhat they tried: One approach could be to modify the network directly at regular intervals, to maintain the normalisation properties. They explain that this doesn\u0026rsquo;t work because\nThe issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place.\nWe have observed this empirically in initial experiments, where the\nmodel blows up when the normalization parameters are\ncomputed outside the gradient descent step\nAnd hence they proposed the solution to bring the batch-normalisation layer\nTo address this issue, we would like to ensure that, for any parameter values,\nthe network always produces activations with the desired\ndistribution. Doing so would allow the gradient of the\nloss with respect to the model parameters to account for\nthe normalization, and for its dependence on the model\nparameters Œò\nThe Batch Normalisation Layer Algo Important points: Normalises dimensions independently: The algorithm works to normalise each dimension independently. So for each dimension k, it hopes to normalise Enables the layers to still adapt It\u0026rsquo;s important to not change what each layer represents. So they don\u0026rsquo;t want to specifically force every activation to be of mean 0 and variance 1. Instead, they introduce the scaling parameters to let the network still learn the biases and scaling factors. The algorithm merely ensures that the distribution of the inputs is maintained.\nThis is done via\nNote that this enables the network to retain the representation\nCons: Creates coupling between the examples in the training\nRather, BNŒ≥,Œ≤ (x) depends both on the training example and the other examples in the mini-batch.\nThe BN layer is differentiable: Normalization is only needed during training The normalization of activations that\ndepends on the mini-batch allows efficient training, but is\nneither necessary nor desirable during inference\nHence after training, only the population statistics is used for the providing the same effect during inference\nThese statistics are calculated by using moving average method\nWe use the unbiased variance estimate Var[x] = m\nm‚àí1 ¬∑ EB[œÉ^2 ], where\nthe expectation is over training mini-batches of size m and œÉ^2 are their sample variances\nSince the means and variances are fixed during inference,\nthe normalization is simply a linear transform applied to\neach activation.\nFinal Algorithm Layer Normalisation Paper: [[Layer Normalization]], following Jimmy Lei Ba, 2016 Layer Normalization The problem\nBatch normalisation depends on mini-batches, and it isn\u0026rsquo;t obvious how to use them in an RNN model\nIn feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.\nThe change is to calculate the mean and variance statistics over all the hidden units in a layer, instead of the batches\nAfter that, the famliar bias and gain are added, similar to BN\nAttention: Neural Machine Translation Paper: Attention, following Dzmitry Bahdanau, 2015 Dzmitry Bahdanau, 2015 Background This paper was trying to solve language translation problems, and while the title doesn\u0026rsquo;t focus on attention - this is the first time that the mechanism of \u0026ldquo;attention\u0026rdquo; was provided. So it brings us to the foundations of how attention came to be.\nAt the time of this paper, the encoder-decoder architecture is prominent for translation. Namely, a bidirectional RNN is used to encode the source sentence, and the decoder RNN is conditioned on the output of this encoder RNN to produce the translated sentence.\nProblem In the words of the author:\nA potential issue with this encoder‚Äìdecoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences\nSolution Abstract: Introduce an extension to the encoder‚Äìdecoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\nArchitecture The authors propose a novel architecture, each output word in the decoder is created by considering not only the previous hidden state of the decoder, but also considering a context vector C of the encoder network outputs. This context vector itself is a weighted sum of the hidden states of the encoder network, where the weights are trained and learn the \u0026ldquo;alignment\u0026rdquo; between output words and input words.\nMath i, is used for the decoder network , and the j is used for the encoder networks\nThe hidden states s[i] of the decoder RNN are calculated as a function of s[i-1], y[i-1] and c[i]\nThe context vector c[i] is calculated as a weighted sum of all the encoder hidden states h[j]\nThese Œ± weights are an important piece here. These represent the \u0026ldquo;alignment\u0026rdquo; of the decoded word to the encoded sentence. Hence, these are trained to be a function of s[i-1], and h[j]. Essentially, the weights help the model understand how much of the j_th input word is resposible for translating the ith output/decoded state.\nNote:\nWe parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system. the alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\nFinally, p(y) is conditioned on previous words, hidden state s[i], and the context vector c[i]\nThe Golden Words The probability Œ±ij , or its associated energy eij , reflects the importance of the annotation hj with respect to the previous hidden state si‚àí1 in deciding the next state si and generating yi. Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to.\nBy letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.\nResults In the next post Now thats we\u0026rsquo;ve covered some of the basics from RNNs to Attention, we\u0026rsquo;ll cover more advanced topics in the next post.\nConclusion A detailed analysis of each influential paper in this domain can facilitate a comprehensive understanding of these models. Recognizing the limitations of each model and how succeeding models strive to address them is integral to this exploration.\nWhile the completion of Andrej Karpathy\u0026rsquo;s series is anticipated, further exploration of these foundational works will serve to strengthen our understanding of modern language models. Anticipate future posts in this series, which will delve into the realm of Transformers.\nI invite readers to share their insights on these concepts. Which paper do you consider most intriguing?\nIf i\u0026rsquo;ve made errors or haven\u0026rsquo;t described something correctly - please do comment, help me learn and correct the article for future readers.\n","permalink":"https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/","summary":"Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I\u0026rsquo;ve always liked to ground myself with foundational knowledge¬†on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.","title":"Understanding GPT - A Journey from RNNs to Attention"},{"content":"Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)\nAll losses in keras defined here\nBut why is the loss function expressed as a negative loss? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1\nThis means, that it penalises a low probability of success exponentially more.\nBut since we do LogLoss = - ( y * Log(p(y)) )\nIf the true label is 0, the effect of the log is ignored.\nOnly true labels contribute to the overall loss, and if for the true labels the P(y) value is low, then the loss magnitude is highly penalised\nWhat are logits? In context of deep learning the¬†logits layer means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer.\nThe logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.\nWhy do we make the loss functions take the logit values instead of the final classification labels?\nPushing the \u0026ldquo;softmax\u0026rdquo; activation into the cross-entropy loss layer significantly simplifies the loss computation and makes it more numerically stable.\nFull derivation in this SO post\nBinary Cross Entropy Math:\nThis is just expanded math for using P(0) = { 1 - P(1) }, and becomes same as log loss\nThe hypthosis/P function used is typically sigmoid\nUsed for:\nWhen the output class is one of two values (binary) in nature.\nCode:\nTensor Flow Code\n# Compute cross entropy from probabilities. bce = target * tf.math.log(output + epsilon()) bce += (1 - target) * tf.math.log(1 - output + epsilon()) Categorical cross entropy Math:\nUsed for:\nIf your¬†ùëå vector values are one-hot encoded, use categorical_crossentropy.\nExamples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\nTF Code Link\nSparse Categorical Cross Entropy Math (same as Categorical Cross Entropy)\nUsed for:\nInteger classes as output\nIntuitively, the sparse categorical just takes the index of the true-value to calculate the loss\nSo when model output is for example¬†[0.1, 0.3, 0.7]¬†and ground truth is¬†3¬†(if indexed from 1) then loss compute only logarithm of¬†0.7. This doesn\u0026rsquo;t change the final value, because in the regular version of categorical crossentropy other values are immediately multiplied by zero (because of one-hot encoding characteristic). Thanks to that it computes logarithm once per instance and omits the summation which leads to better performance.\n","permalink":"https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/","summary":"Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)\nAll losses in keras defined here\nBut why is the loss function expressed as a negative loss? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1\nThis means, that it penalises a low probability of success exponentially more.","title":"Loss Functions in ML"},{"content":" A quick cheatsheet on python operations\nSlice:\nastring = \u0026#34;Hello World\u0026#34; print(astring[3:7]) # prints-\u0026gt; lo w print(astring[0:10:2]) # skips one character, prints -\u0026gt; Hlowr print(astring[::-1]) # reverse a string using step -1 Case\nastring.upper() astring.lower() Slicing complete list performs a copy\nspam_copy = spam[:] Zip to loop\nfurniture = [\u0026#39;table\u0026#39;, \u0026#39;chair\u0026#39;, \u0026#39;rack\u0026#39;, \u0026#39;shelf\u0026#39;] price = [100, 50, 80, 40] for item, amount in zip(furniture, price): print(f\u0026#39;The {item} costs ${amount}\u0026#39;) Multiple assignments\nfurniture = [\u0026#39;table\u0026#39;, \u0026#39;chair\u0026#39;, \u0026#39;rack\u0026#39;, \u0026#39;shelf\u0026#39;] table, chair, rack, shelf = furniture find in list = furnitures.index('chaiur')\nSort:\nreverse sort = furniture.sort(reverse=True)\nsorted returns a new list, and accepts any iterable.\nlist.sort() sorts in place, only works for lists.\nkey function passed for custom sort\n\u0026gt;\u0026gt;\u0026gt; letters = [\u0026#39;a\u0026#39;, \u0026#39;z\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;Z\u0026#39;] \u0026gt;\u0026gt;\u0026gt; letters.sort(key=str.lower) \u0026gt;\u0026gt;\u0026gt; letters # [\u0026#39;a\u0026#39;, \u0026#39;A\u0026#39;, \u0026#39;z\u0026#39;, \u0026#39;Z\u0026#39;] \u0026gt;\u0026gt;\u0026gt; sorted(letters, key=str.lower) # also same tuples are compared lexicographically; the first items are compared; if they are the same then the second items are compared, and so on.\nTuples:\ntuples¬†are¬†immutable¬†objects,¬†lists¬†are¬†mutable.\nTuples are more memory efficient\nDictionary\n\u0026gt;\u0026gt;\u0026gt; for key in pet.keys(): \u0026gt;\u0026gt;\u0026gt; for value in pet.values(): \u0026gt;\u0026gt;\u0026gt; for key, value in pet.items(): \u0026gt;\u0026gt;\u0026gt; pet.setdefault(\u0026#39;has_hair\u0026#39;, True) # merge dictionaries \u0026gt;\u0026gt;\u0026gt; dict_c = {**dict_a, **dict_b} Set\n\u0026gt;\u0026gt;\u0026gt; s = {1, 2, 3} \u0026gt;\u0026gt;\u0026gt; s = set([1, 2, 3]) \u0026gt;\u0026gt;\u0026gt; s.add(4) \u0026gt;\u0026gt;\u0026gt; s.update([2, 3, 4, 5, 6]) #add multiple \u0026gt;\u0026gt;\u0026gt; s.remove(3) # raises errors \u0026gt;\u0026gt;\u0026gt; s.discard(3) # does NOT raise errors \u0026gt;\u0026gt;\u0026gt; s1.union(s2) # or \u0026#39;s1 | s2\u0026#39; \u0026gt;\u0026gt;\u0026gt; s1.intersection(s2, s3) # or \u0026#39;s1 \u0026amp; s2 \u0026amp; s3\u0026#39; \u0026gt;\u0026gt;\u0026gt; s1.difference(s2) # or \u0026#39;s1 - s2\u0026#39; \u0026gt;\u0026gt;\u0026gt; s1.symmetric_difference(s2) # or \u0026#39;s1 ^ s2\u0026#39; all the elements that are not common between them. Comprehension\n# LIST COMPREHENSION \u0026gt;\u0026gt;\u0026gt; new_list = [n for n in names] \u0026gt;\u0026gt;\u0026gt; n = [(a, b) for a in range(1, 3) for b in range(1, 3)] # [(1, 1), (1, 2), (2, 1), (2, 2)] \u0026gt;\u0026gt;\u0026gt; new_list = [n for n in names if n.startswith(\u0026#39;C\u0026#39;)] # SET COMPREHENSION \u0026gt;\u0026gt;\u0026gt; {s.upper() for s in b} # DICT COMPREHENSION \u0026gt;\u0026gt;\u0026gt; {v: k for k, v in c.items()} Strings\n\u0026gt;\u0026gt;\u0026gt; \u0026#39;, \u0026#39;.join([\u0026#39;cats\u0026#39;, \u0026#39;rats\u0026#39;, \u0026#39;bats\u0026#39;]) References\nlists and tuples: sheet\nAnother useful pdf : mementopython3-english.pdf\n","permalink":"https://Kshitij-Banerjee.github.io/2023/01/02/python-cheet-sheet/","summary":"A quick cheatsheet on python operations\nSlice:\nastring = \u0026#34;Hello World\u0026#34; print(astring[3:7]) # prints-\u0026gt; lo w print(astring[0:10:2]) # skips one character, prints -\u0026gt; Hlowr print(astring[::-1]) # reverse a string using step -1 Case\nastring.upper() astring.lower() Slicing complete list performs a copy\nspam_copy = spam[:] Zip to loop\nfurniture = [\u0026#39;table\u0026#39;, \u0026#39;chair\u0026#39;, \u0026#39;rack\u0026#39;, \u0026#39;shelf\u0026#39;] price = [100, 50, 80, 40] for item, amount in zip(furniture, price): print(f\u0026#39;The {item} costs ${amount}\u0026#39;) Multiple assignments","title":"Python Cheet Sheet"},{"content":"Book Summary - Grit by Angela Duckworth The post is a book summary of the main bullet points from the book \u0026ldquo;Grit\u0026rdquo; by \u0026ldquo;Angela Duckworth\u0026rdquo;\nComponents of Grit Angela breaks down grit in the following components:-\nInterest: I love what I do\nPractice: I will do what it takes to improve and become world-class\nPurpose: What I do is important for everyone\nHope: I will keep going even when it‚Äôs difficult\nInterest \u0026amp; Practice : Finding your passion I\u0026rsquo;ve merged both Interest and Practice as they flow nicely, and have many couples aspects to them.\nWhat does it practically mean? Some period of discovery, a period of development and a long period of deepening.\nTips Once you find it , don‚Äôt rush - rush can bludgeon a bouldering interest\nIf you still haven‚Äôt found your passion, consider indulging into one a little bit longer and going deeper to what you already have a good hang of . Sometimes the depth clicks\nPurpose : Finding it There\u0026rsquo;s this para which clarifies what she means by purpose in the context of grit, and it summarises the concept quite well. The key here is the difference between:-\nA job - something you do to pay bills\nA career - stepping stone to other jobs\nA calling - work is the most important thing in my life. i‚Äôm born to do this. my work is important to the world\nPurpose alone doesn‚Äôt work as effectively. Purposeful Passion is what makes a difference\nHope : I will keep doing it, no matter how many failures I get Things can change if you push. Don\u0026rsquo;t give up. Keep trying until you succeed, be agile but keep trying towards your goal no matter what, no matter how hard it gets\nDon\u0026rsquo;t let inertia of previous problems, failures stop you from trying again and again\nTheory There was an interesting part where she explains the concept of achievement as an equation and how it relates to grit.\nTalent X Effort = Skill\nSkill X Effort = Achievement\nTalent is the speed at which you gain skill by effort\nBut it also takes effort to use that skill to get results\nGrit is not just perseverance. It is also Passion and Loyalty to the same goal / task for a long period of time until you reach it\nGrit is about holding the same top level goal for a very long time\n","permalink":"https://Kshitij-Banerjee.github.io/2022/12/12/grit-angela-duckworth/","summary":"Book Summary - Grit by Angela Duckworth The post is a book summary of the main bullet points from the book \u0026ldquo;Grit\u0026rdquo; by \u0026ldquo;Angela Duckworth\u0026rdquo;\nComponents of Grit Angela breaks down grit in the following components:-\nInterest: I love what I do\nPractice: I will do what it takes to improve and become world-class\nPurpose: What I do is important for everyone\nHope: I will keep going even when it‚Äôs difficult","title":"Grit - Angela Duckworth"},{"content":"Problem Statement: GfG quoted: Find the largest rectangular area possible in a given histogram where the largest rectangle can be made of a number of contiguous bars. For simplicity, assume that all bars have same width and the width is 1 unit. The Clever Solution Sometimes, the nicest solutions come from clues we receive from the worst ones.\nWhat\u0026rsquo;s the naive solution ? Iterate through all possible rectangles and calculate the area. How is the area bounded ? What is the extra information we need other than the \u0026ldquo;free\u0026rdquo; variables i,j ? We realize that min(i..j) is what constraints the area for every (i,j) pair. For every possible combination of left and right extremes. Find the maximum value of (j-i+1)*min(i..j) General way our brain thinks is :- Create every situation and try to find the value of the contraint that is needed to solve the problem. And we happily convert that to code as :- find the value of contraint(min) for each situation(pair(i,j)) Or,\nMax( (i,j) -\u0026gt; (j-i+1)*min(i,i+1,i+2,...j) ) The clever solutions tries to flip the problem. Hereon refered to as inversion of constraint solution\nFor each constraint/min value of tha area, what is the best possible left and right extremes ?\nSo if we traverse over each possible min in the array. What are the left and right extremes for each value ? Little thought says, the first left most value less than the current min and similarly the first rightmost value that is lesser than the current min. Try some examples, if the above is difficult to validate. So now we need to see if we can find a clever way to find the first left and right values lesser than the current value. To think: If we have traversed the array partially say till min_i, how can the solution to min_i+1 be built? take some time to think.. For each min_i\u0026hellip;..\nWe need the first value less than min_i to its left.\nInverting the statement : we need to ignore values to the left of it that are greater than min_i.\nThe troughs /\\ in the curve hence become useless once we have crossed it.\nExample: In histogram , (2 4 3) =\u0026gt; if 3 is curr min_i being evaluated, the 4 before it being larger to it, is of no interest since we have crossed it in the previous calculations.\nCorrollary: Any area being considered on the right, with a min value larger than current j, will be binded at j.\nSo in our processing, for each value being considered, we need the set of values before it that are less than it.\nThe values of interest on the left form a monotonically increasing sequence with j being the largest value. (Values of interest here being possible values that may be of interest for the later array)\nThis solves the left side. Lets concretize with an example\nIf the array being evaluated is : (1,6,2,56,4,23,7) [currently at 4]\nTo know the value just less than 4 the only interesting part we need to retain is (1, 2). i.e, 6 and 56 are useless for calculation of 4, and have been ignored Since, we are travelling from left to right, for each min value/ current value - we do not know whether the right side of the array will have an element smaller than it.\nSo we have to keep it in memory until we get to know this value is useless. All this leads to a usage of our very own stack structure.\nWe keep on stack until we don\u0026rsquo;t know its useless. We remove from stack once we know the thing is crap. So for each min value to find its left smaller value, we do the following:- pop the elements larger to it (useless values) The first element smaller than the value is are leftmost extreme. The i to our min. We can do the same thing from the right side of the array and we will get j to each of our min.\nObservation : If we observe the stack for each iteration i. What does it contain ?\nA monotonically increasing subsequence with i its max value. Notice how this becomes usefull later. Code examples are in plenty. But here is an implementation I did a while back\nhttps://github.com/Kshitij-Banerjee/CompetitiveCoding/blob/master/IB-LargestREctHistogram/IB-LargestREctHistogram/main.cpp What\u0026rsquo;s the time complexity of one traversal ? While the constant pushing popping of the items seems cumbersome. Here is what helps me. The question to ask in general is: How many times is each item seen ? Once when it is pushed. Once when it is popped. Hence -\u0026gt; O(n) Even better ? The above needs 2 traversals. One to get i for each min, and one to get j for each min. Can we do it in one traversal ? The trick is to infer the i and j values together from the stack. All min values fall between two smaller elements. Consider that we are traversing for each value j. each value being popped is a potential min value with j - the first value seen to its right that is smaller. since at any point, the values in the stack are in monotonically increasing order. The i value to this min is the value just before it. More [http://www.spoj.com/problems/C1TABOVI/](SPOJ problem CITABOVI) [http://www.geeksforgeeks.org/largest-rectangle-under-histogram/](Geeks for Geeks with code) ","permalink":"https://Kshitij-Banerjee.github.io/2017/01/30/largest-area-under-a-histogram-and-related-concepts/problems./","summary":"Problem Statement: GfG quoted: Find the largest rectangular area possible in a given histogram where the largest rectangle can be made of a number of contiguous bars. For simplicity, assume that all bars have same width and the width is 1 unit. The Clever Solution Sometimes, the nicest solutions come from clues we receive from the worst ones.\nWhat\u0026rsquo;s the naive solution ? Iterate through all possible rectangles and calculate the area.","title":"Largest Area Under a Histogram (and related concepts/problems)."},{"content":"How to recover data from raw .tokudb files. Why? Recently my tokudb database went corrupt after a bad shutdown and a lot of data was now lost. After a lot of googling, asking on forums check here, here and panicking in general, I finally figured out how to get my data back after some Hard core. Brute force. Raw file Reverse-Engineering. How? Step 1 : Find your raw data files. The tokufiles have an extension of .tokudb and will be found in your mysql data directory. /var/lib/mysql if you follow the standard conventions. Toku keeps multiple files per tables for data and indices, unlike innodbs combined .ibd files. * Status file : example format: (database_table_status_table(some hash)1_X.tokudb) * Main file (this has the data - example format : _database_table_main_table(som hash)_2_X.tokudb) * One file per secondary index of thpe table. (this has the indexes - example format :- _database_table_key_somethinsomething.tokudb) Another thing to note is that usually the files will have table names in them the first time they are created, but if your ever had an alter creating a temp table on them, the new table doesn\u0026rsquo;t get the name due to a known bug, so not always will you see the table name in the files) Step 2 : Map the files to the tables How to know which files correspond to which tables then ? - Fortunately, tokudb keeps a map oef the table to file names in the information_schema.TokuDB_file_map table! - If you have a slave (even broken) with the same tables , you can run this command on it to figure out which files map to which tables. Everything below is a fat Step 3: Reverse Engineer the files 3. Clone the open source repository for toku\u0026rsquo;s fractal trees here 4. Notice that in the tools directory, you will find a tokuftdump utility. (ft-index/tools/tokuftdump.cc)\nI suggest going through the utility to understand how the fractal trees are being parsed (This is optional though) Run the utility tool to print out the dump. You should notice that the leafs are printed out as byte streams. These bytestreams contain the complete row of your table in tokudb\u0026rsquo;s internal structure! We want to convert the byte data into meaningful data. So we\u0026rsquo;ll have to first understand the structure of the this bytestream.We need to do some reverse engineering for this. Modify the utility to dump hex streams instead of the byte streams. Use sprintf(hexstr, \u0026ldquo;%02x\u0026rdquo;, array[i]); SO link Once you have the hex stream, lets get down to some grammer cracking reverse engineering. Download a tool called 010 Editor from here Copy one of the rows of the table into the hex window as shown below and use the .bt files to come up with the structure of the row. This step might need you to experiment a bit with your pointer placements and watch the value convertions carefully. Some things to note. Varchars are padded at the end of the rows. Just before the varchars, you will notice a bunch of numbers that count the size of the ith varchar in your schema. This is because the varchars are only trying to store the actual characters to save space. Once youre happy with the reverse engineering, note this grammer of the data value. Modify the toku print methods to cast the bytestream into your struct as shown below. Run your utility again and voila!, you\u0026rsquo;ll have your data back! This covers the basics. If you\u0026rsquo;re really stuck and need help. Comment below or reach out to me and I\u0026rsquo;ll try to help, coz I know how much we love our data. ;)\n","permalink":"https://Kshitij-Banerjee.github.io/1/01/01/reverse-engineer-data-from-raw-database-files./","summary":"How to recover data from raw .tokudb files. Why? Recently my tokudb database went corrupt after a bad shutdown and a lot of data was now lost. After a lot of googling, asking on forums check here, here and panicking in general, I finally figured out how to get my data back after some Hard core. Brute force. Raw file Reverse-Engineering. How? Step 1 : Find your raw data files. The tokufiles have an extension of .","title":"Reverse Engineer data from raw database files. "}]