
---
Title: Understanding GPT History (through papers)
Category: Machine Learning  
Layout: post  
Name: Understanding GPT and it's History  
date: 2023-06-18  
---
  
# Introduction

This post is inspired by Andrej Karpathy's [makemore](https://github.com/karpathy/makemore) series. Unfortunately, as of the time of this writing, he hasn't updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding. The series jumps straight from wave nets, to transformers and GPT and it leaves a lot to be filled.

So as I go on my path to understand the history, I'd like this to fill the gap for anyone else who might be going through the same journey.
	  I believe, andrej will eventually complete this series in terms of code, and when it does - it will be a far better source than this post will ever be.  
	  But, if you're like me, and looking for a crash-course on the various papers and the core concepts. Follow along:-  
## Papers NOT be going through

The following papers are ones, that andrew has explained in a lot of detail in his make more lecture series on [YouTube](https://www.youtube.com/watch?v=PaCmpygFfXo) , and I would recommend anyone to go through the series - as its the best explanation I've seen so far.

Bigram (one character predicts the next one with a lookup table of counts)

MLP, following [Bengio](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

CNN, following [DeepMind WaveNet 2016](https://arxiv.org/abs/1609.03499)

## Papers intend to deep-dive into:

[[RNN]] , following Mikolov et al. 2010 ![Recurrent neural network based language model](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)

[[Backpropagation through time ]], followed in Mikael Bod ́en 2001 ![BPTT](https://axon.cs.byu.edu/~martinez/classes/678/Papers/RNN_Intro.pdf)

[[LSTM]] , following Graves et al. 2014 ![Generating Sequences With Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf)

[[GRU]] , following Kyunghyun Cho et al. 2014 ![On the Properties of Neural Machine Translation: Encoder–Decoder](https://arxiv.org/pdf/1409.1259.pdf)

[[Batch Normalisation]], following Sergey Ioffe et al. 2015 ![Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/pdf/1502.03167.pdf)

[[Attention]], following Dzmitry Bahdanau, 2015 ![Dzmitry Bahdanau, 2015](https://arxiv.org/pdf/1409.0473.pdf)

[[Layer Normalization]], following Jimmy Lei Ba, 2016 ![Layer Normalization](https://arxiv.org/pdf/1607.06450.pdf)

[[Transformers]] , following Vaswani et al. 2017 ![Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

## Let's get started

### RNN - Recurrent Neural Networks

**Paper:** [Mikolov et al. 2010](https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf)

**Summary**

In effect, the paper references the MLP paper by [Bengio](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), and cites the shortcoming that the context length is fixed and manually set. (The context length is the number of previous tokens that are fed into the MLP to generate the next word).

Quoting the author:-

> It is well known that humans can exploit longer context with great success. Also, cache models provide comple-  
mentary information to neural network models, so it is natural to think about a model that would encode temporal information  
implicitly for contexts with arbitrary lengths  
The authors then explain how a simple recurrent neural network works

![image.png](./image_1687079484371_0.png)

![image.png](./image_1687079552173_0.png)

Where

w(t) is the input word at t

s(t-1) is the state previously generated by the RNN in its last time-step

Output layer y(t) represents probability distribution of next word given previous word w(t) and context. Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.

Back-propagation through time (BPTT) algorithm is used. (This is covered next)

Notable lines:

> Based on our experiments, size of hidden layer should reflect amount of training data - for large  
amounts of data, large hidden layer is needed  
> Convergence is usually achieved after 10-20 epochs.  
> regularization of networks to penalize large weights did not provide any significant improvements.  
[[Backpropagation through time ]], followed in Mikael Bod ́en 2001

The key insight is around how to back-propagate through the recursion caused loop

The solution is to "unroll" the model T times, and then follow normal backpropation

Instead, of keeping separate weight matrix for each time-step, the weight matrix is instead shared across the unfolded layers.

![image.png](./image_1687080413026_0.png)

Note, how weights V and U , remain the same through the unfolding process

Important quotes from the paper:

> It is important to note, however, that after error deltas have been calculated, weights  
are folded back adding up to one big change for each weight. Obviously there is a greater  
memory requirement (both past errors and activations need to be stored away), the larger  
τ we choose.  
> In practice, a large τ is quite useless due to a “vanishing gradient effect” (see e.g.  
(Bengio et al., 1994)). For each layer the error is backpropagated through the error  
gets smaller and smaller until it diminishes completely. Some have also pointed out that  
the instability caused by possibly ambiguous deltas (e.g. (Pollack, 1991)) may disrupt  
convergence. An opposing result has been put forward for certain learning tasks (Bod ́en  
et al., 1999).  
Note that batch normalization and layer normalization were probably not present at this time.

PyTorch

[Code Doc](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)

![image.png](./image_1687081259085_0.png)

Input:

(*N*,*L*,*H**in*​) when batch_first=True

N = Batch Size

L  = Sequence Length

H_in = Hidden Layer

### LSTM - Long Short-term Memory

**Paper:** Graves et al. 2014 [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/pdf/1308.0850.pdf)

**Summary**

Quoting the paper to best describe the problem they are addressing

> In practice however, standard RNNs are unable to  
store information about past inputs for very long [15]. As well as diminishing  
their ability to model long-range structure, this ‘amnesia’ makes them prone to  
instability when generating sequences. The problem (common to all conditional  
generative models) is that if the network’s predictions are only based on the last  
few inputs, and these inputs were themselves predicted by the network, it has  
little opportunity to recover from past mistakes. Having a longer memory has  
a stabilising effect, because even if the network cannot make sense of its recent  
history, it can look further back in the past to formulate its predictions.  
TO BE CONTINUED..
