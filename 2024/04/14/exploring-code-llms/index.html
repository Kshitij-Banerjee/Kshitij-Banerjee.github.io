<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Exploring Code LLMs | KiloBytes by KB</title>
<meta name=keywords content="machine-learning,AI"><meta name=description content="Introduction The goal of this post is to deep-dive into LLM&rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.
Note: Unlike copilot, we&rsquo;ll focus on locally running LLM&rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we&rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings."><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms/><link crossorigin=anonymous href=/assets/css/stylesheet.4ef0a4ca216439b70bd161c6aae5b22feb6d371b58aa70a84e5dfc6ef4acd473.css integrity="sha256-TvCkyiFkObcL0WHGquWyL+ttNxtYqnCoTl38bvSs1HM=" rel="preload stylesheet" as=style><link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-KFSE3K3EMC"></script><script>var doNotTrack=!1,dnt;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KFSE3K3EMC")}</script><meta property="og:title" content="Exploring Code LLMs"><meta property="og:description" content="Introduction The goal of this post is to deep-dive into LLM&rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.
Note: Unlike copilot, we&rsquo;ll focus on locally running LLM&rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we&rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings."><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms/"><meta property="og:image" content="https://Kshitij-Banerjee.github.io/exploring_code_llms.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-14T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-14T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Kshitij-Banerjee.github.io/exploring_code_llms.png"><meta name=twitter:title content="Exploring Code LLMs"><meta name=twitter:description content="Introduction The goal of this post is to deep-dive into LLM&rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.
Note: Unlike copilot, we&rsquo;ll focus on locally running LLM&rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.
To test our understanding, we&rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Exploring Code LLMs","item":"https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Exploring Code LLMs","name":"Exploring Code LLMs","description":"Introduction The goal of this post is to deep-dive into LLM\u0026rsquo;s that are specialised in code generation tasks, and see if we can use them to write code.\nNote: Unlike copilot, we\u0026rsquo;ll focus on locally running LLM\u0026rsquo;s. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.\nTo test our understanding, we\u0026rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings.","keywords":["machine-learning","AI"],"articleBody":"Introduction The goal of this post is to deep-dive into LLM‚Äôs that are specialised in code generation tasks, and see if we can use them to write code.\nNote: Unlike copilot, we‚Äôll focus on locally running LLM‚Äôs. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.\nTo test our understanding, we‚Äôll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings.\nThe goal - A few simple coding task Test 1: Generate a higher-order-component / decorator that enables logging on a react component\nTest 2: Write a test plan, and implement the test cases\nTest 3: Parse an uploaded excel file in the browser.\nHow the rest of the post is structured We‚Äôre going to cover some theory, explain how to setup a locally running LLM model, and then finally conclude with the test results.\nPart 1: Quick theory\nInstead of explaining the concepts in painful detail, I‚Äôll refer to papers and quote specific interesting points that provide a summary. For a detailed reading, refer to the papers and links I‚Äôve attached.\nInstruction Fine-tuning: Why instruction fine-tuning leads to much smaller models that can perform quite well on specific tasks, compared to much larger models\nOpen source models available: A quick intro on mistral, and deepseek-coder and their comparison.\nModel Quantization: How we can significantly improve model inference costs, by improving memory footprint via using less precision weights.\nIf you know all of the above, you may want to skip to Part 2\nPart 2: Local LLM Setup\nUsing Ollama and setting up my VSCode extension\nVSCode Extension available here: https://github.com/Kshitij-Banerjee/kb-ollama-coder\nPart 3: Test Results\nShowing results on all 3 tasks outlines above.\n[Part 1] Understanding Instruction Finetuning Before we venture into our evaluation of coding efficient LLMs. Let‚Äôs quickly discuss what ‚ÄúInstruction Fine-tuning‚Äù really means.\nWe refer to this paper: Training language models to follow instructions with human feedback Why instruction fine-tuning ? predicting the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù\nPerformance Implications In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer paramete\nHow they did it? SpeciÔ¨Åcally, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to Ô¨Åne-tune GPT-3 to follow a broad class of written instructions. This technique uses human preferences as a reward signal to Ô¨Åne-tune our models. We Ô¨Årst hire a team of 40 contractors to label our data, based on their performance on a screening tes We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API3 and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and Ô¨Åne-tune our supervised learning baseline to maximize this reward using the PPO algorithm\nPaper Results We call the resulting models InstructGPT. {:height 636, :width 1038}\nOn the TruthfulQA benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3 During RLHF Ô¨Åne-tuning, we observe performance regressions compared to GPT-3 We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores. InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises$$\nSome notes on RLHF Step 1: Supervised Fine Tuning: We Ô¨Åne-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2\nStep 2 : Reward model Starting from the SFT model with the Ô¨Ånal unembedding layer removed, we trained a model to take in a prompt and response, and output a scalar reward The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference. These reward models are themselves pretty huge. 6B parameters in Open AI case\nStep 3: Fine-tuning with RL using PPO PPO : Proximal Policy Optimization Given the prompt and response, it produces a reward determined by the reward model and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models ‚ÄúPPO.‚Äù\nFrom : https://huggingface.co/blog/rlhf ‚ÄúLet‚Äôs first formulate this fine-tuning task as a RL problem. First, the¬†policy¬†is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The¬†action space¬†of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the¬†observation space¬†is the distribution of possible input token sequences, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary ^ length of the input token sequence). The¬†reward function¬†is a combination of the preference model and a constraint on policy shift.‚Äù Concatenated with the original prompt, that text is passed to the preference model, which returns a scalar notion of ‚Äúpreferability‚Äù,¬†rŒ∏‚Äã. In addition, per-token probability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback‚ÄìLeibler¬†(KL) divergence¬†between these sequences of distributions over tokens,¬†r_kl. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Finally, the¬†update rule¬†is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process.\nHelpful schematic showing the RL fine-tune process Final Thoughts InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints deÔ¨Åned in the instruction (e.g. ‚ÄúWrite your answer in 2 paragraphs or less.‚Äù),\nAre less likely to fail to follow the correct instruction entirely,\nAre less likely to make up facts (‚Äòhallucinate‚Äô) less often in closed-domain tasks. These results suggest that InstructGPT models are more reliable and easier to control than GPT-3\nComparison of GPT vs Instruct GPT [Part 1] Deep dive into Mistral Models Brief introduction to Mistral models, their architecture, and key features We refer to the paper: Mistral 7b Objective: The search for balanced models delivering both high-level performance and efficiency\nKey Results: Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25 ]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20 ], without sacrificing performance on non-code related benchmarks\nKey Insights: Mistral 7B leverages grouped-query attention (GQA) [ 1 ], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost\nSliding Window Attention Why? The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most W tokens from the previous layer Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after k attention layers, information can move forward by up to k √ó W tokens SWA exploits the stacked layers of a transformer to attend information beyond the window size W . The hidden state in position i of the layer k, hi, attends to all hidden states from the previous layer with positions between i ‚àí W and i. Recursively, hi can access tokens from the input layer at a distance of up to W √ó k tokens, as illustrated in Figure 1. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately131K tokens. In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [ 11 ] and xFormers [18 ] yield a 2x speed improvement over a vanilla attention baseline. This fixed attention span, means we can implement a rolling buffer cache. After W size, the cache starts overwriting the from the beginning. This also allows some pre-filling based optimizations.\nComparison with Llama Instruction Finetuning To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: Mistral 7B ‚Äì Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance.\nSystem prompt We introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2. The prompt: ‚ÄúAlways assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.‚Äù\nHow to select various model sizes, a Thumbrule from Mistral AI. Refer to this article: Model Selection\nTL;DR\nSmall Tasks (Custom support, classification) =\u003e use Mistral Small\nMedium Tasks (Data Extraction, Summarizing Documents, Writing emails.. ) =\u003e Mistral medium\nComplex Tasks (Code Generation, RAG) =\u003e Mistral Large\nBenchmark on coding :\nModel MMLU hellaswag (10-shot) winograde (5-shot) arc challenge (25-shot) TriviaQA (5-shot) TruthfulQA Mistral 7B 62.5% 83.1% 78.0% 78.1% 68.8% 42.35% Mixtral 8x7B 70.6% 86.7% 81.2% 85.8% 78.38% 47.5% Mistral Small 72.2% 86.9% 84.7% 86.9% 79.5% 51.7% Mistral Medium 75.3% 88.0% 88% 89.9% 81.1% 47% Mistral Large 81.2% 89.2% 86.7% 94.0% 82.7% 50.6% [Part 1] Deepseek Coder, an upgrade? Overview of Eval metrics Before we understand and compare deepseeks performance, here‚Äôs a quick overview on how models are measured on code specific tasks.\nLeaderboard is provided here : https://evalplus.github.io/leaderboard.html\nWhat is HumanEval ? https://github.com/openai/human-eval\nHumanEval consists of 164 hand-written Python problems that are validated using test cases to assess the code generated by a Code LLM in a zero-shot setting,\nWhat is MBPP ? https://huggingface.co/datasets/mbpp\nWhile the MBPP benchmark includes 500 problems in a few-shot setting.\nDS-1000: More practical programming tasks, compared to Human Eval DS-1000 benchmark, as introduced in the work by Lai et al. (2023), offers a comprehensive collection of 1,000 practical and realistic data science workflows across seven different libraries\nDeepseek coder Summary Each model in the series has been trained from scratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive understanding of coding languages and syntax. Refer to the paper from DeepSeek coder: DeepSeek Code Useful links:\nhttps://deepseekcoder.github.io/\nhttps://ollama.com/library/deepseek-coder/tags\nHow it‚Äôs built Repository Context in Pre-training Besides, we attempt to organize the pretraining data at the repository level to enhance the pre-trained model‚Äôs understanding capability within the context of cross-files within a repository They do this, by doing a topological sort on the dependent files and appending them into the context window of the LLM. More details below.\nWe find that it can significantly boost the capability of cross-file code generation\nNext token prediction + Fill-in-the middle (like BERT) In addition to employing the next token prediction loss during pre-training, we have also incorporated the Fill-In-Middle (FIM) approach.\n16K context window (Mistral models have 4K sliding window attention) To meet the requirements of handling longer code inputs, we have extended the context length to 16K. This adjustment allows our models to handle more complex and extensive coding tasks, thereby increasing their versatility and applicability in various coding scenarios\nData Preparation Filtering Rule: TL;DR: Remove non-code related, or data heavy files\nRemove files with avg line length \u003e 100, OR, maximum line length \u003e 1000 characters.\nRemove files with fewer than 25% alphabetic characters\nremove \u003c?xml version files\nJSON/YAML files - keep fields that have character counts ranging from 50 -\u003e 5000 . This removes data-heavy files.\nDependency Parsing Instead of simply passing in the current file, the dependent files within repository are parsed.\nParse Dependency between files, then arrange files in order that ensures context of each file is before the code of the current file. By aligning files based on dependencies, it accurately represents real coding practices and structures.\nThis enhanced alignment not only makes our dataset more relevant but also potentially increases the practicality and applicability of the model in handling project-level code scenarios It‚Äôs worth noting that we only consider the invocation relationships between files and use regular expressions to extract them, such as\"import\" in Python, ‚Äúusing‚Äù in C#, and ‚Äúinclude‚Äù in C. A topological sort algorithm for doing this is provided in the paper.\nTo incorporate file path information, a comment indicating the file‚Äôs path is added at the beginning of each file.\nModel Architecture Each model is a decoder-only Transformer, incorporating Rotary Position Embedding (RoPE) Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) as described by Su et al. (2023), with a group size of 8, enhancing both training and inference efficiency. Additionally, we employ FlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism we use AdamW (Loshchilov and Hutter, 2019) as the optimizer with ùõΩ1 and ùõΩ2 values of 0.9 and 0.95. he learning rate at each stage is scaled down to‚àöÔ∏É 110 of the preceding stage‚Äôs rate Context Length:\nTheoretically, these modifications enable our model to process up to 64K tokens in context. However, empirical observations suggest that the model delivers its most reliable outputs within a 16K token range.\\\nInstruction Tuning This data comprises helpful and impartial human instructions, structured by the Alpaca Instruction format. To demarcate each dialogue turn, we employed a unique delimiter token \u003c|EOT|\u003e\nPerformance Surpasses GPT3.5, and within reach of GPT4\nTo evaluate the model‚Äôs multilingual capabilities, we expanded the Python problems of Humaneval Benchmark to seven additional commonly used programming languages, namely C++, Java, PHP, TypeScript (TS), C#, Bash, and JavaScript (JS) (Cassano et al.,2023). For both benchmarks, We adopted a greedy search approach and re-implemented the baseline results using the same script and environment for fair comparison. Interesting Notes Chain of thought prompting\nOur analysis indicates that the implementation of Chain-of-Thought (CoT) prompting notably enhances the capabilities of DeepSeek-Coder-Instruct models. This improvement becomes particularly evident in the more challenging subsets of tasks. By adding the directive, ‚ÄúYou need first to write a step-by-step outline and then write the code.‚Äù following the initial prompt, we have observed enhancements in performance. This observation leads us to believe that the process of first crafting detailed code descriptions assists the model in more effectively understanding and addressing the intricacies of logic and dependencies in coding tasks, particularly those of higher complexity. Therefore, we strongly recommend employing CoT prompting strategies when utilizing DeepSeek-Coder-Instruct models for complex coding challenges.\n[Part 1] Model Quantization Along with instruction fine-tuning, another neat technique that makes LLM‚Äôs more performant (in terms of memory and resources), is model quantization\nModel quantization enables one to reduce the memory footprint, and improve inference speed - with a tradeoff against the accuracy.\nIn short, Quantization is a process from moving the weights of the model, from a high-information type like fp32 to a low-information but performant data-type like int8\nReference: Huggingface guide on quantization - https://huggingface.co/docs/optimum/en/concept_guides/quantization\nThe two most common quantization cases are¬†float32 -\u003e float16¬†and¬†float32 -\u003e int8.\nSome schematics that explain the concept.\n{:height 362, :width 719}\nQuantization to Int8 Let‚Äôs consider a float¬†x¬†in¬†[a, b], then we can write the following quantization scheme, also called the¬†affine quantization scheme:\nx = S * (x_q - Z) x_q¬†is the quantized¬†int8¬†value associated to¬†x\nS¬†is the scale, and is a positive¬†float32\nZ¬†is called the zero-point, it is the¬†int8¬†value corresponding to the value¬†0¬†in the¬†float32¬†realm.\nx_q = round(x/S + Z) x_q = clip(round(x/S + Z), round(a/S + Z), round(b/S + Z)) In effect, this means that we clip the ends, and perform a scaling computation in the middle. The clip-off obviously will lose to accuracy of information, and so will the rounding.\nCalibration An example, explaining calibration to optimise clipping vs rounding error\n{:height 187, :width 314}\nTo ensure that we have a good balance of clipping vs rounding errors, based on the range [a, b] that we select. Some techniques are available\nUse per-channel granularity for weights and per-tensor for activations\nQuantize residual connections separately by replacing blocks\nIdentify sensitive layers and skip them from quantization\nhttps://huggingface.co/blog/4bit-transformers-bitsandbytes\nModel quantization + instruct = Quite Good results\nGood reference reading on the topic: https://deci.ai/quantization-and-quantization-aware-training\n[Part 2] Setting Up the Environment: Ollama on M1 Option 1: Hosting the model To host the models, I chose the ollama project: https://ollama.com/\nOllama is essentially, docker for LLM models and allows us to quickly run various LLM‚Äôs and host them over standard completion APIs locally.\nThe website and documentation is pretty self-explanatory, so I wont go into the details of setting it up.\nOption 2: My machine is not strong enough, but I‚Äôd like to experiment If your machine doesn‚Äôt support these LLM‚Äôs well (unless you have an M1 and above, you‚Äôre in this category), then there is the following alternative solution I‚Äôve found.\nYou can rent machines relatively cheaply (~0.4$ / hour) for inference methods, using vast.ai\nOnce you‚Äôve setup an account, added your billing methods, and have copied your API key from settings.\nClone the llm-deploy repo, and follow the instructions.\nThis repo figures out the cheapest available machine and hosts the ollama model as a docker image on it.\nFrom 1 and 2, you should now have a hosted LLM model running. Now we need VSCode to call into these models and produce code.\nVSCode Extension Calling into the Model Given the above best practices on how to provide the model its context, and the prompt engineering techniques that the authors suggested have positive outcomes on result. I created a VSCode plugin that implements these techniques, and is able to interact with Ollama running locally.\nThe source code for this plugin is available here:\nhttps://github.com/Kshitij-Banerjee/kb-ollama-coder\nThis plugin achieves the following:-\nIt provides the LLM context on project/repository relevant files.\nThe plugin not only pulls the current file, but also loads all the currently open files in Vscode into the LLM context.\nIt then trims the context to the last 16000/24000 characters (configurable)\nThis is an approximation, as deepseek coder enables 16K tokens, and approximate that each token is 1.5 tokens. In practice, I believe this can be much higher - so setting a higher value in the configuration should also work.\nIt adds a header prompt, based on the guidance from the paper. (Configurable) Example:-\n‚ÄúYou need to first write a step-by-step outline and then write the code. The following is a complete {LANG} file named {FILE_NAME} in the project {PROJECT_NAME}. Anything NOT code is written as a CODE COMMENT.‚Äù\nSource code The source code for this plugin is available here:\nhttps://github.com/Kshitij-Banerjee/kb-ollama-coder\nFor completeness, there are other VSCode plugins that might also solve a similar problem - and I am yet to try them\nhttps://github.com/rjmacarthy/twinny\nhttps://github.com/smallcloudai/refact\nhttps://github.com/TabbyML/tabby\n[Part 3] Testing DeepseekCoder General Observations For the most part, the 7b instruct model was quite useless and produces mostly error and incomplete responses.\nThe 33b models can do quite a few things correct. However, its probably at the brink of usefuleness\nThe model doesn‚Äôt really understand writing test cases at all.\nTest Setting For all the tests, I used a temperature setting of 0, as the guideline is to have lower temperatures for tasks like coding, instead of creative generation tasks (I find coding a pretty creative activity though, but just going with the guidelines for now)\nTest 1: Mount/Unmount logging using hooks and HOC In this task, I wanted to test its ability to understand basic react concepts of useEffect and hooks / HOC\n1.1 Writing custom hook with mount/unmount logging PROMPT\n// A react custom hook that logs the component name on mount and unmount CODE OUTPUT:\nimport { useEffect } from 'react'; export default function withLoggingHook(componentName: string) { return () =\u003e { useEffect(() =\u003e { console.log(`${componentName} mounted`); return () =\u003e { console.log(`${componentName} unmounted`); }; }, []); }; } Things to note:\nIt doesn‚Äôt know when to stop, and kindof keep spitting code at times - so I had to manually stop it 1.2 Writing an HOC that augments the decorates the logging instead of using the hook. PROMPT:\n// A react HOC, that wraps a given component and logs the component name // on mount and unmount. CODE OUTPUT (Attempt 1):\nimport React from 'react'; export function withLoggingHook\u003cT extends {}\u003e( WrappedComponent: React.ComponentType\u003cT\u003e, ) { return class WithLoggingHoc extends React.PureComponent\u003cT, {}\u003e { constructor(props: T) { super(props); console.log('Mounted ' + WrappedComponent.name); } componentWillUnmount() { console.log('Unmounted ' + WrappedComponent.name); } render() { return \u003cWrappedComponent {...this.props} /\u003e; } }; } // ./src/generated/withLoggingHook.test.tsx import React from 'react'; import { render, fireEvent } from '@testing-library/react'; import { withLoggingHook } from './withLoggingHook'; const TestComponent = () =\u003e \u003cdiv\u003eTest Component\u003c/div\u003e; describe('withLoggingHook', () =\u003e { beforeEach(() =\u003e { jest.spyOn(console, 'log'); }); afterEach(() =\u003e { jest.resetAllMocks(); }); it('should log when mounted and unmounted', () =\u003e { const ComponentWithLogging = withLoggingHook(TestComponent); const { rerender, unmount } = render(\u003cComponentWithLogging /\u003e); expect(console.log).toHaveBeenCalledWith('Mounted TestComponent'); rerender(\u003cdiv\u003e\u003c/div\u003e); expect(console.log).not.toHaveBeenLastCalledWith('Unmounted TestComponent'); unmount(); expect(console.log).toHaveBeenLastCalledWith('Unmounted TestComponent'); }); }); Mistakes: It used a constructor, instead of the componentDidMount method. added a fireEvent import, but didn‚Äôt use it later. Retry: I retried a couple more times. In the next attempt, it jumbled the output and got things completely wrong.\nOn attempt 3, it did pretty well though CODE OUTPUT (Attempt 3):\n// A react HOC, that wraps a given component and logs the component name // on mount and unmount. import React from 'react'; import { useEffect } from 'react'; import { ComponentType } from 'react'; export function withLoggingHook\u003cTProps\u003e( WrappedComponent: ComponentType\u003cTProps\u003e, componentName: string, ) { const WithLogging = (props: TProps) =\u003e { useEffect(() =\u003e { console.log(`${componentName} is mounted`); return () =\u003e { console.log(`${componentName} is unmounted`); }; }, []); return \u003cWrappedComponent {...props} /\u003e; }; WithLogging.displayName = `WithLogging(${getDisplayName(WrappedComponent)})`; return WithLogging; } function getDisplayName\u003cTProps\u003e(WrappedComponent: ComponentType\u003cTProps\u003e) { return WrappedComponent.displayName || WrappedComponent.name || 'Component'; } I also noticed, that asking it specifically what I want, returned the right output in the first attempt\nCODE OUTPUT (Attempt 4): asking explicitly what I want\n// A react HOC, that wraps a given component and logs the component name // on mount and unmount. Use effects, instead of componentDidMount / unmount import React from 'react'; export function withLoggingHook\u003cP extends object\u003e( Component: React.ComponentType\u003cP\u003e, ) { return (props: P): JSX.Element =\u003e { console.log('[withLoggingHook]', Component.name, 'mounted'); React.useEffect(() =\u003e { return () =\u003e console.log('[withLoggingHook]', Component.name, 'un-mounted'); }, []); return \u003cComponent {...props} /\u003e; }; } Verdict: It‚Äôs able to write basic hooks, but makes some errors with HOCs\nSometimes, it automatically wrote the test case for the code it generated, which was a pleasant surprise!\nRetrying a few times leads to automatically producing a better answer.\nComponent.name, is not defined, so that seems like a bug.\nPotential future experiments Experimenting with n, top_p ,and top_k parameters, and enabling the LLM to try multiple completions\nUsing a ‚ÄòVerifier‚Äô agent, that checks the completion for compilation and validity and retries\nGiving it concrete examples, that it can follow.\nTest 2: Write a test plan, and implement the test cases Next, let‚Äôs test its test case writing abilities\nTest 2.1 - Writing tests for withLogginHook In the previous test, it wrote a withLoggingHook, let‚Äôs use that as its context\nAs mentioned before, I used the vscode plugin I created previously, to give it context of the withLoggingHooks code\nPROMPT:\n// [FILE-NAME] ./src/generated/withLoggingHook.tsx // A react HOC, that wraps a given component and logs the component name // on mount and unmount. Use effects, instead of componentDidMount / unmount ... code from withLogginHook\u003cP\u003e ... // [FILE-NAME] generatedApps/DoctorFinderApp/doctor-finder-app/src/generated/withLoggingHook.spec.tsx // Test cases for withLoggingHook CODE OUTPUT:\nimport { render } from '@testing-library/react'; import { withLoggingHook } from './withLoggingHook'; describe('withLoggingHook', () =\u003e { beforeEach(() =\u003e { // Mock console.log to prevent unwanted output during testing jest.spyOn(console, 'log').mockImplementation(() =\u003e {}); }); afterEach(() =\u003e { // Restore original console.log after each test (console.log as jest.Mock).mockRestore(); }); it('should log the component name on mount', () =\u003e { const TestComponent = withLoggingHook((): null =\u003e null); render(\u003cTestComponent /\u003e); expect(console.log).toHaveBeenCalledWith( '[withLoggingHook]', 'TestComponent', 'mounted', ); }); it('should log the component name on unmount', () =\u003e { const TestComponent = withLoggingHook((): null =\u003e null); const { unmount } = render(\u003cTestComponent /\u003e); unmount(); expect(console.log).toHaveBeenCalledWith( '[withLoggingHook]', 'TestComponent', 'un-mounted', ); }); }); Mistakes: The test case in itself is fine, the issue with the withLoggingHook causes the tests to fail\nHowever, I did realise that multiple attempts on the same test case did not always lead to promising results.\nVerdict For simple test cases, it works quite well, but just barely.\nAnything more complex, it kinda makes too many bugs to be productively useful.\nTask 3: Excel Upload Component Let‚Äôs ask it to make a slightly more complicated component with some functionality\nPROMPT\n// A react component, that allows the user to upload excel files. // The excel file has one sheet, and that sheet has two columns // Columns headers are foo, and bar, and both should have number values. // When the file is uploaded, it is parsed using XLSX. // The final parsed values of foo and bar, are given back as an array to the callback. CODE OUTPUT:\nimport React from 'react'; import XLSX from 'xlsx'; interface ExcelUploadComponentProps { onFileLoad: (data: { foo: number[]; bar: number[] }) =\u003e void; } const ExcelUploadComponent: React.FC\u003cExcelUploadComponentProps\u003e = ({ onFileLoad, }: ExcelUploadComponentProps) =\u003e { const handleChange = (e: React.ChangeEvent\u003cHTMLInputElement\u003e) =\u003e { if (!e.target.files || e.target.files.length === 0) return; const file = e.target.files[0]; const reader = new FileReader(); reader.onload = (event: ProgressEvent\u003cFileReader\u003e) =\u003e { if (!event.target || !event.target.result) return; const data = event.target.result as string; const workbook = XLSX.read(data, { type: 'binary' }); const sheetName = workbook.SheetNames[0]; const worksheet = workbook.Sheets[sheetName]; // Convert the excel data to JSON const jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1 }); // Extract the foo and bar values from the JSON data const fooValues = []; const barValues = []; for (let i = 0; i \u003c jsonData.length; i++) { if (!jsonData[i]) continue; const row = jsonData[i]; fooValues.push(row[0]); barValues.push(row[1]); } onFileLoad({ foo: fooValues, bar: barValues }); }; reader.readAsBinaryString(file); }; return \u003cinput type=\"file\" accept=\".xls,.xlsx,.csv\" onChange={handleChange} /\u003e; }; export default ExcelUploadComponent; Verdict Now that was pretty good. I have no idea how to use the xlsx utility, and have not looked at its documentation\nEven though, I had to correct some typos and some other minor edits - this gave me a component that does exactly what I needed.\nI‚Äôd say this save me atleast 10-15 minutes of time googling for the api documentation and fumbling till I got it right.\nConclusion All in all, I think having locally running LLMs that can help us with code is possibly very near\nThese current models, while don‚Äôt really get things correct always, do provide a pretty handy tool and in situations where new territory / new apps are being made, I think they can make significant progress.\nSomething to note, is that once I provide more longer contexts, the model seems to make a lot more errors. This is potentially only model specific, so future experimentation is needed here.\nWhat‚Äôs next There were quite a few things I didn‚Äôt explore here. I will cover those in future posts.\nHere‚Äôs a list of a few things I‚Äôm going to experiment next\nProviding more examples of good code, instead of trying to explicitly mention every detail we want\nComparing other models on similar exercises. Possibly making a benchmark test suite to compare them against.\nTrying multi-agent setups. I having another LLM that can correct the first ones mistakes, or enter into a dialogue where two minds reach a better outcome is totally possible.\nA hint on this, is that once it gets something wrong, and I add the mistake to the prompt - the next iteration of the output is usually much better.\n","wordCount":"4880","inLanguage":"en","image":"https://Kshitij-Banerjee.github.io/exploring_code_llms.png","datePublished":"2024-04-14T00:00:00Z","dateModified":"2024-04-14T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2024/04/14/exploring-code-llms/"},"publisher":{"@type":"Organization","name":"KiloBytes by KB","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io/ accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Exploring Code LLMs</h1><div class=post-meta><span title='2024-04-14 00:00:00 +0000 UTC'>April 14, 2024</span></div></header><figure class=entry-cover><img loading=eager src=https://Kshitij-Banerjee.github.io/exploring_code_llms.png alt></figure><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>The goal of this post is to deep-dive into LLM&rsquo;s that are <strong>specialised in code generation tasks</strong>, and see if we can use them to write code.</p><p>Note: Unlike copilot, we&rsquo;ll focus on <em>locally running LLM&rsquo;s</em>. This should be appealing to any developers working in enterprises that have data privacy and sharing concerns, but still want to improve their developer productivity with locally running models.</p><p>To test our understanding, we&rsquo;ll perform a few simple coding tasks, and compare the various methods in achieving the desired results and also show the shortcomings.</p><h2 id=the-goal---a-few-simple-coding-task>The goal - A few simple coding task<a hidden class=anchor aria-hidden=true href=#the-goal---a-few-simple-coding-task>#</a></h2><ol><li><p>Test 1: Generate a higher-order-component / decorator that enables logging on a react component</p></li><li><p>Test 2: Write a test plan, and implement the test cases</p></li><li><p>Test 3: Parse an uploaded excel file in the browser.</p></li></ol><h1 id=how-the-rest-of-the-post-is-structured>How the rest of the post is structured<a hidden class=anchor aria-hidden=true href=#how-the-rest-of-the-post-is-structured>#</a></h1><p>We&rsquo;re going to cover some theory, explain how to setup a locally running LLM model, and then finally conclude with the test results.</p><p><strong>Part 1: Quick theory</strong></p><p>Instead of explaining the concepts in painful detail, I&rsquo;ll refer to papers and quote specific interesting points that provide a summary. For a detailed reading, refer to the papers and links I&rsquo;ve attached.</p><ol><li><p><em>Instruction Fine-tuning</em>: Why instruction fine-tuning leads to much smaller models that can perform quite well on specific tasks, compared to much larger models</p></li><li><p><em>Open source models available</em>: A quick intro on mistral, and deepseek-coder and their comparison.</p></li><li><p><em>Model Quantization</em>: How we can significantly improve model inference costs, by improving memory footprint via using less precision weights.</p></li></ol><p><strong>If you know all of the above, you may want to skip to <a href=#part-2-setting-up-the-environment-ollama-on-m1>Part 2</a></strong></p><p><strong>Part 2: Local LLM Setup</strong></p><p>Using Ollama and setting up my VSCode extension</p><p>VSCode Extension available here: <a href=https://github.com/Kshitij-Banerjee/kb-ollama-coder>https://github.com/Kshitij-Banerjee/kb-ollama-coder</a></p><p><strong>Part 3: Test Results</strong></p><p>Showing results on all 3 tasks outlines above.</p><h1 id=part-1-understanding-instruction-finetuning>[Part 1] Understanding Instruction Finetuning<a hidden class=anchor aria-hidden=true href=#part-1-understanding-instruction-finetuning>#</a></h1><p>Before we venture into our evaluation of coding efficient LLMs. Let&rsquo;s quickly discuss what &ldquo;Instruction Fine-tuning&rdquo; really means.</p><p>We refer to this paper: <a href=https://arxiv.org/pdf/2203.02155.pdf>Training language models to follow instructions with human feedback</a></p><h3 id=why-instruction-fine-tuning->Why instruction fine-tuning ?<a hidden class=anchor aria-hidden=true href=#why-instruction-fine-tuning->#</a></h3><blockquote><p>predicting the next token on a webpage from the internet‚Äîis different from the objective ‚Äúfollow the user‚Äôs instructions helpfully and safely‚Äù</p></blockquote><h3 id=performance-implications>Performance Implications<a hidden class=anchor aria-hidden=true href=#performance-implications>#</a></h3><blockquote><p>In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer paramete</p></blockquote><h3 id=how-they-did-it>How they did it?<a hidden class=anchor aria-hidden=true href=#how-they-did-it>#</a></h3><blockquote><p>SpeciÔ¨Åcally, we use reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Stiennon et al., 2020) to Ô¨Åne-tune GPT-3 to follow a broad class of written instructions. This technique uses human preferences as a reward signal to Ô¨Åne-tune our models. We Ô¨Årst hire a team of 40 contractors to label our data, based on their performance on a screening tes
We then collect a dataset of human-written demonstrations of the desired output behavior on (mostly English) prompts submitted to the OpenAI API3 and some labeler-written prompts, and use this to train our supervised learning baselines. Next, we collect a dataset of human-labeled comparisons between outputs from our models on a larger set of API prompts. We then train a reward model (RM) on this dataset to predict which model output our labelers would prefer. Finally, we use this RM as a reward function and Ô¨Åne-tune our supervised learning baseline to maximize this reward using the PPO algorithm</p></blockquote><h3 id=paper-results>Paper Results<a hidden class=anchor aria-hidden=true href=#paper-results>#</a></h3><blockquote><p>We call the resulting models InstructGPT.
<img loading=lazy src=/image_1711792916760_0.png alt=image.png>
{:height 636, :width 1038}</p></blockquote><blockquote><p>On the TruthfulQA benchmark, InstructGPT generates truthful and informative answers about twice as often as GPT-3
During RLHF Ô¨Åne-tuning, we observe performance regressions compared to GPT-3
We can greatly reduce the performance regressions on these datasets by mixing PPO updates with updates that increase the log likelihood of the pretraining distribution (PPO-ptx), without compromising labeler preference scores.
InstructGPT still makes simple mistakes. For example, InstructGPT can still fail to follow instructions, make up facts, give long hedging answers to simple questions, or fail to detect instructions with false premises$$</p></blockquote><h3 id=some-notes-on-rlhf>Some notes on RLHF<a hidden class=anchor aria-hidden=true href=#some-notes-on-rlhf>#</a></h3><h4 id=step-1-supervised-fine-tuning>Step 1: Supervised Fine Tuning:<a hidden class=anchor aria-hidden=true href=#step-1-supervised-fine-tuning>#</a></h4><blockquote><p>We Ô¨Åne-tune GPT-3 on our labeler demonstrations using supervised learning. We trained for 16 epochs, using a cosine learning rate decay, and residual dropout of 0.2</p></blockquote><h4 id=step-2---reward-model>Step 2 : Reward model<a hidden class=anchor aria-hidden=true href=#step-2---reward-model>#</a></h4><blockquote><p>Starting from the SFT model with the Ô¨Ånal unembedding layer removed, we trained a model to take in a prompt and response, and output a scalar reward
The underlying goal is to get a model or system that takes in a sequence of text, and returns a scalar reward which should numerically represent the human preference.
These reward models are themselves pretty huge. 6B parameters in Open AI case</p></blockquote><h4 id=step-3-fine-tuning-with-rl-using-ppo>Step 3: Fine-tuning with RL using PPO<a hidden class=anchor aria-hidden=true href=#step-3-fine-tuning-with-rl-using-ppo>#</a></h4><h5 id=ppo--hahahugoshortcode9s1hbhb>PPO : <a href=https://arxiv.org/pdf/1707.06347.pdf>Proximal Policy Optimization</a></h5><blockquote><p>Given the prompt and response, it produces a reward determined by the reward model and ends the episode. In addition, we add a per-token KL penalty from the SFT model at each token to mitigate overoptimization of the reward model. The value function is initialized from the RM. We call these models ‚ÄúPPO.‚Äù</p></blockquote><h6 id=from--httpshuggingfacecoblogrlhf>From : <a href=https://huggingface.co/blog/rlhf>https://huggingface.co/blog/rlhf</a><a hidden class=anchor aria-hidden=true href=#from--httpshuggingfacecoblogrlhf>#</a></h6><blockquote><p>&ldquo;Let&rsquo;s first formulate this fine-tuning task as a RL problem. First, the¬†<strong>policy</strong>¬†is a language model that takes in a prompt and returns a sequence of text (or just probability distributions over text). The¬†<strong>action space</strong>¬†of this policy is all the tokens corresponding to the vocabulary of the language model (often on the order of 50k tokens) and the¬†<strong>observation space</strong>¬†is the distribution of possible input token <em>sequences</em>, which is also quite large given previous uses of RL (the dimension is approximately the size of vocabulary ^ length of the input token sequence). The¬†<strong>reward function</strong>¬†is a combination of the preference model and a constraint on policy shift.&rdquo;
Concatenated with the original prompt, that text is passed to the preference model, which returns a scalar notion of ‚Äúpreferability‚Äù,¬†r<em>Œ∏</em>‚Äã. In addition, per-token probability distributions from the RL policy are compared to the ones from the initial model to compute a penalty on the difference between them. In multiple papers from OpenAI, Anthropic, and DeepMind, this penalty has been designed as a scaled version of the Kullback‚ÄìLeibler¬†<a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence>(KL) divergence</a>¬†between these sequences of distributions over tokens,¬†r_kl. The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets.
Finally, the¬†<strong>update rule</strong>¬†is the parameter update from PPO that maximizes the reward metrics in the current batch of data (PPO is on-policy, which means the parameters are only updated with the current batch of prompt-generation pairs). PPO is a trust region optimization algorithm that uses constraints on the gradient to ensure the update step does not destabilize the learning process.</p></blockquote><h4 id=helpful-schematic-showing-the-rl-fine-tune-process>Helpful schematic showing the RL fine-tune process<a hidden class=anchor aria-hidden=true href=#helpful-schematic-showing-the-rl-fine-tune-process>#</a></h4><p><img loading=lazy src=https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/rlhf/rlhf.png alt="RL Process"></p><h3 id=final-thoughts>Final Thoughts<a hidden class=anchor aria-hidden=true href=#final-thoughts>#</a></h3><p>InstructGPT outputs are more appropriate in the context of a customer assistant, more often follow explicit constraints deÔ¨Åned in the instruction (e.g. ‚ÄúWrite your answer in 2 paragraphs or less.‚Äù),</p><p>Are less likely to fail to follow the correct instruction entirely,</p><p>Are less likely to make up facts (‚Äòhallucinate‚Äô) less often in closed-domain tasks. These results suggest that InstructGPT models are more reliable and easier to control than GPT-3</p><h4 id=comparison-of-gpt-vs-instruct-gpt>Comparison of GPT vs Instruct GPT<a hidden class=anchor aria-hidden=true href=#comparison-of-gpt-vs-instruct-gpt>#</a></h4><p><img loading=lazy src=/image_1711810033442_0.png alt=image.png></p><h1 id=part-1-deep-dive-into-mistral-models>[Part 1] Deep dive into Mistral Models<a hidden class=anchor aria-hidden=true href=#part-1-deep-dive-into-mistral-models>#</a></h1><h2 id=brief-introduction-to-mistral-models-their-architecture-and-key-features>Brief introduction to Mistral models, their architecture, and key features<a hidden class=anchor aria-hidden=true href=#brief-introduction-to-mistral-models-their-architecture-and-key-features>#</a></h2><p>We refer to the paper: <a href=https://arxiv.org/pdf/2310.06825.pdf>Mistral 7b</a></p><h3 id=objective>Objective:<a hidden class=anchor aria-hidden=true href=#objective>#</a></h3><blockquote><p>The search for balanced models delivering both high-level performance and efficiency</p></blockquote><h3 id=key-results>Key Results:<a hidden class=anchor aria-hidden=true href=#key-results>#</a></h3><blockquote><p>Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25 ]) in mathematics and code generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20 ], without sacrificing performance on non-code related benchmarks</p></blockquote><h3 id=key-insights>Key Insights:<a hidden class=anchor aria-hidden=true href=#key-insights>#</a></h3><blockquote><p>Mistral 7B leverages grouped-query attention (GQA) [ 1 ], and sliding window attention (SWA) [6, 3]. GQA significantly accelerates the inference speed, and also reduces the memory requirement during decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced computational cost</p></blockquote><h3 id=sliding-window-attention>Sliding Window Attention<a hidden class=anchor aria-hidden=true href=#sliding-window-attention>#</a></h3><h4 id=why>Why?<a hidden class=anchor aria-hidden=true href=#why>#</a></h4><blockquote><p>The number of operations in vanilla attention is quadratic in the sequence length, and the memory increases linearly with the number of tokens. At inference time, this incurs higher latency and smaller throughput due to reduced cache availability. To alleviate this issue, we use sliding window attention: each token can attend to at most W tokens from the previous layer
<img loading=lazy src=/image_1711811647174_0.png alt=image.png></p></blockquote><blockquote><p>Note that tokens outside the sliding window still influence next word prediction. At each attention layer, information can move forward by W tokens. Hence, after k attention layers, information can move forward by up to k √ó W tokens
SWA exploits the stacked layers of a transformer to attend information beyond the window size W . The hidden state in position i of the layer k, hi, attends to all hidden states from the previous layer with positions between i ‚àí W and i. Recursively, hi can access tokens from the input layer at a distance of up to W √ó k tokens, as illustrated in Figure 1. At the last layer, using a window size of W = 4096, we have a theoretical attention span of approximately131K tokens. In practice, for a sequence length of 16K and W = 4096, changes made to FlashAttention [ 11 ] and xFormers [18 ] yield a 2x speed improvement over a vanilla attention baseline.
This fixed attention span, means we can implement a rolling buffer cache. After W size, the cache starts overwriting the from the beginning. This also allows some pre-filling based optimizations.</p></blockquote><h3 id=comparison-with-llama>Comparison with Llama<a hidden class=anchor aria-hidden=true href=#comparison-with-llama>#</a></h3><p><img loading=lazy src=/image_1711812059350_0.png alt=image.png></p><h3 id=instruction-finetuning>Instruction Finetuning<a hidden class=anchor aria-hidden=true href=#instruction-finetuning>#</a></h3><blockquote><p>To evaluate the generalization capabilities of Mistral 7B, we fine-tuned it on instruction datasets publicly available on the Hugging Face repository. No proprietary data or training tricks were utilized: <strong>Mistral 7B ‚Äì Instruct model is a simple and preliminary demonstration that the base model can easily be fine-tuned to achieve good performance.</strong></p></blockquote><h3 id=system-prompt>System prompt<a hidden class=anchor aria-hidden=true href=#system-prompt>#</a></h3><blockquote><p>We introduce a system prompt (see below) to guide the model to generate answers within specified guardrails, similar to the work done with Llama 2.
The prompt: &ldquo;Always assist with care, respect, and truth. Respond with utmost utility yet securely. Avoid harmful, unethical, prejudiced, or negative content. Ensure replies promote fairness and positivity.&rdquo;</p></blockquote><h2 id=how-to-select-various-model-sizes-a-thumbrule-from-mistral-ai>How to select various model sizes, a Thumbrule from Mistral AI.<a hidden class=anchor aria-hidden=true href=#how-to-select-various-model-sizes-a-thumbrule-from-mistral-ai>#</a></h2><p>Refer to this article: <a href=https://docs.mistral.ai/guides/model-selection/>Model Selection</a></p><p><em>TL;DR</em></p><p>Small Tasks (Custom support, classification) => use <strong>Mistral Small</strong></p><p>Medium Tasks (Data Extraction, Summarizing Documents, Writing emails.. ) => <strong>Mistral medium</strong></p><p>Complex Tasks (Code Generation, RAG) => <strong>Mistral Large</strong></p><p><img loading=lazy src=https://docs.mistral.ai/img/guides/modelselection1.png alt=drawing></p><p>Benchmark on coding :</p><p><img loading=lazy src=/image_1711862922548_0.png alt=image.png></p><table><thead><tr><th>Model</th><th>MMLU</th><th>hellaswag (10-shot)</th><th>winograde (5-shot)</th><th>arc challenge (25-shot)</th><th>TriviaQA (5-shot)</th><th>TruthfulQA</th></tr></thead><tbody><tr><td>Mistral 7B</td><td>62.5%</td><td>83.1%</td><td>78.0%</td><td>78.1%</td><td>68.8%</td><td>42.35%</td></tr><tr><td>Mixtral 8x7B</td><td>70.6%</td><td>86.7%</td><td>81.2%</td><td>85.8%</td><td>78.38%</td><td>47.5%</td></tr><tr><td>Mistral Small</td><td>72.2%</td><td>86.9%</td><td>84.7%</td><td>86.9%</td><td>79.5%</td><td>51.7%</td></tr><tr><td>Mistral Medium</td><td>75.3%</td><td>88.0%</td><td>88%</td><td>89.9%</td><td>81.1%</td><td>47%</td></tr><tr><td>Mistral Large</td><td>81.2%</td><td>89.2%</td><td>86.7%</td><td>94.0%</td><td>82.7%</td><td>50.6%</td></tr></tbody></table><h1 id=part-1-deepseek-coder-an-upgrade>[Part 1] Deepseek Coder, an upgrade?<a hidden class=anchor aria-hidden=true href=#part-1-deepseek-coder-an-upgrade>#</a></h1><h2 id=overview-of-eval-metrics>Overview of Eval metrics<a hidden class=anchor aria-hidden=true href=#overview-of-eval-metrics>#</a></h2><p>Before we understand and compare deepseeks performance, here&rsquo;s a quick overview on how models are measured on code specific tasks.</p><p>Leaderboard is provided here : <a href=https://evalplus.github.io/leaderboard.html>https://evalplus.github.io/leaderboard.html</a></p><h3 id=what-is-humaneval->What is HumanEval ?<a hidden class=anchor aria-hidden=true href=#what-is-humaneval->#</a></h3><p><a href=https://github.com/openai/human-eval>https://github.com/openai/human-eval</a></p><p>HumanEval consists of 164 hand-written Python problems that are validated using test cases to assess the code generated by a Code LLM in a zero-shot setting,</p><h3 id=what-is-mbpp->What is MBPP ?<a hidden class=anchor aria-hidden=true href=#what-is-mbpp->#</a></h3><p><a href=https://huggingface.co/datasets/mbpp>https://huggingface.co/datasets/mbpp</a></p><p>While the MBPP benchmark includes 500 problems in a few-shot setting.</p><h3 id=ds-1000-more-practical-programming-tasks-compared-to-human-eval>DS-1000: More practical programming tasks, compared to Human Eval<a hidden class=anchor aria-hidden=true href=#ds-1000-more-practical-programming-tasks-compared-to-human-eval>#</a></h3><p>DS-1000 benchmark, as introduced in the work by Lai et al. (2023), offers a comprehensive collection of 1,000 <strong>practical</strong> and realistic data science workflows across seven different libraries</p><h2 id=deepseek-coder>Deepseek coder<a hidden class=anchor aria-hidden=true href=#deepseek-coder>#</a></h2><h3 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h3><blockquote><p>Each model in the series has been trained from scratch on 2 trillion tokens sourced from 87 programming languages, ensuring a comprehensive understanding of coding languages and syntax.
Refer to the paper from DeepSeek coder: <a href=https://arxiv.org/pdf/2401.14196.pdf>DeepSeek Code</a></p></blockquote><p>Useful links:</p><p><a href=https://deepseekcoder.github.io/>https://deepseekcoder.github.io/</a></p><p><a href=https://ollama.com/library/deepseek-coder/tags>https://ollama.com/library/deepseek-coder/tags</a></p><h3 id=how-its-built>How it&rsquo;s built<a hidden class=anchor aria-hidden=true href=#how-its-built>#</a></h3><h3 id=repository-context-in-pre-training>Repository Context in Pre-training<a hidden class=anchor aria-hidden=true href=#repository-context-in-pre-training>#</a></h3><blockquote><p>Besides, we attempt to organize the pretraining data at the repository level to enhance the pre-trained model‚Äôs understanding capability within the context of cross-files within a repository
They do this, by doing a topological sort on the dependent files and appending them into the context window of the LLM. More details below.</p></blockquote><blockquote><p>We find that it can significantly boost the capability of cross-file code generation</p></blockquote><h3 id=next-token-prediction--fill-in-the-middle--like-bert>Next token prediction + Fill-in-the middle (like BERT)<a hidden class=anchor aria-hidden=true href=#next-token-prediction--fill-in-the-middle--like-bert>#</a></h3><blockquote><p>In addition to employing the next token prediction loss during pre-training, we have also incorporated the Fill-In-Middle (FIM) approach.</p></blockquote><h3 id=16k-context-window-mistral-models-have-4k-sliding-window-attention>16K context window (Mistral models have 4K sliding window attention)<a hidden class=anchor aria-hidden=true href=#16k-context-window-mistral-models-have-4k-sliding-window-attention>#</a></h3><blockquote><p>To meet the requirements of handling longer code inputs, we have extended the context length to 16K. This adjustment allows our models to handle more complex and extensive coding tasks, thereby increasing their versatility and applicability in various coding scenarios</p></blockquote><h3 id=data-preparation>Data Preparation<a hidden class=anchor aria-hidden=true href=#data-preparation>#</a></h3><p><img loading=lazy src=/image_1711865913142_0.png alt=image.png></p><h3 id=filtering-rule>Filtering Rule:<a hidden class=anchor aria-hidden=true href=#filtering-rule>#</a></h3><p><em>TL;DR:</em> Remove non-code related, or data heavy files</p><ol><li><p>Remove files with avg line length > 100, OR, maximum line length > 1000 characters.</p></li><li><p>Remove files with fewer than 25% alphabetic characters</p></li><li><p>remove &lt;?xml version files</p></li><li><p>JSON/YAML files - keep fields that have character counts ranging from 50 -> 5000 . This removes data-heavy files.</p></li></ol><h3 id=dependency-parsing>Dependency Parsing<a hidden class=anchor aria-hidden=true href=#dependency-parsing>#</a></h3><p>Instead of simply passing in the current file, the dependent files within repository are parsed.</p><p>Parse Dependency between files, then arrange files in order that ensures context of each file is <em>before</em> the code of the current file. By aligning files based on dependencies, it accurately represents real coding practices and structures.</p><blockquote><p>This enhanced alignment not only makes our dataset more relevant but also potentially increases the practicality and applicability of the model in handling project-level code scenarios
It‚Äôs worth noting that we only consider the invocation relationships between files and use regular expressions to extract them, such as"import" in Python, &ldquo;using&rdquo; in C#, and &ldquo;include&rdquo; in C.
A topological sort algorithm for doing this is provided in the paper.</p></blockquote><blockquote><p>To incorporate file path information, a comment indicating the file‚Äôs path is added at the beginning of each file.</p></blockquote><h3 id=model-architecture>Model Architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h3><blockquote><p>Each model is a decoder-only Transformer, incorporating Rotary Position Embedding (RoPE)
Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) as described by Su et al. (2023), with a group size of 8, enhancing both training and inference efficiency. Additionally, we employ FlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism
we use AdamW (Loshchilov and Hutter, 2019) as the optimizer with ùõΩ1 and ùõΩ2 values of 0.9 and 0.95.
he learning rate at each stage is scaled down to‚àöÔ∏É 110 of the preceding stage‚Äôs rate
Context Length:</p></blockquote><blockquote><p>Theoretically, these modifications enable our model to process up to 64K tokens in context. However, empirical observations suggest that the model delivers its most reliable outputs within a 16K token range.\</p></blockquote><h3 id=instruction-tuning>Instruction Tuning<a hidden class=anchor aria-hidden=true href=#instruction-tuning>#</a></h3><blockquote><p>This data comprises helpful and impartial human instructions, structured by the Alpaca Instruction format. To demarcate each dialogue turn, we employed a unique delimiter token &lt;|EOT|></p></blockquote><h3 id=performance>Performance<a hidden class=anchor aria-hidden=true href=#performance>#</a></h3><p>Surpasses GPT3.5, and within reach of GPT4</p><p><img loading=lazy src=/image_1711864589567_0.png alt=image.png></p><blockquote><p>To evaluate the model‚Äôs multilingual capabilities, we expanded the Python problems of Humaneval Benchmark to seven additional commonly used programming languages, namely C++, Java, PHP, TypeScript (TS), C#, Bash, and JavaScript (JS) (Cassano et al.,2023). For both benchmarks, We adopted a greedy search approach and re-implemented the baseline results using the same script and environment for fair comparison.
<img loading=lazy src=/image_1711869248083_0.png alt=image.png></p></blockquote><h3 id=interesting-notes>Interesting Notes<a hidden class=anchor aria-hidden=true href=#interesting-notes>#</a></h3><p>Chain of thought prompting</p><blockquote><p>Our analysis indicates that the implementation of Chain-of-Thought (CoT) prompting notably enhances the capabilities of DeepSeek-Coder-Instruct models. This improvement becomes particularly evident in the more challenging subsets of tasks. By adding the directive, &ldquo;You need first to write a step-by-step outline and then write the code.&rdquo; following the initial prompt, we have observed enhancements in performance.
This observation leads us to believe that the process of first crafting detailed code descriptions assists the model in more effectively understanding and addressing the intricacies of logic and dependencies in coding tasks, particularly those of higher complexity. Therefore, we strongly recommend employing CoT prompting strategies when utilizing DeepSeek-Coder-Instruct models for complex coding challenges.</p></blockquote><h1 id=part-1-model-quantization>[Part 1] Model Quantization<a hidden class=anchor aria-hidden=true href=#part-1-model-quantization>#</a></h1><p>Along with instruction fine-tuning, another neat technique that makes LLM&rsquo;s more performant (in terms of memory and resources), is model quantization</p><p>Model quantization enables one to reduce the memory footprint, and improve inference speed - with a tradeoff against the accuracy.</p><p>In short, Quantization is a process from <strong>moving the weights of the model, from a high-information type like fp32 to a low-information but performant data-type like int8</strong></p><p>Reference: Huggingface guide on quantization - <a href=https://huggingface.co/docs/optimum/en/concept_guides/quantization>https://huggingface.co/docs/optimum/en/concept_guides/quantization</a></p><p>The two most common quantization cases are¬†<code>float32 -> float16</code>¬†and¬†<code>float32 -> int8</code>.</p><p>Some schematics that explain the concept.</p><p><img loading=lazy src=https://miro.medium.com/v2/resize:fit:516/1*Jlq_cyLvRdmp_K5jCd3LkA.png alt="Model Quantization 1: Basic Concepts | by Florian June | Medium"></p><p><img loading=lazy src=https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-1b.png alt="Model Quantization: single precision, half precision, 8-bit integer">
{:height 362, :width 719}</p><h2 id=quantization-to-int8>Quantization to Int8<a hidden class=anchor aria-hidden=true href=#quantization-to-int8>#</a></h2><p>Let‚Äôs consider a float¬†<code>x</code>¬†in¬†<code>[a, b]</code>, then we can write the following quantization scheme, also called the¬†<em>affine quantization scheme</em>:</p><pre tabindex=0><code>x = S * (x_q - Z)
</code></pre><p><code>x_q</code>¬†is the quantized¬†<code>int8</code>¬†value associated to¬†<code>x</code></p><p><code>S</code>¬†is the scale, and is a positive¬†<code>float32</code></p><p><code>Z</code>¬†is called the zero-point, it is the¬†<code>int8</code>¬†value corresponding to the value¬†<code>0</code>¬†in the¬†<code>float32</code>¬†realm.</p><pre tabindex=0><code>x_q = round(x/S + Z)
</code></pre><pre tabindex=0><code>x_q = clip(round(x/S + Z), round(a/S + Z), round(b/S + Z))
</code></pre><p>In effect, this means that we clip the ends, and perform a scaling computation in the middle. The clip-off obviously will lose to accuracy of information, and so will the rounding.</p><h2 id=calibration>Calibration<a hidden class=anchor aria-hidden=true href=#calibration>#</a></h2><p>An example, explaining calibration to optimise clipping vs rounding error</p><p><img loading=lazy src=https://deci.ai/wp-content/uploads/2023/02/deci-quantization-blog-2a.jpg alt="Model Quantization: Calibration">
{:height 187, :width 314}</p><p>To ensure that we have a good balance of clipping vs rounding errors, based on the range [a, b] that we select. Some techniques are available</p><p><strong>Use per-channel granularity for weights and per-tensor for activations</strong></p><p><strong>Quantize residual connections separately by replacing blocks</strong></p><p><strong>Identify sensitive layers and skip them from quantization</strong></p><p><a href=https://huggingface.co/blog/4bit-transformers-bitsandbytes>https://huggingface.co/blog/4bit-transformers-bitsandbytes</a></p><p>Model quantization + instruct = <em>Quite Good</em> results</p><p>Good reference reading on the topic: <a href=https://deci.ai/quantization-and-quantization-aware-training>https://deci.ai/quantization-and-quantization-aware-training</a></p><h1 id=part-2-setting-up-the-environment-ollama-on-m1>[Part 2] Setting Up the Environment: Ollama on M1<a hidden class=anchor aria-hidden=true href=#part-2-setting-up-the-environment-ollama-on-m1>#</a></h1><h2 id=option-1-hosting-the-model>Option 1: Hosting the model<a hidden class=anchor aria-hidden=true href=#option-1-hosting-the-model>#</a></h2><p>To host the models, I chose the ollama project: <a href=https://ollama.com/>https://ollama.com/</a></p><p>Ollama is essentially, docker for LLM models and allows us to quickly run various LLM&rsquo;s and host them over standard completion APIs locally.</p><p>The website and documentation is pretty self-explanatory, so I wont go into the details of setting it up.</p><h2 id=option-2-my-machine-is-not-strong-enough-but-id-like-to-experiment>Option 2: My machine is not strong enough, but I&rsquo;d like to experiment<a hidden class=anchor aria-hidden=true href=#option-2-my-machine-is-not-strong-enough-but-id-like-to-experiment>#</a></h2><p>If your machine doesn&rsquo;t support these LLM&rsquo;s well (unless you have an M1 and above, you&rsquo;re in this category), then there is the following alternative solution I&rsquo;ve found.</p><p>You can rent machines relatively cheaply (~0.4$ / hour) for inference methods, using <a href=https://vast.ai/>vast.ai</a></p><p>Once you&rsquo;ve setup an account, added your billing methods, and have copied your API key from settings.</p><p>Clone the <a href=https://github.com/g1ibby/llm-deploy>llm-deploy repo</a>, and follow the instructions.</p><p>This repo figures out the cheapest available machine and hosts the ollama model as a docker image on it.</p><p>From 1 and 2, you should now have a hosted LLM model running. Now we need VSCode to call into these models and produce code.</p><h2 id=vscode-extension-calling-into-the-model>VSCode Extension Calling into the Model<a hidden class=anchor aria-hidden=true href=#vscode-extension-calling-into-the-model>#</a></h2><p>Given the above best practices on how to provide the model its context, and the prompt engineering techniques that the authors suggested have positive outcomes on result. I created a VSCode plugin that implements these techniques, and is able to interact with Ollama running locally.</p><p>The source code for this plugin is available here:</p><p><a href=https://github.com/Kshitij-Banerjee/kb-ollama-coder>https://github.com/Kshitij-Banerjee/kb-ollama-coder</a></p><p>This plugin achieves the following:-</p><p>It provides the LLM context on project/repository relevant files.</p><p>The plugin not only pulls the current file, but also loads all the currently open files in Vscode into the LLM context.</p><p>It then trims the context to the last 16000/24000 characters (configurable)</p><p>This is an approximation, as deepseek coder enables 16K tokens, and approximate that each token is 1.5 tokens. In practice, I believe this can be much higher - so setting a higher value in the configuration should also work.</p><p>It adds a header prompt, based on the guidance from the paper. (Configurable) Example:-</p><p>&ldquo;You need to first write a step-by-step outline and then write the code. The following is a complete {LANG} file named {FILE_NAME} in the project {PROJECT_NAME}. Anything NOT code is written as a CODE COMMENT.&rdquo;</p><h2 id=source-code>Source code<a hidden class=anchor aria-hidden=true href=#source-code>#</a></h2><p>The source code for this plugin is available here:</p><p><a href=https://github.com/Kshitij-Banerjee/kb-ollama-coder>https://github.com/Kshitij-Banerjee/kb-ollama-coder</a></p><p>For completeness, there are other VSCode plugins that might also solve a similar problem - and I am yet to try them</p><p><a href=https://github.com/rjmacarthy/twinny>https://github.com/rjmacarthy/twinny</a></p><p><a href=https://github.com/smallcloudai/refact>https://github.com/smallcloudai/refact</a></p><p><a href=https://github.com/TabbyML/tabby>https://github.com/TabbyML/tabby</a></p><h1 id=part-3-testing-deepseekcoder>[Part 3] Testing DeepseekCoder<a hidden class=anchor aria-hidden=true href=#part-3-testing-deepseekcoder>#</a></h1><h2 id=general-observations>General Observations<a hidden class=anchor aria-hidden=true href=#general-observations>#</a></h2><p>For the most part, the 7b instruct model was quite useless and produces mostly error and incomplete responses.</p><p>The 33b models can do quite a few things correct. However, its probably at the brink of usefuleness</p><p>The model doesn&rsquo;t really understand writing test cases at all.</p><h2 id=test-setting>Test Setting<a hidden class=anchor aria-hidden=true href=#test-setting>#</a></h2><p>For all the tests, I used a temperature setting of 0, as the guideline is to have lower temperatures for tasks like coding, instead of creative generation tasks (I find coding a pretty creative activity though, but just going with the guidelines for now)</p><h2 id=test-1-mountunmount-logging-using-hooks-and-hoc>Test 1: Mount/Unmount logging using hooks and HOC<a hidden class=anchor aria-hidden=true href=#test-1-mountunmount-logging-using-hooks-and-hoc>#</a></h2><p>In this task, I wanted to test its ability to understand basic react concepts of useEffect and hooks / HOC</p><h3 id=11-writing-custom-hook-with-mountunmount-logging>1.1 Writing custom hook with mount/unmount logging<a hidden class=anchor aria-hidden=true href=#11-writing-custom-hook-with-mountunmount-logging>#</a></h3><p><em>PROMPT</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// A react custom hook that logs the component name on mount and unmount
</span></span></span></code></pre></div><p><em>CODE OUTPUT:</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>useEffect</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>default</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>withLoggingHook</span>(<span style=color:#a6e22e>componentName</span>: <span style=color:#66d9ef>string</span>) {
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#a6e22e>useEffect</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>      <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>componentName</span><span style=color:#e6db74>}</span><span style=color:#e6db74> mounted`</span>);
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>        <span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>componentName</span><span style=color:#e6db74>}</span><span style=color:#e6db74> unmounted`</span>);
</span></span><span style=display:flex><span>      };
</span></span><span style=display:flex><span>    }, []);
</span></span><span style=display:flex><span>  };
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><strong>Things to note:</strong></p><ul><li>It doesn&rsquo;t know when to stop, and kindof keep spitting code at times - so I had to manually stop it</li></ul><h3 id=12-writing-an-hoc-that-augments-the-decorates-the-logging-instead-of-using-the-hook>1.2 Writing an HOC that augments the decorates the logging instead of using the hook.<a hidden class=anchor aria-hidden=true href=#12-writing-an-hoc-that-augments-the-decorates-the-logging-instead-of-using-the-hook>#</a></h3><p><em>PROMPT:</em></p><pre tabindex=0><code class=language-tyepscript data-lang=tyepscript>// A react HOC, that wraps a given component and logs the component name
// on mount and unmount.
</code></pre><p><em>CODE OUTPUT (Attempt 1):</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>React</span> <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>withLoggingHook</span>&lt;<span style=color:#f92672>T</span> <span style=color:#a6e22e>extends</span> {}&gt;(
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>WrappedComponent</span>: <span style=color:#66d9ef>React.ComponentType</span>&lt;<span style=color:#f92672>T</span>&gt;,
</span></span><span style=display:flex><span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>WithLoggingHoc</span> <span style=color:#66d9ef>extends</span> <span style=color:#a6e22e>React</span>.<span style=color:#a6e22e>PureComponent</span>&lt;<span style=color:#f92672>T</span><span style=color:#960050;background-color:#1e0010>,</span> {}&gt; {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>constructor</span>(<span style=color:#a6e22e>props</span>: <span style=color:#66d9ef>T</span>) {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>super</span>(<span style=color:#a6e22e>props</span>);
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>&#39;Mounted &#39;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>WrappedComponent</span>.<span style=color:#a6e22e>name</span>);
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>componentWillUnmount() {</span>
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>&#39;Unmounted &#39;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>WrappedComponent</span>.<span style=color:#a6e22e>name</span>);
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>render() {</span>
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>return</span> &lt;<span style=color:#f92672>WrappedComponent</span> {<span style=color:#a6e22e>...this.props</span>} /&gt;;
</span></span><span style=display:flex><span>		}
</span></span><span style=display:flex><span>	};
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// ./src/generated/withLoggingHook.test.tsx
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>React</span> <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>render</span>, <span style=color:#a6e22e>fireEvent</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;@testing-library/react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>withLoggingHook</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;./withLoggingHook&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>TestComponent</span> <span style=color:#f92672>=</span> () <span style=color:#f92672>=&gt;</span> &lt;<span style=color:#f92672>div</span>&gt;<span style=color:#a6e22e>Test</span> <span style=color:#a6e22e>Component</span>&lt;/<span style=color:#f92672>div</span>&gt;;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>describe</span>(<span style=color:#e6db74>&#39;withLoggingHook&#39;</span>, () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>beforeEach</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>jest</span>.<span style=color:#a6e22e>spyOn</span>(<span style=color:#a6e22e>console</span>, <span style=color:#e6db74>&#39;log&#39;</span>);
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>afterEach</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>jest</span>.<span style=color:#a6e22e>resetAllMocks</span>();
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>it</span>(<span style=color:#e6db74>&#39;should log when mounted and unmounted&#39;</span>, () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>ComponentWithLogging</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>withLoggingHook</span>(<span style=color:#a6e22e>TestComponent</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>rerender</span>, <span style=color:#a6e22e>unmount</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>render</span>(&lt;<span style=color:#f92672>ComponentWithLogging</span> /&gt;);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>expect</span>(<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>).<span style=color:#a6e22e>toHaveBeenCalledWith</span>(<span style=color:#e6db74>&#39;Mounted TestComponent&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>rerender</span>(&lt;<span style=color:#f92672>div</span>&gt;&lt;/<span style=color:#f92672>div</span>&gt;);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>expect</span>(<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>).<span style=color:#a6e22e>not</span>.<span style=color:#a6e22e>toHaveBeenLastCalledWith</span>(<span style=color:#e6db74>&#39;Unmounted TestComponent&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>unmount</span>();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>expect</span>(<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>).<span style=color:#a6e22e>toHaveBeenLastCalledWith</span>(<span style=color:#e6db74>&#39;Unmounted TestComponent&#39;</span>);
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>});
</span></span></code></pre></div><h4 id=mistakes>Mistakes:<a hidden class=anchor aria-hidden=true href=#mistakes>#</a></h4><ul><li>It used a constructor, instead of the componentDidMount method.</li><li>added a fireEvent import, but didn&rsquo;t use it later.</li></ul><h4 id=retry>Retry:<a hidden class=anchor aria-hidden=true href=#retry>#</a></h4><p>I retried a couple more times. In the next attempt, it jumbled the output and got things completely wrong.</p><p>On attempt 3, it did pretty well though
<em>CODE OUTPUT (Attempt 3):</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// A react HOC, that wraps a given component and logs the component name
</span></span></span><span style=display:flex><span><span style=color:#75715e>// on mount and unmount.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>React</span> <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>useEffect</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>ComponentType</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>withLoggingHook</span>&lt;<span style=color:#f92672>TProps</span>&gt;(
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>WrappedComponent</span>: <span style=color:#66d9ef>ComponentType</span>&lt;<span style=color:#f92672>TProps</span>&gt;,
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>componentName</span>: <span style=color:#66d9ef>string</span>,
</span></span><span style=display:flex><span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>WithLogging</span> <span style=color:#f92672>=</span> (<span style=color:#a6e22e>props</span>: <span style=color:#66d9ef>TProps</span>) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>useEffect</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>componentName</span><span style=color:#e6db74>}</span><span style=color:#e6db74> is mounted`</span>);
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>return</span> () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>`</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>componentName</span><span style=color:#e6db74>}</span><span style=color:#e6db74> is unmounted`</span>);
</span></span><span style=display:flex><span>			};
</span></span><span style=display:flex><span>		}, []);
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> &lt;<span style=color:#f92672>WrappedComponent</span> {<span style=color:#a6e22e>...props</span>} /&gt;;
</span></span><span style=display:flex><span>	};
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>WithLogging</span>.<span style=color:#a6e22e>displayName</span> <span style=color:#f92672>=</span> <span style=color:#e6db74>`WithLogging(</span><span style=color:#e6db74>${</span><span style=color:#a6e22e>getDisplayName</span>(<span style=color:#a6e22e>WrappedComponent</span>)<span style=color:#e6db74>}</span><span style=color:#e6db74>)`</span>;
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>WithLogging</span>;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>function</span> <span style=color:#a6e22e>getDisplayName</span>&lt;<span style=color:#f92672>TProps</span>&gt;(<span style=color:#a6e22e>WrappedComponent</span>: <span style=color:#66d9ef>ComponentType</span>&lt;<span style=color:#f92672>TProps</span>&gt;) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>WrappedComponent</span>.<span style=color:#a6e22e>displayName</span> <span style=color:#f92672>||</span> <span style=color:#a6e22e>WrappedComponent</span>.<span style=color:#a6e22e>name</span> <span style=color:#f92672>||</span> <span style=color:#e6db74>&#39;Component&#39;</span>;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>I also noticed, that asking it specifically what I want, returned the right output in the first attempt</p><p><em>CODE OUTPUT (Attempt 4): asking explicitly what I want</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// A react HOC, that wraps a given component and logs the component name
</span></span></span><span style=display:flex><span><span style=color:#75715e>// on mount and unmount. Use effects, instead of componentDidMount / unmount
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>React</span> <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>function</span> <span style=color:#a6e22e>withLoggingHook</span>&lt;<span style=color:#f92672>P</span> <span style=color:#a6e22e>extends</span> <span style=color:#a6e22e>object</span>&gt;(
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>Component</span>: <span style=color:#66d9ef>React.ComponentType</span>&lt;<span style=color:#f92672>P</span>&gt;,
</span></span><span style=display:flex><span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> (<span style=color:#a6e22e>props</span>: <span style=color:#66d9ef>P</span>)<span style=color:#f92672>:</span> <span style=color:#a6e22e>JSX</span>.<span style=color:#a6e22e>Element</span> <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>&#39;[withLoggingHook]&#39;</span>, <span style=color:#a6e22e>Component</span>.<span style=color:#a6e22e>name</span>, <span style=color:#e6db74>&#39;mounted&#39;</span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>React</span>.<span style=color:#a6e22e>useEffect</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>return</span> () <span style=color:#f92672>=&gt;</span>
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>(<span style=color:#e6db74>&#39;[withLoggingHook]&#39;</span>, <span style=color:#a6e22e>Component</span>.<span style=color:#a6e22e>name</span>, <span style=color:#e6db74>&#39;un-mounted&#39;</span>);
</span></span><span style=display:flex><span>		}, []);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>return</span> &lt;<span style=color:#f92672>Component</span> {<span style=color:#a6e22e>...props</span>} /&gt;;
</span></span><span style=display:flex><span>	};
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=verdict>Verdict:<a hidden class=anchor aria-hidden=true href=#verdict>#</a></h3><ul><li><p>It&rsquo;s able to write basic hooks, but makes some errors with HOCs</p></li><li><p>Sometimes, it automatically wrote the test case for the code it generated, which was a pleasant surprise!</p></li><li><p>Retrying a few times leads to automatically producing a better answer.</p></li><li><p>Component.name, is not defined, so that seems like a bug.</p></li></ul><h4 id=potential-future-experiments>Potential future experiments<a hidden class=anchor aria-hidden=true href=#potential-future-experiments>#</a></h4><ul><li><p>Experimenting with <a href=https://platform.openai.com/docs/api-reference/chat/create#chat-create-n>n, top_p ,and top_k parameters</a>, and enabling the LLM to try multiple completions</p></li><li><p>Using a &lsquo;Verifier&rsquo; agent, that checks the completion for compilation and validity and retries</p></li><li><p>Giving it concrete examples, that it can follow.</p></li></ul><h2 id=test-2-write-a-test-plan-and-implement-the-test-cases>Test 2: Write a test plan, and implement the test cases<a hidden class=anchor aria-hidden=true href=#test-2-write-a-test-plan-and-implement-the-test-cases>#</a></h2><p>Next, let&rsquo;s test its test case writing abilities</p><h3 id=test-21---writing-tests-for-withlogginhook>Test 2.1 - Writing tests for withLogginHook<a hidden class=anchor aria-hidden=true href=#test-21---writing-tests-for-withlogginhook>#</a></h3><p>In the previous test, it wrote a withLoggingHook, let&rsquo;s use that as its context</p><p>As mentioned before, I used the vscode plugin I created previously, to give it context of the withLoggingHooks code</p><p><em>PROMPT:</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// [FILE-NAME] ./src/generated/withLoggingHook.tsx
</span></span></span><span style=display:flex><span><span style=color:#75715e>// A react HOC, that wraps a given component and logs the component name
</span></span></span><span style=display:flex><span><span style=color:#75715e>// on mount and unmount. Use effects, instead of componentDidMount / unmount
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>... <span style=color:#a6e22e>code</span> <span style=color:#66d9ef>from</span> <span style=color:#a6e22e>withLogginHook</span>&lt;<span style=color:#f92672>P</span>&gt; ...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// [FILE-NAME] generatedApps/DoctorFinderApp/doctor-finder-app/src/generated/withLoggingHook.spec.tsx
</span></span></span><span style=display:flex><span><span style=color:#75715e>// Test cases for withLoggingHook
</span></span></span></code></pre></div><p><em>CODE OUTPUT:</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>render</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;@testing-library/react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> { <span style=color:#a6e22e>withLoggingHook</span> } <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;./withLoggingHook&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>describe</span>(<span style=color:#e6db74>&#39;withLoggingHook&#39;</span>, () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>beforeEach</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#75715e>// Mock console.log to prevent unwanted output during testing
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		<span style=color:#a6e22e>jest</span>.<span style=color:#a6e22e>spyOn</span>(<span style=color:#a6e22e>console</span>, <span style=color:#e6db74>&#39;log&#39;</span>).<span style=color:#a6e22e>mockImplementation</span>(() <span style=color:#f92672>=&gt;</span> {});
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>afterEach</span>(() <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#75715e>// Restore original console.log after each test
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>		(<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span> <span style=color:#66d9ef>as</span> <span style=color:#a6e22e>jest</span>.<span style=color:#a6e22e>Mock</span>).<span style=color:#a6e22e>mockRestore</span>();
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>it</span>(<span style=color:#e6db74>&#39;should log the component name on mount&#39;</span>, () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>TestComponent</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>withLoggingHook</span>(()<span style=color:#f92672>:</span> <span style=color:#66d9ef>null</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>null</span>);
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>render</span>(&lt;<span style=color:#f92672>TestComponent</span> /&gt;);
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>expect</span>(<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>).<span style=color:#a6e22e>toHaveBeenCalledWith</span>(
</span></span><span style=display:flex><span>			<span style=color:#e6db74>&#39;[withLoggingHook]&#39;</span>,
</span></span><span style=display:flex><span>			<span style=color:#e6db74>&#39;TestComponent&#39;</span>,
</span></span><span style=display:flex><span>			<span style=color:#e6db74>&#39;mounted&#39;</span>,
</span></span><span style=display:flex><span>		);
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>it</span>(<span style=color:#e6db74>&#39;should log the component name on unmount&#39;</span>, () <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>TestComponent</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>withLoggingHook</span>(()<span style=color:#f92672>:</span> <span style=color:#66d9ef>null</span> <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>null</span>);
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> { <span style=color:#a6e22e>unmount</span> } <span style=color:#f92672>=</span> <span style=color:#a6e22e>render</span>(&lt;<span style=color:#f92672>TestComponent</span> /&gt;);
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>unmount</span>();
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>expect</span>(<span style=color:#a6e22e>console</span>.<span style=color:#a6e22e>log</span>).<span style=color:#a6e22e>toHaveBeenCalledWith</span>(
</span></span><span style=display:flex><span>			<span style=color:#e6db74>&#39;[withLoggingHook]&#39;</span>,
</span></span><span style=display:flex><span>			<span style=color:#e6db74>&#39;TestComponent&#39;</span>,
</span></span><span style=display:flex><span>			<span style=color:#e6db74>&#39;un-mounted&#39;</span>,
</span></span><span style=display:flex><span>		);
</span></span><span style=display:flex><span>	});
</span></span><span style=display:flex><span>});
</span></span></code></pre></div><h3 id=mistakes-1>Mistakes:<a hidden class=anchor aria-hidden=true href=#mistakes-1>#</a></h3><ul><li><p>The test case in itself is fine, the issue with the withLoggingHook causes the tests to fail</p></li><li><p>However, I did realise that multiple attempts on the same test case did not always lead to promising results.</p></li></ul><h3 id=verdict-1>Verdict<a hidden class=anchor aria-hidden=true href=#verdict-1>#</a></h3><ul><li><p>For simple test cases, it works quite well, but just barely.</p></li><li><p>Anything more complex, it kinda makes too many bugs to be productively useful.</p></li></ul><h2 id=task-3-excel-upload-component>Task 3: Excel Upload Component<a hidden class=anchor aria-hidden=true href=#task-3-excel-upload-component>#</a></h2><p>Let&rsquo;s ask it to make a slightly more complicated component with some functionality</p><p><em>PROMPT</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#75715e>// A react component, that allows the user to upload excel files.
</span></span></span><span style=display:flex><span><span style=color:#75715e>// The excel file has one sheet, and that sheet has two columns
</span></span></span><span style=display:flex><span><span style=color:#75715e>// Columns headers are foo, and bar, and both should have number values.
</span></span></span><span style=display:flex><span><span style=color:#75715e>// When the file is uploaded, it is parsed using XLSX.
</span></span></span><span style=display:flex><span><span style=color:#75715e>// The final parsed values of foo and bar, are given back as an array to the callback.
</span></span></span></code></pre></div><p><em>CODE OUTPUT:</em></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-typescript data-lang=typescript><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>React</span> <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;react&#39;</span>;
</span></span><span style=display:flex><span><span style=color:#66d9ef>import</span> <span style=color:#a6e22e>XLSX</span> <span style=color:#66d9ef>from</span> <span style=color:#e6db74>&#39;xlsx&#39;</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>interface</span> <span style=color:#a6e22e>ExcelUploadComponentProps</span> {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>onFileLoad</span><span style=color:#f92672>:</span> (<span style=color:#a6e22e>data</span><span style=color:#f92672>:</span> { <span style=color:#a6e22e>foo</span>: <span style=color:#66d9ef>number</span>[]; <span style=color:#a6e22e>bar</span>: <span style=color:#66d9ef>number</span>[] }) <span style=color:#f92672>=&gt;</span> <span style=color:#66d9ef>void</span>;
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>const</span> <span style=color:#a6e22e>ExcelUploadComponent</span>: <span style=color:#66d9ef>React.FC</span>&lt;<span style=color:#f92672>ExcelUploadComponentProps</span>&gt; <span style=color:#f92672>=</span> ({
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>onFileLoad</span>,
</span></span><span style=display:flex><span>}<span style=color:#f92672>:</span> <span style=color:#a6e22e>ExcelUploadComponentProps</span>) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>handleChange</span> <span style=color:#f92672>=</span> (<span style=color:#a6e22e>e</span>: <span style=color:#66d9ef>React.ChangeEvent</span>&lt;<span style=color:#f92672>HTMLInputElement</span>&gt;) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>e</span>.<span style=color:#a6e22e>target</span>.<span style=color:#a6e22e>files</span> <span style=color:#f92672>||</span> <span style=color:#a6e22e>e</span>.<span style=color:#a6e22e>target</span>.<span style=color:#a6e22e>files</span>.<span style=color:#a6e22e>length</span> <span style=color:#f92672>===</span> <span style=color:#ae81ff>0</span>) <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>file</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>e</span>.<span style=color:#a6e22e>target</span>.<span style=color:#a6e22e>files</span>[<span style=color:#ae81ff>0</span>];
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>reader</span> <span style=color:#f92672>=</span> <span style=color:#66d9ef>new</span> <span style=color:#a6e22e>FileReader</span>();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>reader</span>.<span style=color:#a6e22e>onload</span> <span style=color:#f92672>=</span> (<span style=color:#a6e22e>event</span>: <span style=color:#66d9ef>ProgressEvent</span>&lt;<span style=color:#f92672>FileReader</span>&gt;) <span style=color:#f92672>=&gt;</span> {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>event</span>.<span style=color:#a6e22e>target</span> <span style=color:#f92672>||</span> <span style=color:#f92672>!</span><span style=color:#a6e22e>event</span>.<span style=color:#a6e22e>target</span>.<span style=color:#a6e22e>result</span>) <span style=color:#66d9ef>return</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>data</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>event</span>.<span style=color:#a6e22e>target</span>.<span style=color:#a6e22e>result</span> <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>string</span>;
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>workbook</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>XLSX</span>.<span style=color:#a6e22e>read</span>(<span style=color:#a6e22e>data</span>, { <span style=color:#66d9ef>type</span><span style=color:#f92672>:</span> <span style=color:#e6db74>&#39;binary&#39;</span> });
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>sheetName</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>workbook</span>.<span style=color:#a6e22e>SheetNames</span>[<span style=color:#ae81ff>0</span>];
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>worksheet</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>workbook</span>.<span style=color:#a6e22e>Sheets</span>[<span style=color:#a6e22e>sheetName</span>];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			<span style=color:#75715e>// Convert the excel data to JSON
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>jsonData</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>XLSX</span>.<span style=color:#a6e22e>utils</span>.<span style=color:#a6e22e>sheet_to_json</span>(<span style=color:#a6e22e>worksheet</span>, { <span style=color:#a6e22e>header</span>: <span style=color:#66d9ef>1</span> });
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			<span style=color:#75715e>// Extract the foo and bar values from the JSON data
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>fooValues</span> <span style=color:#f92672>=</span> [];
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>barValues</span> <span style=color:#f92672>=</span> [];
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>let</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> <span style=color:#f92672>&lt;</span> <span style=color:#a6e22e>jsonData</span>.<span style=color:#a6e22e>length</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span>) {
</span></span><span style=display:flex><span>				<span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span><span style=color:#a6e22e>jsonData</span>[<span style=color:#a6e22e>i</span>]) <span style=color:#66d9ef>continue</span>;
</span></span><span style=display:flex><span>				<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>row</span> <span style=color:#f92672>=</span> <span style=color:#a6e22e>jsonData</span>[<span style=color:#a6e22e>i</span>];
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>fooValues</span>.<span style=color:#a6e22e>push</span>(<span style=color:#a6e22e>row</span>[<span style=color:#ae81ff>0</span>]);
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>barValues</span>.<span style=color:#a6e22e>push</span>(<span style=color:#a6e22e>row</span>[<span style=color:#ae81ff>1</span>]);
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>onFileLoad</span>({ <span style=color:#a6e22e>foo</span>: <span style=color:#66d9ef>fooValues</span>, <span style=color:#a6e22e>bar</span>: <span style=color:#66d9ef>barValues</span> });
</span></span><span style=display:flex><span>		};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>reader</span>.<span style=color:#a6e22e>readAsBinaryString</span>(<span style=color:#a6e22e>file</span>);
</span></span><span style=display:flex><span>	};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> &lt;<span style=color:#f92672>input</span> <span style=color:#a6e22e>type</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;file&#34;</span> <span style=color:#a6e22e>accept</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;.xls,.xlsx,.csv&#34;</span> <span style=color:#a6e22e>onChange</span><span style=color:#f92672>=</span>{<span style=color:#a6e22e>handleChange</span>} /&gt;;
</span></span><span style=display:flex><span>};
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>export</span> <span style=color:#66d9ef>default</span> <span style=color:#a6e22e>ExcelUploadComponent</span>;
</span></span></code></pre></div><h3 id=verdict-2>Verdict<a hidden class=anchor aria-hidden=true href=#verdict-2>#</a></h3><ul><li><p>Now that was <em>pretty</em> good. I have no idea how to use the xlsx utility, and have not looked at its documentation</p></li><li><p>Even though, I had to correct some typos and some other minor edits - this gave me a component that does exactly what I needed.</p></li><li><p>I&rsquo;d say this save me atleast 10-15 minutes of time googling for the api documentation and fumbling till I got it right.</p></li></ul><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>All in all, I think having locally running LLMs that can help us with code is possibly <em>very</em> near</p><p>These current models, while don&rsquo;t really get things correct always, <em>do</em> provide a pretty handy tool and in situations where new territory / new apps are being made, I think they can make significant progress.</p><p>Something to note, is that once I provide more longer contexts, the model seems to make a lot more errors. This is potentially only model specific, so future experimentation is needed here.</p><h1 id=whats-next>What&rsquo;s next<a hidden class=anchor aria-hidden=true href=#whats-next>#</a></h1><p>There were quite a few things I didn&rsquo;t explore here. I will cover those in future posts.</p><ul><li><p>Here&rsquo;s a list of a few things I&rsquo;m going to experiment next</p></li><li><p>Providing more examples of <em>good</em> code, instead of trying to explicitly mention every detail we want</p></li><li><p>Comparing other models on similar exercises. Possibly making a benchmark test suite to compare them against.</p></li><li><p>Trying multi-agent setups. I having another LLM that can correct the first ones mistakes, or enter into a dialogue where two minds reach a better outcome is totally possible.</p></li><li><p>A hint on this, is that once it gets something wrong, and I add the mistake to the prompt - the next iteration of the output is usually much better.</p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li></ul></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kshitij-banerjee.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a>
<script src=https://giscus.app/client.js data-repo=Kshitij-Banerjee/Kshitij-Banerjee.github.io data-repo-id=R_kgDOI1NsBg data-category=General data-category-id=DIC_kwDOI1NsBs4Cdf1h data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://Kshitij-Banerjee.github.io/>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>