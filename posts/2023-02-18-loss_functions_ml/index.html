<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script type=text/javascript src=https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js></script><meta name=robots content="index, follow"><title>Loss Functions In ML | Kshitij's Notes</title><meta name=keywords content="ML,AI,Andrew,Ng,Stanford"><meta name=description content="Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
Why do these functions use negative log of the loss?
Plot: As probabilities only lie bettween [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.
But since we do LogLoss = - ( y * Log(p(y)) )"><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/posts/2023-02-18-loss_functions_ml/><link crossorigin=anonymous href=/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Loss Functions In ML"><meta property="og:description" content="Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
Why do these functions use negative log of the loss?
Plot: As probabilities only lie bettween [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.
But since we do LogLoss = - ( y * Log(p(y)) )"><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/posts/2023-02-18-loss_functions_ml/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-17T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Loss Functions In ML"><meta name=twitter:description content="Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
Why do these functions use negative log of the loss?
Plot: As probabilities only lie bettween [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more.
But since we do LogLoss = - ( y * Log(p(y)) )"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Loss Functions In ML","item":"https://Kshitij-Banerjee.github.io/posts/2023-02-18-loss_functions_ml/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Loss Functions In ML","name":"Loss Functions In ML","description":"Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)\nWhy do these functions use negative log of the loss?\nPlot: As probabilities only lie bettween [0-1], the plot is only relevant between X from 0-1\nThis means, that it penalises a low probability of success exponentially more.\nBut since we do LogLoss = - ( y * Log(p(y)) )","keywords":["ML","AI","Andrew","Ng","Stanford"],"articleBody":"Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)\nWhy do these functions use negative log of the loss?\nPlot: As probabilities only lie bettween [0-1], the plot is only relevant between X from 0-1\nThis means, that it penalises a low probability of success exponentially more.\nBut since we do LogLoss = - ( y * Log(p(y)) )\nIf the true label is 0, the effect of the log is ignored.\nOnly true labels contribute to the overall loss, and if for the true labels the P(y) value is low, then the loss magnitude is highly penalised\nBinary Cross Entropy\nMath: + This is just expanded math for using P(0) = { 1 - P(1) }, and becomes same as log loss Used for:\nWhen the output class is one of two values (binary) in nature. Categorical cross entropy\nMath:\nid:: 44f0007d-05af-4f07-b88e-4c39eaaf40c6 $ J(W)=‚àí ‚àë y_{i} * log( P(y_{i}) ) $ Used for:\nIf your¬†ùëå vector values are one-hot encoded, use categorical_crossentropy.\nExamples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1] Sparse Categorical Cross Entropy\nMath (same as Categorical Cross Entropy)\n$ J(W)=‚àí ‚àë y_{i} * log( P(y_{i}) ) $ Used for: Integer classes as output\nIntuitively, the sparse categorical just takes the index of the true-value to calculate the loss\nSo when model output is for example¬†[0.1, 0.3, 0.7]¬†and ground truth is¬†3¬†(if indexed from 1) then loss compute only logarithm of¬†0.7. This doesn‚Äôt change the final value, because in the regular version of categorical crossentropy other values are immediately multiplied by zero (because of one-hot encoding characteristic). Thanks to that it computes logarithm once per instance and omits the summation which leads to better performance. The formula might look like this: ","wordCount":"310","inLanguage":"en","datePublished":"2023-02-17T00:00:00Z","dateModified":"2023-02-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/posts/2023-02-18-loss_functions_ml/"},"publisher":{"@type":"Organization","name":"Kshitij's Notes","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="Kshitij's Notes (Alt + H)">Kshitij's Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Loss Functions In ML</h1><div class=post-meta><span title='2023-02-17 00:00:00 +0000 UTC'>February 17, 2023</span></div></header><div class=post-content><p>Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)</p><p>Why do these functions use negative log of the loss?</p><p>Plot: <img loading=lazy src=/image_1676730500910_0.png alt=image.png></p><ul><li><p>As probabilities only lie bettween [0-1], the plot is only relevant between X from 0-1</p></li><li><p>This means, that it penalises a low probability of success exponentially more.</p></li><li><p>But since we do LogLoss = - ( y * Log(p(y)) )</p><ul><li><p>If the true label is 0, the effect of the log is ignored.</p></li><li><p>Only true labels contribute to the overall loss, and if for the true labels the P(y) value is low, then the loss magnitude is highly penalised</p></li></ul></li></ul><p><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy>Binary Cross Entropy</a></p><ul><li><strong>Math:</strong></li></ul><p><img loading=lazy src=/image_1676732588575_0.png alt=image.png></p><pre><code>+ This is just expanded math for using P(0) = {  1 - P(1) }, and becomes same as log loss
</code></pre><ul><li><p><strong>Used for:</strong></p><ul><li>When the output class is one of two values (binary) in nature.</li></ul></li></ul><p><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy>Categorical cross entropy</a></p><ul><li><p>Math:</p><ul><li>id:: 44f0007d-05af-4f07-b88e-4c39eaaf40c6
$ J(W)=‚àí ‚àë y_{i} * log( P(y_{i}) ) $</li></ul></li><li><p>Used for:</p><ul><li><p>If your¬†ùëå vector values are one-hot encoded, use categorical_crossentropy.</p><ul><li>Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]</li></ul></li></ul></li></ul><p><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy>Sparse Categorical Cross Entropy</a></p><ul><li><p>Math (same as Categorical Cross Entropy)</p><ul><li>$ J(W)=‚àí ‚àë y_{i} * log( P(y_{i}) ) $</li></ul></li><li><p>Used for: Integer classes as output</p></li><li><p>Intuitively, the sparse categorical just takes the index of the true-value to calculate the loss</p><ul><li>So when model output is for example¬†<code>[0.1, 0.3, 0.7]</code>¬†and ground truth is¬†<code>3</code>¬†(if indexed from 1) then loss compute only logarithm of¬†<code>0.7</code>. This doesn&rsquo;t change the final value, because in the regular version of categorical crossentropy other values are immediately multiplied by zero (because of one-hot encoding characteristic). Thanks to that it computes logarithm once per instance and omits the summation which leads to better performance. The formula might look like this:</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/ml/>ML</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/andrew/>Andrew</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ng/>Ng</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/stanford/>Stanford</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://Kshitij-Banerjee.github.io>Kshitij's Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>