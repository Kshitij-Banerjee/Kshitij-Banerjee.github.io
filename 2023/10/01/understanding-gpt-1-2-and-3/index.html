<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding GPT 1, 2 and 3 | KiloBytes by KB</title>
<meta name=keywords content="ML,machine-learning,AI,Transformers,GPT"><meta name=description content="Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered transformers via the original paper &ldquo;Attention is all you need&rdquo; that brought the innovation that made all this progress possible.
This post will focus on GPT-3 and its predecessors GPT-1 and 2."><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-KFSE3K3EMC"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KFSE3K3EMC",{anonymize_ip:!1})}</script><meta property="og:title" content="Understanding GPT 1, 2 and 3"><meta property="og:description" content="Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered transformers via the original paper &ldquo;Attention is all you need&rdquo; that brought the innovation that made all this progress possible.
This post will focus on GPT-3 and its predecessors GPT-1 and 2."><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/"><meta property="og:image" content="https://Kshitij-Banerjee.github.io/GPT-3_banner.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2023-10-01T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Kshitij-Banerjee.github.io/GPT-3_banner.png"><meta name=twitter:title content="Understanding GPT 1, 2 and 3"><meta name=twitter:description content="Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered transformers via the original paper &ldquo;Attention is all you need&rdquo; that brought the innovation that made all this progress possible.
This post will focus on GPT-3 and its predecessors GPT-1 and 2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Understanding GPT 1, 2 and 3","item":"https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding GPT 1, 2 and 3","name":"Understanding GPT 1, 2 and 3","description":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered transformers via the original paper \u0026ldquo;Attention is all you need\u0026rdquo; that brought the innovation that made all this progress possible.\nThis post will focus on GPT-3 and its predecessors GPT-1 and 2.","keywords":["ML","machine-learning","AI","Transformers","GPT"],"articleBody":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered transformers via the original paper “Attention is all you need” that brought the innovation that made all this progress possible.\nThis post will focus on GPT-3 and its predecessors GPT-1 and 2. The progression from GPT 1,2 and finally to 3, explains how the authors found a way to generalise the transformer architecture to task-agnostic workloads, and what led to the discovery of the GPT-3 175B parameter model.\nI intend for this to be summary of the original papers, and do refer to the detailed results sections in the paper itself\nPapers previously covered Previously Covered Transformers, following Vaswani et al. 2017 Google Attention is all you need Papers covered in this post GPT-1, following Radford et al. 2018 Improving Language Understanding by Generative Pre-Training GPT-2, following Radford et al. 2018 Language Models are Unsupervised Multitask Learners GPT-3: Few shot learners, 2020 OpenAI Language Models are Few-Shot Learners GPT-1 \u0026 2 While transformers found an effective way to utilise raw textual content to solve tasks like machine translation, there was still no consensus on the most effective way to transfer these learned representations to any other target task.\nExisting techniques involved a combination of task-specific model architectures, or using intricate learning schemes.\nGPT-1 paper, explores a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and a supervised fine-training.\nThe primary goal, is to find a universal representation that transfers with little adaptation to a wide range of tasks.\nPre-training The pre-training phase is quite similar to what we previously covered in the transformers paper\nThe difference, being that the authors use a decoder only architecture (as there is no translation involved here, only predicting the next token)\nFine Tuning This is also quite similar to the previous stage, with some key differences.\nThe core motivation is, that instead of predicting the next token, the model is supposed to predict the output Y.\nSo, a pre-labeled dataset C is chosen, where given the input in the form of tokens X_1 … X_n, the model is evaluated to predict y\nTo do this, an additional linear output layer with parameters W_y is used to predict the final y\nAuxilary objectives Additionally, they add an auxiliary objective (language modelling), to the mix, as this helps with generalisation.\nTo do this, they combine the Loss functions of the pre-training with the auxiliary task\nTask Specific Input Transformations As described from the paper, with some modifications to the input they are able to use the same model for different tasks.\nwe convert structured inputs into an ordered sequence that our pre-trained model can process.\nTo explain with an example, consider how they change the inputs for sentence similarity tasks\nFor similarity tasks, there is no inherent ordering of the two sentences being compared. To reﬂect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations hm l which are added element-wise before being fed into the linear output layer\nGPT-2 Primarily it is a larger model (1.5B) with a few additional modificaitons\nLayer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the ﬁnal selfattention block. A modiﬁed initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers.\nThe vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used\nGPT-3 - Language Models are Few-Shot Learners The motivation A major limitation to this approach is that while the architecture (GPT-1/2) is task-agnostic, there is still a need for task-speciﬁc datasets and task-speciﬁc ﬁne-tuning: to achieve strong performance on a desired task typically requires ﬁne-tuning on a dataset of thousands to hundreds of thousands of examples speciﬁc to that task. Removing this limitation would be desirable, for several reasons\nHumans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often enough to produce satisfactory results.\nCan machines learn like humans do ? With few-shots ? What if we give few examples to the model, to come with the answer like humans learn. Will that work?\n“In-context learning”, uses the text input of a pretrained language model as a form of task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.\nWhile initially, the results were not at par with the fine-tuning approach. The authors believed there is hope, as they see a linear trend of improvement with increased model sizes. The authors hence hope that increased model size, would also help with the “in-context” learning capabilities, to bring them at par.\nSince in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale\nTo test this, the authors built a 175B parameters model, and measured its in-context learning abilities.\nThey test in the following conditions\n(a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will ﬁt into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditionalﬁne-tuning setting, but we leave this to future work\nModel architecture We use the same model and architecture as GPT-2 [ RWC+19 ], including the modiﬁed initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ].\nResults The authors then show a variety of results on various tasks like news article generation, story closure, Translation, Common sense reasoning, Reading comprehension. The results are fascinating, and are best read from the paper directly than me repeating them here.\nConclusion The success of ChatGPT is something we see now, but clearly it was years in the making and required ruthless process of discovery and experimentaion.\nMy personal takeaways are\nThe authors formulated a varied set of “tests” along with a large pool of training data. They evaluated the output across translation, comprehension, question answering etc.\nThe authors followed a clear hypothesis, tests, result method - and for the most part tested a limited set of parameters in each test. In the GPT-3 example, they are majorly testing size of the model compared with performance.\nThe ability to question ones own achievements. Even though they achieved great results from their GPT 1 and 2 models, they found issues in the fine-tuning approach and were able to pivot.\nI’m next interested in, models that take visual inputs, and how the world is reaching GPT-4V (visual) multi-modal techniques.\n","wordCount":"1282","inLanguage":"en","image":"https://Kshitij-Banerjee.github.io/GPT-3_banner.png","datePublished":"2023-10-01T00:00:00Z","dateModified":"2023-10-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2023/10/01/understanding-gpt-1-2-and-3/"},"publisher":{"@type":"Organization","name":"KiloBytes by KB","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io/ accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Understanding GPT 1, 2 and 3</h1><div class=post-meta><span title='2023-10-01 00:00:00 +0000 UTC'>October 1, 2023</span></div></header><figure class=entry-cover><img loading=eager src=https://Kshitij-Banerjee.github.io/GPT-3_banner.png alt></figure><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>The goal of this series of posts, is to form <em>foundational knowledge</em> that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.</p><p>In my previous <a href=https://kshitij-banerjee.github.io/2023/07/07/understanding-gpt-transformers/>post</a>, I covered transformers via the original paper &ldquo;Attention is all you need&rdquo; that brought the innovation that made all this progress possible.</p><p>This post will focus on GPT-3 and its predecessors GPT-1 and 2. The progression from GPT 1,2 and finally to 3, explains how the authors found a way to generalise the transformer architecture to task-agnostic workloads, and what led to the discovery of the GPT-3 175B parameter model.</p><p>I intend for this to be summary of the original papers, and do refer to the detailed results sections in the paper itself</p><h1 id=papers-previously-covered>Papers previously covered<a hidden class=anchor aria-hidden=true href=#papers-previously-covered>#</a></h1><ol><li><strong><a href=https://kshitij-banerjee.github.io/2023/07/07/understanding-gpt-transformers/>Previously Covered</a></strong> Transformers, following Vaswani et al. 2017 Google  <a href=https://arxiv.org/pdf/1706.03762.pdf>Attention is all you need</a></li></ol><h1 id=papers-covered-in-this-post>Papers covered in this post<a hidden class=anchor aria-hidden=true href=#papers-covered-in-this-post>#</a></h1><ol><li><p>GPT-1, following Radford et al. 2018 <a href=https://www.mikecaptain.com/resources/pdf/GPT-1.pdf>Improving Language Understanding by Generative Pre-Training</a></p></li><li><p>GPT-2, following Radford et al. 2018 <a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>Language Models are Unsupervised Multitask Learners</a></p></li><li><p>GPT-3: Few shot learners, 2020 OpenAI <a href=https://arxiv.org/pdf/2005.14165.pdf>Language Models are Few-Shot Learners</a></p></li></ol><h1 id=gpt-1--2>GPT-1 & 2<a hidden class=anchor aria-hidden=true href=#gpt-1--2>#</a></h1><p>While transformers found an effective way to utilise raw textual content to solve tasks like machine translation, there was still no consensus on the most effective way to transfer these learned representations to any other target task.</p><p>Existing techniques involved a combination of task-specific model architectures, or using intricate learning schemes.</p><p>GPT-1 paper, explores a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and a supervised fine-training.</p><p>The primary goal, is to find a universal representation that transfers with little adaptation to a wide range of tasks.</p><h2 id=pre-training>Pre-training<a hidden class=anchor aria-hidden=true href=#pre-training>#</a></h2><p>The pre-training phase is quite similar to what we <a href=https://kshitij-banerjee.github.io/2023/07/07/understanding-gpt-transformers/>previously covered</a> in the transformers paper</p><p>The difference, being that the authors use a decoder only architecture (as there is no translation involved here, only predicting the next token)</p><p><img loading=lazy src=/image_1696151607714_0.png alt=image.png></p><h2 id=fine-tuning>Fine Tuning<a hidden class=anchor aria-hidden=true href=#fine-tuning>#</a></h2><p>This is also quite similar to the previous stage, with some key differences.</p><p>The core motivation is, that instead of predicting the next token, the model is supposed to predict the output Y.</p><p>So, a pre-labeled dataset C is chosen, where given the input in the form of tokens X_1 &mldr; X_n, the model is evaluated to predict y</p><p>To do this, an additional linear output layer with parameters W_y is used to predict the final y</p><h1 id=auxilary-objectives>Auxilary objectives<a hidden class=anchor aria-hidden=true href=#auxilary-objectives>#</a></h1><p>Additionally, they add an auxiliary objective (language modelling), to the mix, as this helps with generalisation.</p><p>To do this, they combine the Loss functions of the pre-training with the auxiliary task</p><p><img loading=lazy src=/image_1696151619955_0.png alt=image.png></p><h2 id=task-specific-input-transformations>Task Specific Input Transformations<a hidden class=anchor aria-hidden=true href=#task-specific-input-transformations>#</a></h2><p><img loading=lazy src=/image_1696151628505_0.png alt=image.png></p><p>As described from the paper, with some modifications to the input they are able to use the same model for different tasks.</p><blockquote><p>we convert structured inputs into an ordered sequence that our pre-trained model can process.<br>To explain with an example, consider how they change the inputs for sentence similarity tasks</p></blockquote><blockquote><p>For similarity tasks, there is no inherent ordering of the two sentences being compared. To reﬂect this, we modify the input sequence to contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations hm l which are added element-wise before being fed into the linear output layer</p></blockquote><h2 id=gpt-2>GPT-2<a hidden class=anchor aria-hidden=true href=#gpt-2>#</a></h2><p>Primarily it is a larger model (1.5B) with a few additional modificaitons</p><blockquote><p>Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the ﬁnal selfattention block. A modiﬁed initialization which accounts for the accumulation on the residual path with model depth is used. We scale the weights of residual layers at initialization by a factor of 1/√N where N is the number of residual layers.<br>The vocabulary is expanded to 50,257. We also increase the context size from 512 to 1024 tokens and a larger batchsize of 512 is used</p></blockquote><h1 id=gpt-3---language-models-are-few-shot-learners>GPT-3 - Language Models are Few-Shot Learners<a hidden class=anchor aria-hidden=true href=#gpt-3---language-models-are-few-shot-learners>#</a></h1><h2 id=the-motivation>The motivation<a hidden class=anchor aria-hidden=true href=#the-motivation>#</a></h2><blockquote><p>A major limitation to this approach is that while the architecture (GPT-1/2) is task-agnostic, there is still a need for task-speciﬁc datasets and task-speciﬁc ﬁne-tuning: to achieve strong performance on a desired task typically requires ﬁne-tuning on a dataset of thousands to hundreds of thousands of examples speciﬁc to that task. Removing this limitation would be desirable, for several reasons<br>Humans do not require large supervised datasets to learn most language tasks – a brief directive in natural language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often enough to produce satisfactory results.</p></blockquote><h2 id=can-machines-learn-like-humans-do--with-few-shots->Can machines learn like humans do ? With few-shots ?<a hidden class=anchor aria-hidden=true href=#can-machines-learn-like-humans-do--with-few-shots->#</a></h2><p>What if we give few examples to the model, to come with the answer like humans learn. Will that work?</p><p>“In-context learning”, uses the text input of a pretrained language model as a form of task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.</p><p>While initially, the results were not at par with the fine-tuning approach. The authors believed there is hope, as they see a linear trend of improvement with increased model sizes. The authors hence hope that increased model size, would also help with the &ldquo;in-context&rdquo; learning capabilities, to bring them at par.</p><blockquote><p>Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale<br>To test this, the authors built a 175B parameters model, and measured its in-context learning abilities.</p></blockquote><p>They test in the following conditions</p><blockquote><p>(a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will ﬁt into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditionalﬁne-tuning setting, but we leave this to future work<br><img loading=lazy src=/image_1696151674104_0.png alt=image.png></p></blockquote><p><img loading=lazy src=/image_1696151689128_0.png alt=image.png></p><h2 id=model-architecture>Model architecture<a hidden class=anchor aria-hidden=true href=#model-architecture>#</a></h2><blockquote><p>We use the same model and architecture as GPT-2 [ RWC+19 ], including the modiﬁed initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [ CGRS19 ].<br><img loading=lazy src=/image_1696151701540_0.png alt=image.png></p></blockquote><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>The authors then show a variety of results on various tasks like news article generation, story closure, Translation, Common sense reasoning, Reading comprehension. The results are fascinating, and are best read from the paper directly than me repeating them here.</p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>The success of ChatGPT is something we see now, but clearly it was years in the making and required ruthless process of discovery and experimentaion.</p><p>My personal takeaways are</p><ol><li><p>The authors formulated a varied set of &ldquo;tests&rdquo; along with a large pool of training data. They evaluated the output across translation, comprehension, question answering etc.</p></li><li><p>The authors followed a clear hypothesis, tests, result method - and for the most part tested a limited set of parameters in each test. In the GPT-3 example, they are majorly testing size of the model compared with performance.</p></li><li><p>The ability to question ones own achievements. Even though they achieved great results from their GPT 1 and 2 models, they found issues in the fine-tuning approach and were able to pivot.</p></li></ol><p>I&rsquo;m next interested in, models that take visual inputs, and how the world is reaching GPT-4V (visual) multi-modal techniques.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/ml/>ML</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/transformers/>Transformers</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://Kshitij-Banerjee.github.io/>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>