<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script type=text/javascript src=https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js></script><meta name=robots content="index, follow"><title>Understanding GPT - Transformers | KiloBytes by KB</title>
<meta name=keywords content="ML,machine-learning,AI,Transformers"><meta name=description content="Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don&rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here."><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/><link crossorigin=anonymous href=/assets/css/stylesheet.4cb5d59298692facab5b270399a4560592b36cb58936d3c922114e90da87e031.css integrity="sha256-TLXVkphpL6yrWycDmaRWBZKzbLWJNtPJIhFOkNqH4DE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta name=author content="Kshitij Banerjee"><meta name=description content="Kshitij Banerjee's notes..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Kshitij Banerjee","url":"https://kshitij-banerjee.github.io/"}</script><meta property="og:title" content="Understanding GPT - Transformers"><meta property="og:description" content="Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don&rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here."><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/"><meta property="og:image" content="https://Kshitij-Banerjee.github.io/Transformers_banner_1689490231707_0.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-07T00:00:00+00:00"><meta property="article:modified_time" content="2023-07-07T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Kshitij-Banerjee.github.io/Transformers_banner_1689490231707_0.png"><meta name=twitter:title content="Understanding GPT - Transformers"><meta name=twitter:description content="Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.
In my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don&rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Understanding GPT - Transformers","item":"https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding GPT - Transformers","name":"Understanding GPT - Transformers","description":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don\u0026rsquo;t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here.","keywords":["ML","machine-learning","AI","Transformers"],"articleBody":"Introduction The goal of this series of posts, is to form foundational knowledge that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.\nIn my previous post, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don’t know about them, or would like a quick refresher - I recommend reading through the previous post before continuing here.\nThis post will focus on the “Attention is all you need” paper that introduced the ground-breaking transformer architecture to the world and has since started a cascading and exponential affect on the AI landscape.\nPapers to be covered in this series [THIS POST] Transformers, following Vaswani et al. 2017 Google Attention is all you need GPT-1, following Radford et al. 2018 Improving Language Understanding by Generative Pre-Training GPT-2, following Radford et al. 2018 Language Models are Unsupervised Multitask Learners BERT, following Devlin et.al. 2019 Google Pre-training of Deep Bidirectional Transformers for Language Understanding RoBERTa, following Liu et. al. A Robustly Optimized BERT Pretraining Approach GPT-3: Few shot learners, 2020 OpenAI Language Models are Few-Shot Learners PaLM: following Chowdhery et al. 2022 Scaling Language Modeling with Pathways Maybe: MACAW-LLM, following Lyu et al. 2023 MULTI-MODAL LANGUAGE MODELING Transformers Paper Transformers, following Vaswani et al. 2017 Google Attention is all you need The problem its solving This inherently sequential nature (of RNNs) precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples\nIntention In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.\nArchitecture Main Points Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\nIts helpful to see the additive attention that was described previously in the paper by Dzmitry Refer to the diagram above from the paper by Dzmitry . In it, attention is realised by creating a context vector C that is generated via an alignment model. The model has weights alpha[tx,ty] that act as weighted sum on the states h[j] of the encoded sentence. This helps provide an “attention” mechanism.\nI believe the authors are summarising this behaviour by explaining attention as a query + key-value pairs =\u003e output.\nThe query in this case, is the alpha vector that understands which parts of the X[t] to query. The values are the hidden-states h[j], and the keys are the time/positions that relate to that value h.\nIn affect, the attention mechanism is a way for the decoder network to query the positionally encoded hidden states, based on the current state s[t-1]\nLater in the paper, they also mention the following:- In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models\nBut note that the previous paper relies on an RNN to create the context of time, which the current papers want to get rid of. So how do we create the keys then?\nAnother helpful visualisation of attention, is found in the paper Massive Exploration of Neural Machine Translation Architectures\nScaled Dot-Product Attention The authors hint that they prefers the multiplicative attention mechanism, due to its computational effeciencies - even though historically the additive attention has proven to work better.\nAdditive vs Multiplicative Attention While the transformers paper doesn’t explain the difference between the additive and multiplicative versions. The referenced paper can be expanded to understand them.\nEquation 6 is the additive version, and 7 is the multiplicative version\nThe multiplicative attention is introduced here by Luong et al.\nThe authors hypothize that the multiplicative attention has underperformed as it moves the logits into extreme ends where the gradients are close to 0. So they choose to scale down the logits before passing them to the softmax.\nWe compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values\nMathematically This is similar to doing the weighted sum on the values, where the weights are a softmax outputs from aligning the queries and the keys via a dot-product Visually Multi-Head Attention Further, the authors propose to do multi-head attention. This is essentially a way to parallelise the attention process on multiple heads instead of a single head.\nSo instead of doing a single attention with d_model dimensions. They, parallely run N attention models with d_model/N dimensions each.\nThe reason for doing this?\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMathematically: Self Attention Self attention, is essentially where the attention is given to itself, rather than a separate encoder model.\nThey use this in the encoder. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nPositional Encoding Since the authors completely got rid of the recurrence, or convolutional parts in the network - they need to provide the model with the positional information to compensate for this missing and crucial context.\nTo that effect, they chose to create positional embeddings (with the same dim size as the text embeddings).\nBut, they chose to not make them learnable parameters - and that makes sense to me.\nThey create the positional embeddings with the following logic\nWe\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of\nP Epos.\nThe d2l.ai book has the best explanation to this that I could find.\nIf we plot different columns, we can see that one can easily be transformed into the other, via linear transformations.\nEven after this though, I don’t think I fully understand this part well. For now, I’ve marked this as a TODO, and will come back to it later.\nWhy Self-Attention The core of this goes back to the original intention described towards the beginning of the paper.\nAs a reminder\nThis inherently sequential nature (of RNNs) precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples\nThe authors detail this by comparing the complexity of different layers, and also how traversing the path for finding long-range dependencies is easy with attention, but relatively complex in other forms.\nBelow is the tabular version for comparison\nFew key points Comparison with convolutions\nIn convolutions, long range dependencies would require a stack of N/k convolutional layers. Traversing such a path, hence takes Log_k(n). They are generally more expensive then recurrent layers by a factor of k in terms of complexity Comparison with recurrence\nThe core win here, is that recurrent connections require n sequential operations, which becomes O(1) with self attention Attention is also more interpretable\nThe authors are able to build attention distributions on the model, to realise that the model is relatively easier to reason about the relationship between positions and tokens. Conclusion The paper packs a ton of things into it. Its brilliant, but also probably takes a few iterations to absorb all the content well.\nI intend to dive into the code that is available in tensor2tensor and update this post with more understanding and learnings from the code.\nIn the next post, I intend to cover GPT-1 and 2 and work our way towards the GPT-3 and other state-of-the-art model architectures and additions.\n","wordCount":"1370","inLanguage":"en","image":"https://Kshitij-Banerjee.github.io/Transformers_banner_1689490231707_0.png","datePublished":"2023-07-07T00:00:00Z","dateModified":"2023-07-07T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2023/07/07/understanding-gpt-transformers/"},"publisher":{"@type":"Organization","name":"KiloBytes by KB","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=post-meta>&lt;span title='2023-07-07 00:00:00 +0000 UTC'>July 7, 2023&lt;/span></div></header><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/Transformers_banner_1689490231707_0.png alt></figure><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>The goal of this series of posts, is to form <em>foundational knowledge</em> that helps us understanding modern state-of-the-art LLM models, and gain a comprehensive understanding of GPT via reading the seminal papers themselves.</p><p>In my previous <a href=https://kshitij-banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-transformers/>post</a>, I covered some of the seminal papers that formulated sequence based models from RNNs to the Attention mechanism in encoder-decoder architectures. If you don&rsquo;t know about them, or would like a quick refresher - I recommend reading through the <a href=https://kshitij-banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-transformers/>previous post</a> before continuing here.</p><p>This post will focus on the &ldquo;Attention is all you need&rdquo; paper that introduced the ground-breaking transformer architecture to the world and has since started a cascading and exponential affect on the AI landscape.</p><h1 id=papers-to-be-covered-in-this-series>Papers to be covered in this series<a hidden class=anchor aria-hidden=true href=#papers-to-be-covered-in-this-series>#</a></h1><ol><li><p><strong>[THIS POST]</strong> Transformers, following Vaswani et al. 2017 Google  <a href=https://arxiv.org/pdf/1706.03762.pdf>Attention is all you need</a></p></li><li><p>GPT-1, following Radford et al. 2018 <a href=https://www.mikecaptain.com/resources/pdf/GPT-1.pdf>Improving Language Understanding by Generative Pre-Training</a></p></li><li><p>GPT-2, following Radford et al. 2018 <a href=https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf>Language Models are Unsupervised Multitask Learners</a></p></li><li><p>BERT, following Devlin et.al. 2019 Google <a href=https://arxiv.org/pdf/1810.04805.pdf>Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p></li><li><p>RoBERTa, following Liu et. al. <a href=https://arxiv.org/pdf/1907.11692.pdf>A Robustly Optimized BERT Pretraining Approach</a></p></li><li><p>GPT-3: Few shot learners, 2020 OpenAI <a href=https://arxiv.org/pdf/2005.14165.pdf>Language Models are Few-Shot Learners</a></p></li><li><p>PaLM: following Chowdhery et al. 2022 <a href=https://arxiv.org/pdf/2204.02311.pdf>Scaling Language Modeling with Pathways</a></p></li><li><p>Maybe: MACAW-LLM, following Lyu et al. 2023 <a href=https://arxiv.org/pdf/2306.09093.pdf>MULTI-MODAL LANGUAGE MODELING</a></p></li></ol><h1 id=transformers>Transformers<a hidden class=anchor aria-hidden=true href=#transformers>#</a></h1><h2 id=paper>Paper<a hidden class=anchor aria-hidden=true href=#paper>#</a></h2><p>Transformers, following Vaswani et al. 2017 Google  <a href=https://arxiv.org/pdf/1706.03762.pdf>Attention is all you need</a></p><h2 id=the-problem-its-solving>The problem its solving<a hidden class=anchor aria-hidden=true href=#the-problem-its-solving>#</a></h2><blockquote><p>This inherently sequential nature (of RNNs) precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples</p></blockquote><h2 id=intention>Intention<a hidden class=anchor aria-hidden=true href=#intention>#</a></h2><blockquote><p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.</p></blockquote><h2 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h2><p><img loading=lazy src=/image_1688744668247_0.png alt=image.png></p><h2 id=main-points>Main Points<a hidden class=anchor aria-hidden=true href=#main-points>#</a></h2><h3 id=attention>Attention<a hidden class=anchor aria-hidden=true href=#attention>#</a></h3><blockquote><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p></blockquote><ul><li>Its helpful to see the additive attention that was described previously in the paper by <a href=https://arxiv.org/pdf/1409.0473.pdf>Dzmitry</a></li></ul><p><img loading=lazy src=/image_1688788567033_0.png alt=image.png></p><p>Refer to the diagram above from the paper by <a href=https://arxiv.org/pdf/1409.0473.pdf>Dzmitry</a>
. In it, attention is realised by creating a context vector C that is generated via an alignment model. The model has weights alpha[tx,ty] that act as weighted sum on the states h[j] of the encoded sentence. This helps provide an &ldquo;attention&rdquo; mechanism.</p><p>I believe the authors are summarising this behaviour by explaining attention as a query + key-value pairs => output.</p><ul><li><p>The query in this case, is the alpha vector that understands which parts of the X[t] to query. The values are the hidden-states h[j], and the keys are the time/positions that relate to that value h.</p></li><li><p>In affect, the attention mechanism is a way for the decoder network to query the positionally encoded hidden states, based on the current state s[t-1]</p></li><li><p>Later in the paper, they also mention the following:- In &ldquo;encoder-decoder attention&rdquo; layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models</p></li><li><p>But note that the previous paper relies on an RNN to create the context of time, which the current papers want to get rid of. So how do we create the keys then?</p></li></ul><p>Another helpful visualisation of attention, is found in the paper <a href=https://arxiv.org/pdf/1703.03906.pdf>Massive Exploration of Neural Machine Translation Architectures</a></p><p><img loading=lazy src=/image_1688789627419_0.png alt=image.png></p><h3 id=scaled-dot-product-attention>Scaled Dot-Product Attention<a hidden class=anchor aria-hidden=true href=#scaled-dot-product-attention>#</a></h3><p>The authors hint that they prefers the multiplicative attention mechanism, due to its computational effeciencies - even though historically the additive attention has proven to work better.</p><h5 id=additive-vs-multiplicative-attention>Additive vs Multiplicative Attention<a hidden class=anchor aria-hidden=true href=#additive-vs-multiplicative-attention>#</a></h5><p>While the transformers paper doesn&rsquo;t explain the difference between the additive and multiplicative versions. The referenced <a href=https://arxiv.org/pdf/1703.03906.pdf>paper</a> can be expanded to understand them.</p><p>Equation 6 is the additive version, and 7 is the multiplicative version</p><p><img loading=lazy src=/image_1688791715884_0.png alt=image.png></p><p>The multiplicative attention is introduced <a href=https://arxiv.org/pdf/1508.04025.pdf>here</a> by Luong et al.</p><p>The authors hypothize that the multiplicative attention has underperformed as it moves the logits into extreme ends where the gradients are close to 0. So they choose to scale down the logits before passing them to the softmax.</p><blockquote><p>We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values</p></blockquote><h5 id=mathematically>Mathematically<a hidden class=anchor aria-hidden=true href=#mathematically>#</a></h5><p><img loading=lazy src=/image_1688789158876_0.png alt=image.png></p><ul><li>This is similar to doing the weighted sum on the values, where the weights are a softmax outputs from aligning the queries and the keys via a dot-product</li></ul><h5 id=visually>Visually<a hidden class=anchor aria-hidden=true href=#visually>#</a></h5><p><img loading=lazy src=/image_1688789296684_0.png alt=image.png></p><h3 id=multi-head-attention>Multi-Head Attention<a hidden class=anchor aria-hidden=true href=#multi-head-attention>#</a></h3><p>Further, the authors propose to do multi-head attention. This is essentially a way to parallelise the attention process on multiple heads instead of a single head.</p><p>So instead of doing a single attention with d_model dimensions. They, parallely run N attention models with d_model/N dimensions each.</p><p>The reason for doing this?</p><blockquote><p>Multi-head attention allows the model to jointly attend to information from different representation<br>subspaces at different positions. With a single attention head, averaging inhibits this.</p></blockquote><h5 id=mathematically-1>Mathematically:<a hidden class=anchor aria-hidden=true href=#mathematically-1>#</a></h5><p><img loading=lazy src=/image_1688828209600_0.png alt=image.png></p><h3 id=self-attention>Self Attention<a hidden class=anchor aria-hidden=true href=#self-attention>#</a></h3><p>Self attention, is essentially where the attention is given to itself, rather than a separate encoder model.</p><p>They use this in the encoder. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</p><h3 id=positional-encoding>Positional Encoding<a hidden class=anchor aria-hidden=true href=#positional-encoding>#</a></h3><p>Since the authors completely got rid of the recurrence, or convolutional parts in the network - they need to provide the model with the positional information to compensate for this missing and crucial context.</p><p>To that effect, they chose to create positional embeddings (with the same dim size as the text embeddings).</p><p>But, they chose to not make them learnable parameters - and that makes sense to me.</p><p>They create the positional embeddings with the following logic</p><p><img loading=lazy src=/image_1689402565327_0.png alt=image.png></p><blockquote><p>We<br>chose this function because we hypothesized it would allow the model to easily learn to attend by<br>relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of<br>P Epos.<br>The d2l.ai book has the best <a href=https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html#positional-encoding>explanation</a> to this that I could find.</p></blockquote><p><img loading=lazy src=/image_1689407591006_0.png alt=image.png></p><p>If we plot different columns, we can see that one can easily be transformed into the other, via linear transformations.</p><p>Even after this though, I don&rsquo;t think I fully understand this part well. For now, I&rsquo;ve marked this as a TODO, and will come back to it later.</p><h2 id=why-self-attention>Why Self-Attention<a hidden class=anchor aria-hidden=true href=#why-self-attention>#</a></h2><p>The core of this goes back to the original intention described towards the beginning of the paper.</p><p>As a reminder</p><blockquote><p>This inherently sequential nature (of RNNs) precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples<br>The authors detail this by comparing the complexity of different layers, and also how traversing the path for finding long-range dependencies is easy with attention, but relatively complex in other forms.</p></blockquote><p>Below is the tabular version for comparison</p><p><img loading=lazy src=/image_1689489195442_0.png alt=image.png></p><h4 id=few-key-points>Few key points<a hidden class=anchor aria-hidden=true href=#few-key-points>#</a></h4><p><strong>Comparison with convolutions</strong></p><ul><li>In convolutions, long range dependencies would require a stack of N/k convolutional layers. Traversing such a path, hence takes Log_k(n). They are generally more expensive then recurrent layers by a factor of k in terms of complexity</li></ul><p><strong>Comparison with recurrence</strong></p><ul><li>The core win here, is that recurrent connections require n sequential operations, which becomes O(1) with self attention</li></ul><p><strong>Attention is also more interpretable</strong></p><ul><li>The authors are able to build attention distributions on the model, to realise that the model is relatively easier to reason about the relationship between positions and tokens.</li></ul><p><img loading=lazy src=/image_1689489530758_0.png alt=image.png></p><h1 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h1><p>The paper packs a ton of things into it. Its brilliant, but also probably takes a few iterations to absorb all the content well.</p><p>I intend to dive into the code that is available in <a href=https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor>tensor2tensor</a> and update this post with more understanding and learnings from the code.</p><p>In the next post, I intend to cover GPT-1 and 2 and work our way towards the GPT-3 and other state-of-the-art model architectures and additions.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/ml/>ML</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/machine-learning/>machine-learning</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/transformers/>Transformers</a></li></ul></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kshitij-banerjee.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2024 <a href=https://Kshitij-Banerjee.github.io>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>