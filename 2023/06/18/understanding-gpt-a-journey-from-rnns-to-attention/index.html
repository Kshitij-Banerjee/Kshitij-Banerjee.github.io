<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script type=text/javascript src=https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js></script><meta name=robots content="index, follow"><title>Understanding GPT - A Journey from RNNs to Attention | KiloBytes by KB</title><meta name=keywords content><meta name=description content="Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood."><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/><link crossorigin=anonymous href=/assets/css/stylesheet.7815c17b278366bc1814e337bfd4e20055d642bc6945b8b7ee716db110d33b25.css integrity="sha256-eBXBeyeDZrwYFOM3v9TiAFXWQrxpRbi37nFtsRDTOyU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta name=author content="Kshitij Banerjee"><meta name=description content="Kshitij Banerjee's notes..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Kshitij Banerjee","url":"https://kshitij-banerjee.github.io/"}</script><meta property="og:title" content="Understanding GPT - A Journey from RNNs to Attention"><meta property="og:description" content="Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood."><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/"><meta property="og:image" content="https://Kshitij-Banerjee.github.io/UnderstandingGPTBanner.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-18T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Kshitij-Banerjee.github.io/UnderstandingGPTBanner.jpg"><meta name=twitter:title content="Understanding GPT - A Journey from RNNs to Attention"><meta name=twitter:description content="Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Understanding GPT - A Journey from RNNs to Attention","item":"https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding GPT - A Journey from RNNs to Attention","name":"Understanding GPT - A Journey from RNNs to Attention","description":"Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I\u0026rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.","keywords":[],"articleBody":"Introduction ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I’ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.\nInspired by Andrej Karpathy’s enlightening ‘makemore’ series, this post aims to dive deep into the key academic papers that shaped our current landscape of language models. From Recurrent Neural Networks (RNNs) to Transformers, let’s demystify these complex concepts together.\nAs of the time of this writing, Andrej hasn’t updated his series in the last 6 months. This leaves a gap in our comprehension as the series jumps from WaveNets to Transformers and GPT. Hence, I’d like this blog to act as a bridge, filling the void for anyone on a similar journey of understanding. Rest assured, when Andrej completes his series, it will serve as a comprehensive resource. Meanwhile, let me summarise as best as I can.\nPapers NOT be going through The following papers are ones, that andrew has explained in a lot of detail in his make more lecture series on YouTube , and I would recommend anyone to go through the series - as its the best explanation I’ve seen so far.\nBigram (one character predicts the next one with a lookup table of counts)\nMLP, following Bengio\nCNN, following DeepMind WaveNet 2016\nPapers intend to deep-dive into: RNN , following Mikolov et al. 2010 Recurrent neural network based language model Bidirectional RNN, following Mike et al 1997 paper Backpropagation through time , followed in Mikael Bod ́en 2001 BPTT LSTM , following Graves et al. 2014 Generating Sequences With Recurrent Neural Networks GRU , following Kyunghyun Cho et al. 2014 On the Properties of Neural Machine Translation: Encoder–Decoder Batch Normalisation, following Sergey Ioffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Layer Normalization, following Jimmy Lei Ba, 2016 Layer Normalization Attention, following Dzmitry Bahdanau, 2015 Dzmitry Bahdanau, 2015 Transformers , following Vaswani et al. 2017 Attention Is All You Need Let’s get started RNN - Recurrent Neural Networks Paper: Mikolov et al. 2010\nSummary The primary challenge that this paper addresses is sequence prediction: given X tokens of a sequence, predict the X+1th token. While the bigram and MLP papers solved this by feeding some fixed context-length to predict the next token, they had their shortcomings - namely fixed and manually set context lengths. To overcome these, the authors propose how a recurrent neural network can “figure-out” the context length instead of manually setting it.\nThe proposed RNNs, can “build a context” of information from the past and incorporate it into their predictions. This feature allows RNNs to capture dependencies between elements in a sequence, making them especially suited for tasks involving sequential data.\nThe problem, in the words of the author:- It is well known that humans can exploit longer context with great success. Also, cache models provide comple-\nmentary information to neural network models, so it is natural to think about a model that would encode temporal information\nimplicitly for contexts with arbitrary lengths\nThe solution The authors then explain how a simple recurrent neural network works\nWhere\nw(t) is the input word at t\ns(t-1) is the state previously generated by the RNN in its last time-step\nOutput layer y(t) represents probability distribution of next word given previous word w(t) and context. Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.\nBack-propagation through time (BPTT) algorithm is used. (This is covered next)\nHow do you back-propagate through the loop ? Backpropagation through time , followed in Mikael Bod ́en 2001 BPTT The key insight is around how to back-propagate through the recursion caused loop\nThe solution is to “unroll” the model T times, and then follow normal backpropation\nInstead, of keeping separate weight matrix for each time-step, the weight matrix is instead shared across the unfolded layers.\nNote, how weights V and U , remain the same through the unfolding process\nImportant quotes from the paper:\nIt is important to note, however, that after error deltas have been calculated, weights are folded back adding up to one big change for each weight. Obviously there is a greater memory requirement (both past errors and activations need to be stored away), the larger τ we choose.\nIn practice, a large τ is quite useless due to a “vanishing gradient effect” (see e.g.\n(Bengio et al., 1994)). For each layer the error is backpropagated through the error\ngets smaller and smaller until it diminishes completely. Some have also pointed out that the instability caused by possibly ambiguous deltas (e.g. (Pollack, 1991)) may disrupt convergence. An opposing result has been put forward for certain learning tasks (Bod ́en et al., 1999).\nNote: Batch normalization and layer normalization were probably not present at this time.\nNotable lines from the paper:-\nBased on our experiments, size of hidden layer should reflect amount of training data - for large amounts of data, large hidden layer is needed\nConvergence is usually achieved after 10-20 epochs.\nregularization of networks to penalize large weights did not provide any significant improvements.\nPyTorch\nCode Doc\nInput:\n(N,L,H**in​) when batch_first=True\nN = Batch Size\nL = Sequence Length\nH_in = Hidden Layer\nBidirectional RNN Paper: Mike et al 1997 paper Future input information coming up later than is usually also useful for prediction. With an RNN, this can be partially\nachieved by delaying the output by a certain number of time frames to include future information. While delaying the output by some frames has been used successfully to improve results in a practical speech recogni-\ntion system [12], which was also confirmed by the experiments conducted here, the optimal delay is task dependent and has to be found by the “trial and error” error method on a validation test set.\nTo overcome the limitations of a regular RNN outlined in the previous section, we propose a bidirectional recurrent\nneural network (BRNN) that can be trained using all available input information in the past and future of a specific time frame.\nLSTM - Long Short-term Memory Paper: Graves et al. 2014 Generating Sequences With Recurrent Neural Networks\nSummary\nQuoting the paper to best describe the problem they are addressing\nIn practice however, standard RNNs are unable to\nstore information about past inputs for very long [15]. As well as diminishing\ntheir ability to model long-range structure, this ‘amnesia’ makes them prone to\ninstability when generating sequences. The problem (common to all conditional\ngenerative models) is that if the network’s predictions are only based on the last\nfew inputs, and these inputs were themselves predicted by the network, it has\nlittle opportunity to recover from past mistakes. Having a longer memory has\na stabilising effect, because even if the network cannot make sense of its recent\nhistory, it can look further back in the past to formulate its predictions.\nIn my words, I understand it as follows\nThe forget gate, tries to find how much to forget in the next iteration. The network learns weights, such that for certain inputs x, at hidden states h and a previous cell state c[t-1] - it predicts how to forget in the next iteration\nSimlarly, the input gate learns how much to store in the new cell state at t.\nCombining both, the new c[t] is a Fc[t-1] + I(WX+Wh+b)\nThe paper has some great examples of text generation and handwriting prediction using LSTMs, that I would encourage going through.\nPaper 2 by Google: hasim et al. 2014 hasim et all The recurrent connections in the\nLSTM layer are directly from the cell output units to the cell input\nunits, input gates, output gates and forget gates. The cell output units\nare connected to the output layer of the network.\nPyTorch:\nReference: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\nGRU - Gated Recurrent Neural Networks Paper:\nCho et. al 2014 Learning Phrase representations using Encoder-Decoder On the Properties of Neural Machine Translation: On the Properties of Neural Machine Translation: Encoder–Decoder paper From the Papers:\nIn addition to a novel model architecture, we also\npropose a new type of hidden unit (f in Eq. (1))\nthat has been motivated by the LSTM unit but is\nmuch simpler to compute and implement.1 Fig. 2\nshows the graphical depiction of the proposed hidden unit.\nWe show that the neural machine translation performs\nrelatively well on short sentences without unknown words,\nbut its performance de-grades rapidly as the length of the sentence\nand the number of unknown words increase.\nFurthermore, we find that the pro-posed gated recursive convolutional net-\nwork learns a grammatical structure of a sentence automatically.\nIn my words, they simplified the task to an update gate and a reset gate, instead of the complicated interactions between multiple gates in LSTMs.\nBoth the reset gates and update gates are a function of the input, and the hidden state at t-1\nand the next h state is calculated as\nAs each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales.\nThose units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active.\nBatch Normalisation Paper: [[Batch Normalisation]], following Sergey Ioffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift The problem\nTraining is complicated by the fact that the inputs to each layer\nare affected by the parameters of all preceding layers – so\nthat small changes to the network parameters amplify as\nthe network becomes deeper.\nChange in the distributions of layers’ inputs presents a problem because the layers need to continu-\nously adapt to the new distribution.\nInput distribution properties that make training more efficient – such as having the same distribution\nbetween the training and test data – apply to training the sub-network as well. As such it is advantageous for the\ndistribution of x to remain fixed over time.\nConsider a layer with a sigmoid activation function z = g(W u + b) where u is the layer input,\nthe weight matrix W and bias vector b are the layer parameters to be learned, and g(x) = 1/ 1+exp(−x) . As |x|\nincreases, g′(x) tends to zero. This means that for all dimensions of x = W u+b except those with small absolute values, the gradient flowing down to u will vanish and the model will train slowly.\nIf, however, we could ensure that the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime,and the training would accelerate.\nSo they propose a new solution\nThe Solution: Introduce a normalization step that fixes the means and variances of every layer inputs\nWhat they tried: One approach could be to modify the network directly at regular intervals, to maintain the normalisation properties. They explain that this doesn’t work because\nThe issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place.\nWe have observed this empirically in initial experiments, where the\nmodel blows up when the normalization parameters are\ncomputed outside the gradient descent step\nAnd hence they proposed the solution to bring the batch-normalisation layer\nTo address this issue, we would like to ensure that, for any parameter values,\nthe network always produces activations with the desired\ndistribution. Doing so would allow the gradient of the\nloss with respect to the model parameters to account for\nthe normalization, and for its dependence on the model\nparameters Θ\nThe Batch Normalisation Layer Algo Important points: Normalises dimensions independently: The algorithm works to normalise each dimension independently. So for each dimension k, it hopes to normalise Enables the layers to still adapt It’s important to not change what each layer represents. So they don’t want to specifically force every activation to be of mean 0 and variance 1. Instead, they introduce the scaling parameters to let the network still learn the biases and scaling factors. The algorithm merely ensures that the distribution of the inputs is maintained.\nThis is done via\nNote that this enables the network to retain the representation\nCons: Creates coupling between the examples in the training\nRather, BNγ,β (x) depends both on the training example and the other examples in the mini-batch.\nThe BN layer is differentiable: Normalization is only needed during training The normalization of activations that\ndepends on the mini-batch allows efficient training, but is\nneither necessary nor desirable during inference\nHence after training, only the population statistics is used for the providing the same effect during inference\nThese statistics are calculated by using moving average method\nWe use the unbiased variance estimate Var[x] = m\nm−1 · EB[σ^2 ], where\nthe expectation is over training mini-batches of size m and σ^2 are their sample variances\nSince the means and variances are fixed during inference,\nthe normalization is simply a linear transform applied to\neach activation.\nFinal Algorithm Layer Normalisation Paper: [[Layer Normalization]], following Jimmy Lei Ba, 2016 Layer Normalization The problem\nBatch normalisation depends on mini-batches, and it isn’t obvious how to use them in an RNN model\nIn feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.\nThe change is to calculate the mean and variance statistics over all the hidden units in a layer, instead of the batches\nAfter that, the famliar bias and gain are added, similar to BN\nAttention: Neural Machine Translation Paper: Attention, following Dzmitry Bahdanau, 2015 Dzmitry Bahdanau, 2015 Background This paper was trying to solve language translation problems, and while the title doesn’t focus on attention - this is the first time that the mechanism of “attention” was provided. So it brings us to the foundations of how attention came to be.\nAt the time of this paper, the encoder-decoder architecture is prominent for translation. Namely, a bidirectional RNN is used to encode the source sentence, and the decoder RNN is conditioned on the output of this encoder RNN to produce the translated sentence.\nProblem In the words of the author:\nA potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences\nSolution Abstract: Introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.\nArchitecture The authors propose a novel architecture, each output word in the decoder is created by considering not only the previous hidden state of the decoder, but also considering a context vector C of the encoder network outputs. This context vector itself is a weighted sum of the hidden states of the encoder network, where the weights are trained and learn the “alignment” between output words and input words.\nMath i, is used for the decoder network , and the j is used for the encoder networks\nThe hidden states s[i] of the decoder RNN are calculated as a function of s[i-1], y[i-1] and c[i]\nThe context vector c[i] is calculated as a weighted sum of all the encoder hidden states h[j]\nThese α weights are an important piece here. These represent the “alignment” of the decoded word to the encoded sentence. Hence, these are trained to be a function of s[i-1], and h[j]. Essentially, the weights help the model understand how much of the j_th input word is resposible for translating the ith output/decoded state.\nNote:\nWe parametrize the alignment model a as a feedforward neural network which is jointly trained with all the other components of the proposed system. the alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through.\nFinally, p(y) is conditioned on previous words, hidden state s[i], and the context vector c[i]\nThe Golden Words The probability αij , or its associated energy eij , reflects the importance of the annotation hj with respect to the previous hidden state si−1 in deciding the next state si and generating yi. Intuitively, this implements a mechanism of attention in the decoder. The decoder decides parts of the source sentence to pay attention to.\nBy letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.\nResults In the next post Now thats we’ve covered some of the basics from RNNs to Attention, we’ll cover more advanced topics in the next post.\nConclusion A detailed analysis of each influential paper in this domain can facilitate a comprehensive understanding of these models. Recognizing the limitations of each model and how succeeding models strive to address them is integral to this exploration.\nWhile the completion of Andrej Karpathy’s series is anticipated, further exploration of these foundational works will serve to strengthen our understanding of modern language models. Anticipate future posts in this series, which will delve into the realm of Transformers.\nI invite readers to share their insights on these concepts. Which paper do you consider most intriguing?\nIf i’ve made errors or haven’t described something correctly - please do comment, help me learn and correct the article for future readers.\n","wordCount":"3068","inLanguage":"en","image":"https://Kshitij-Banerjee.github.io/UnderstandingGPTBanner.jpg","datePublished":"2023-06-18T00:00:00Z","dateModified":"2023-06-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2023/06/18/understanding-gpt-a-journey-from-rnns-to-attention/"},"publisher":{"@type":"Organization","name":"KiloBytes by KB","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=post-meta><span title='2023-06-18 00:00:00 +0000 UTC'>June 18, 2023</span></div></header><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/UnderstandingGPTBanner.jpg alt></figure><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>ChatGPT has rightly taken the world by storm, and has possibly started the 6th wave. Given its importance, the rush to build new products and research on top is understandable. But, I&rsquo;ve always liked to ground myself with foundational knowledge on how things work, before exploring anything additive. To gain such foundational knowledge, I believe understanding the progression of techniques and models is crucial to comprehend how these LLM models work under the hood.</p><p>Inspired by Andrej Karpathy&rsquo;s enlightening &lsquo;makemore&rsquo; series, this post aims to dive deep into the key academic papers that shaped our current landscape of language models. From Recurrent Neural Networks (RNNs) to Transformers, let&rsquo;s demystify these complex concepts together.</p><p>As of the time of this writing, Andrej hasn&rsquo;t updated his series in the last 6 months. This leaves a gap in our comprehension as the series jumps from WaveNets to Transformers and GPT. Hence, I&rsquo;d like this blog to act as a bridge, filling the void for anyone on a similar journey of understanding. Rest assured, when Andrej completes his series, it will serve as a comprehensive resource. Meanwhile, let me summarise as best as I can.</p><h2 id=papers-not-be-going-through>Papers NOT be going through<a hidden class=anchor aria-hidden=true href=#papers-not-be-going-through>#</a></h2><p>The following papers are ones, that andrew has explained in a lot of detail in his make more lecture series on <a href="https://www.youtube.com/watch?v=PaCmpygFfXo">YouTube</a> , and I would recommend anyone to go through the series - as its the best explanation I&rsquo;ve seen so far.</p><p>Bigram (one character predicts the next one with a lookup table of counts)</p><p>MLP, following <a href=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>Bengio</a></p><p>CNN, following <a href=https://arxiv.org/abs/1609.03499>DeepMind WaveNet 2016</a></p><h2 id=papers-intend-to-deep-dive-into>Papers intend to deep-dive into:<a hidden class=anchor aria-hidden=true href=#papers-intend-to-deep-dive-into>#</a></h2><ol><li><p>RNN , following Mikolov et al. 2010 <a href=https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf>Recurrent neural network based language model</a></p></li><li><p>Bidirectional RNN, following Mike et al 1997 <a href=https://deeplearning.cs.cmu.edu/F23/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf>paper</a></p></li><li><p>Backpropagation through time , followed in Mikael Bod ́en 2001 <a href=https://axon.cs.byu.edu/~martinez/classes/678/Papers/RNN_Intro.pdf>BPTT</a></p></li><li><p>LSTM , following Graves et al. 2014 <a href=https://arxiv.org/pdf/1308.0850.pdf>Generating Sequences With Recurrent Neural Networks</a></p></li><li><p>GRU , following Kyunghyun Cho et al. 2014 <a href=https://arxiv.org/pdf/1409.1259.pdf>On the Properties of Neural Machine Translation: Encoder–Decoder</a></p></li><li><p>Batch Normalisation, following Sergey Ioffe et al. 2015 <a href=https://arxiv.org/pdf/1502.03167.pdf>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p></li><li><p>Layer Normalization, following Jimmy Lei Ba, 2016 <a href=https://arxiv.org/pdf/1607.06450.pdf>Layer Normalization</a></p></li><li><p>Attention, following Dzmitry Bahdanau, 2015 <a href=https://arxiv.org/pdf/1409.0473.pdf>Dzmitry Bahdanau, 2015</a></p></li><li><p>Transformers , following Vaswani et al. 2017 <a href=https://arxiv.org/pdf/1706.03762.pdf>Attention Is All You Need</a></p></li></ol><h2 id=lets-get-started>Let&rsquo;s get started<a hidden class=anchor aria-hidden=true href=#lets-get-started>#</a></h2><h3 id=rnn---recurrent-neural-networks>RNN - Recurrent Neural Networks<a hidden class=anchor aria-hidden=true href=#rnn---recurrent-neural-networks>#</a></h3><p><em>Paper: <a href=https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf>Mikolov et al. 2010</a></em></p><h4 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h4><p>The primary challenge that this paper addresses is sequence prediction: given X tokens of a sequence, predict the X+1th token. While the bigram and <a href=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>MLP</a> papers solved this by feeding some fixed context-length to predict the next token, they had their shortcomings - namely fixed and manually set context lengths. To overcome these, the authors propose how a recurrent neural network can &ldquo;figure-out&rdquo; the context length instead of manually setting it.</p><p>The proposed RNNs, can &ldquo;build a context&rdquo; of information from the past and incorporate it into their predictions. This feature allows RNNs to capture dependencies between elements in a sequence, making them especially suited for tasks involving sequential data.</p><h4 id=the-problem-in-the-words-of-the-author->The problem, in the words of the author:-<a hidden class=anchor aria-hidden=true href=#the-problem-in-the-words-of-the-author->#</a></h4><blockquote><p>It is well known that humans can exploit longer context with great success. Also, cache models provide comple-<br>mentary information to neural network models, so it is natural to think about a model that would encode temporal information<br>implicitly for contexts with arbitrary lengths</p></blockquote><h4 id=the-solution>The solution<a hidden class=anchor aria-hidden=true href=#the-solution>#</a></h4><p>The authors then explain how a simple recurrent neural network works</p><p><img loading=lazy src=/image_1687079484371_0.png alt=image.png></p><p><img loading=lazy src=/image_1687079552173_0.png alt=image.png></p><p>Where</p><p>w(t) is the input word at t</p><p>s(t-1) is the state previously generated by the RNN in its last time-step</p><p>Output layer y(t) represents probability distribution of next word given previous word w(t) and context. Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.</p><p>Back-propagation through time (BPTT) algorithm is used. (This is covered next)</p><h4 id=how-do-you-back-propagate-through-the-loop->How do you back-propagate through the loop ?<a hidden class=anchor aria-hidden=true href=#how-do-you-back-propagate-through-the-loop->#</a></h4><p>Backpropagation through time , followed in Mikael Bod ́en 2001 <a href=https://axon.cs.byu.edu/~martinez/classes/678/Papers/RNN_Intro.pdf>BPTT</a></p><p>The key insight is around how to back-propagate through the recursion caused loop</p><p>The solution is to &ldquo;unroll&rdquo; the model T times, and then follow normal backpropation</p><p><strong>Instead, of keeping separate weight matrix for each time-step, the weight matrix is instead shared across the unfolded layers.</strong></p><p><img loading=lazy src=/image_1687080413026_0.png alt=image.png></p><p>Note, how weights V and U , remain the same through the unfolding process</p><p>Important quotes from the paper:</p><blockquote><p>It is important to note, however, that after error deltas have been calculated, weights are folded back adding up to one big change for each weight. Obviously there is a greater memory requirement (both past errors and activations need to be stored away), the larger τ we choose.<br>In practice, a large τ is quite useless due to a “vanishing gradient effect” (see e.g.<br>(Bengio et al., 1994)). For each layer the error is backpropagated through the error<br>gets smaller and smaller until it diminishes completely. Some have also pointed out that the instability caused by possibly ambiguous deltas (e.g. (Pollack, 1991)) may disrupt convergence. An opposing result has been put forward for certain learning tasks (Bod ́en et al., 1999).<br>Note: Batch normalization and layer normalization were probably not present at this time.</p></blockquote><p>Notable lines from the paper:-</p><blockquote><p>Based on our experiments, size of hidden layer should reflect amount of training data - for large amounts of data, large hidden layer is needed<br>Convergence is usually achieved after 10-20 epochs.<br>regularization of networks to penalize large weights did not provide any significant improvements.<br>PyTorch</p></blockquote><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.RNN.html>Code Doc</a></p><p><img loading=lazy src=/image_1687081259085_0.png alt=image.png></p><p>Input:</p><p>(<em>N</em>,<em>L</em>,<em>H**in</em>​) when batch_first=True</p><p>N = Batch Size</p><p>L = Sequence Length</p><p>H_in = Hidden Layer</p><h3 id=bidirectional-rnn>Bidirectional RNN<a hidden class=anchor aria-hidden=true href=#bidirectional-rnn>#</a></h3><p><em>Paper: Mike et al 1997 <a href=https://deeplearning.cs.cmu.edu/F23/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf>paper</a></em></p><blockquote><p>Future input information coming up later than is usually also useful for prediction. With an RNN, this can be partially<br>achieved by delaying the output by a certain number of time frames to include future information. While delaying the output by some frames has been used successfully to improve results in a practical speech recogni-<br>tion system [12], which was also confirmed by the experiments conducted here, the optimal delay is task dependent and has to be found by the “trial and error” error method on a validation test set.<br>To overcome the limitations of a regular RNN outlined in the previous section, we propose a bidirectional recurrent<br>neural network (BRNN) that can be trained using all available input information in the past and future of a specific time frame.<br><img loading=lazy src=/image_1687353507326_0.png alt=image.png></p></blockquote><h3 id=lstm---long-short-term-memory>LSTM - Long Short-term Memory<a hidden class=anchor aria-hidden=true href=#lstm---long-short-term-memory>#</a></h3><p><em>Paper: Graves et al. 2014 <a href=https://arxiv.org/pdf/1308.0850.pdf>Generating Sequences With Recurrent Neural Networks</a></em></p><p><strong>Summary</strong></p><p>Quoting the paper to best describe the problem they are addressing</p><blockquote><p>In practice however, standard RNNs are unable to<br>store information about past inputs for very long [15]. As well as diminishing<br>their ability to model long-range structure, this ‘amnesia’ makes them prone to<br>instability when generating sequences. The problem (common to all conditional<br>generative models) is that if the network’s predictions are only based on the last<br>few inputs, and these inputs were themselves predicted by the network, it has<br>little opportunity to recover from past mistakes. Having a longer memory has<br>a stabilising effect, because even if the network cannot make sense of its recent<br>history, it can look further back in the past to formulate its predictions.<br><img loading=lazy src=/image_1687273677960_0.png alt=image.png></p></blockquote><p><img loading=lazy src=/image_1687273755357_0.png alt=image.png></p><p>In my words, I understand it as follows</p><p>The forget gate, tries to find how much to forget in the next iteration. The network learns weights, such that for certain inputs x, at hidden states h and a previous cell state c[t-1] - it predicts how to forget in the next iteration</p><p>Simlarly, the input gate learns how much to store in the new cell state at t.</p><p>Combining both, the new c[t] is a F<em>c[t-1] + I</em>(WX+Wh+b)</p><p>The paper has some great examples of text generation and handwriting prediction using LSTMs, that I would encourage going through.</p><p>Paper 2 by Google: hasim et al. 2014 <a href=https://arxiv.org/pdf/1402.1128.pdf>hasim et all</a></p><blockquote><p>The recurrent connections in the<br>LSTM layer are directly from the cell output units to the cell input<br>units, input gates, output gates and forget gates. The cell output units<br>are connected to the output layer of the network.<br><img loading=lazy src=/image_1687352816805_0.png alt=image.png></p></blockquote><p>PyTorch:</p><p>Reference: <a href=https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html</a></p><h3 id=gru---gated-recurrent-neural-networks>GRU - Gated Recurrent Neural Networks<a hidden class=anchor aria-hidden=true href=#gru---gated-recurrent-neural-networks>#</a></h3><p>Paper:</p><p><a href=https://arxiv.org/pdf/1406.1078.pdf>Cho et. al 2014</a> <a href=https://arxiv.org/pdf/1406.1078.pdf>Learning Phrase representations using Encoder-Decoder</a></p><p>On the Properties of Neural Machine Translation: <a href=https://arxiv.org/pdf/1409.1259.pdf>On the Properties of Neural Machine Translation: Encoder–Decoder</a></p><a href=https://arxiv.org/pdf/1412.3555.pdf>paper</a><p><strong>From the Papers:</strong></p><blockquote><p>In addition to a novel model architecture, we also<br>propose a new type of hidden unit (f in Eq. (1))<br>that has been motivated by the LSTM unit but is<br>much simpler to compute and implement.1 Fig. 2<br>shows the graphical depiction of the proposed hidden unit.<br>We show that the neural machine translation performs<br>relatively well on short sentences without unknown words,<br>but its performance de-grades rapidly as the length of the sentence<br>and the number of unknown words increase.<br>Furthermore, we find that the pro-posed gated recursive convolutional net-<br>work learns a grammatical structure of a sentence automatically.<br><img loading=lazy src=/image_1687355590275_0.png alt=image.png></p></blockquote><p>In my words, they simplified the task to an update gate and a reset gate, instead of the complicated interactions between multiple gates in LSTMs.</p><p>Both the reset gates and update gates are a function of the input, and the hidden state at t-1</p><p><img loading=lazy src=/image_1687590382787_0.png alt=image.png></p><p><img loading=lazy src=/image_1687590390930_0.png alt=image.png></p><p>and the next h state is calculated as</p><p><img loading=lazy src=/image_1687590493510_0.png alt=image.png></p><p>As each hidden unit has separate reset and update gates, each hidden unit will learn to capture
dependencies over different time scales.<br>Those units that learn to capture short-term dependencies
will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active.</p><h3 id=batch-normalisation>Batch Normalisation<a hidden class=anchor aria-hidden=true href=#batch-normalisation>#</a></h3><p><em>Paper: [[Batch Normalisation]], following Sergey Ioffe et al. 2015 <a href=https://arxiv.org/pdf/1502.03167.pdf>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></em></p><p><strong>The problem</strong></p><blockquote><p>Training is complicated by the fact that the inputs to each layer<br>are affected by the parameters of all preceding layers – so<br>that small changes to the network parameters amplify as<br>the network becomes deeper.</p></blockquote><blockquote><p>Change in the distributions of layers’ inputs presents a problem because the layers need to continu-<br>ously adapt to the new distribution.</p></blockquote><blockquote><p>Input distribution properties that make training more efficient – such as having the same distribution<br>between the training and test data – apply to training the sub-network as well. As such it is advantageous for the<br>distribution of x to remain fixed over time.</p></blockquote><blockquote><p>Consider a layer with a sigmoid activation function z = g(W u + b) where u is the layer input,<br>the weight matrix W and bias vector b are the layer parameters to be learned, and g(x) = 1/ 1+exp(−x) . As |x|<br>increases, g′(x) tends to zero. This means that for all dimensions of x = W u+b except those with small absolute values, the gradient flowing down to u will vanish and the model will train slowly.</p></blockquote><blockquote><p>If, however, we could ensure that the distribution of nonlinearity inputs remains more<br>stable as the network trains, then the optimizer would be less likely to get stuck in the saturated regime,and the training would accelerate.<br>So they propose a new solution</p></blockquote><h4 id=the-solution-1>The Solution:<a hidden class=anchor aria-hidden=true href=#the-solution-1>#</a></h4><p>Introduce a normalization step that fixes the means and variances of every layer inputs</p><p>What they tried: One approach could be to modify the network directly at regular intervals, to maintain the normalisation properties. They explain that this doesn&rsquo;t work because</p><blockquote><p>The issue with the above approach is that the gradient descent optimization does not take into account the fact that the normalization takes place.</p></blockquote><blockquote><p>We have observed this empirically in initial experiments, where the<br>model blows up when the normalization parameters are<br>computed outside the gradient descent step<br>And hence they proposed the solution to bring the batch-normalisation layer</p></blockquote><blockquote><p>To address this issue, we would like to ensure that, for any parameter values,<br>the network always produces activations with the desired<br>distribution. Doing so would allow the gradient of the<br>loss with respect to the model parameters to account for<br>the normalization, and for its dependence on the model<br>parameters Θ</p></blockquote><h4 id=the-batch-normalisation-layer-algo>The Batch Normalisation Layer Algo<a hidden class=anchor aria-hidden=true href=#the-batch-normalisation-layer-algo>#</a></h4><p><img loading=lazy src=/image_1688217694180_0.png alt=image.png></p><h5 id=important-points>Important points:<a hidden class=anchor aria-hidden=true href=#important-points>#</a></h5><h6 id=normalises-dimensions-independently><em>Normalises dimensions independently:</em><a hidden class=anchor aria-hidden=true href=#normalises-dimensions-independently>#</a></h6><p>The algorithm works to normalise each dimension independently. So for each dimension k, it hopes to normalise
<img loading=lazy src=/image_1688217792094_0.png alt=image.png></p><h6 id=enables-the-layers-to-still-adapt><em>Enables the layers to still adapt</em><a hidden class=anchor aria-hidden=true href=#enables-the-layers-to-still-adapt>#</a></h6><p>It&rsquo;s important to not change what each layer represents. So they don&rsquo;t want to specifically force every activation to be of mean 0 and variance 1. Instead, they introduce the scaling parameters to let the network still learn the biases and scaling factors. The algorithm merely ensures that the <em>distribution</em> of the inputs is maintained.</p><p>This is done via</p><p><img loading=lazy src=/image_1688217971417_0.png alt=image.png></p><p>Note that this enables the network to retain the representation</p><p><em>Cons: Creates coupling</em> between the examples in the training</p><blockquote><p>Rather, BNγ,β (x) depends both on the training example and the other examples in the mini-batch.</p></blockquote><h6 id=the-bn-layer-is-differentiable><em>The BN layer is differentiable:</em><a hidden class=anchor aria-hidden=true href=#the-bn-layer-is-differentiable>#</a></h6><p><img loading=lazy src=/image_1688218206587_0.png alt=image.png></p><h5 id=normalization-is-only-needed-during-training><em>Normalization is only needed during training</em><a hidden class=anchor aria-hidden=true href=#normalization-is-only-needed-during-training>#</a></h5><blockquote><p>The normalization of activations that<br>depends on the mini-batch allows efficient training, but is<br>neither necessary nor desirable during inference<br>Hence after training, only the population statistics is used for the providing the same effect during inference</p></blockquote><p><img loading=lazy src=/image_1688218400152_0.png alt=image.png></p><p>These statistics are calculated by using moving average method</p><blockquote><p>We use the unbiased variance estimate Var[x] = m<br>m−1 · EB[σ^2 ], where<br>the expectation is over training mini-batches of size m and σ^2 are their sample variances</p></blockquote><blockquote><p>Since the means and variances are fixed during inference,<br>the normalization is simply a linear transform applied to<br>each activation.</p></blockquote><h4 id=final-algorithm>Final Algorithm<a hidden class=anchor aria-hidden=true href=#final-algorithm>#</a></h4><p><img loading=lazy src=/image_1688218663442_0.png alt=image.png></p><h3 id=layer-normalisation>Layer Normalisation<a hidden class=anchor aria-hidden=true href=#layer-normalisation>#</a></h3><p><em>Paper: [[Layer Normalization]], following Jimmy Lei Ba, 2016 <a href=https://arxiv.org/pdf/1607.06450.pdf>Layer Normalization</a></em></p><p><strong>The problem</strong></p><p>Batch normalisation depends on mini-batches, and it isn&rsquo;t obvious how to use them in an RNN model</p><blockquote><p>In feed-forward networks with fixed depth, it is straightforward to store the statistics separately for each hidden layer. However, the summed inputs to the recurrent neurons in a recurrent neural network (RNN) often vary with the length of the sequence so applying batch normalization to RNNs appears to require different statistics for different time-steps.<br>The change is to calculate the mean and variance statistics over all the hidden units in a layer, instead of the batches</p></blockquote><p><img loading=lazy src=/image_1688222306090_0.png alt=image.png></p><p>After that, the famliar bias and gain are added, similar to BN</p><p><img loading=lazy src=/image_1688222333541_0.png alt=image.png></p><h3 id=attention-neural-machine-translation>Attention: Neural Machine Translation<a hidden class=anchor aria-hidden=true href=#attention-neural-machine-translation>#</a></h3><p>Paper: Attention, following Dzmitry Bahdanau, 2015 <a href=https://arxiv.org/pdf/1409.0473.pdf>Dzmitry Bahdanau, 2015</a></p><h4 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h4><p>This paper was trying to solve language translation problems, and while the title doesn&rsquo;t focus on attention - this is the first time that the mechanism of &ldquo;attention&rdquo; was provided. So it brings us to the foundations of how attention came to be.</p><p>At the time of this paper, the encoder-decoder architecture is prominent for translation. Namely, a bidirectional RNN is used to encode the source sentence, and the decoder RNN is conditioned on the output of this encoder RNN to produce the translated sentence.</p><h4 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h4><p>In the words of the author:</p><blockquote><p>A potential issue with this encoder–decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences</p></blockquote><h4 id=solution-abstract>Solution Abstract:<a hidden class=anchor aria-hidden=true href=#solution-abstract>#</a></h4><blockquote><p>Introduce an extension to the encoder–decoder model which learns to align and translate jointly. Each time the proposed model generates a word in a translation, it (soft-)searches for a set of positions in a source sentence where the most relevant information is concentrated. The model then predicts a target word based on the context vectors associated with these source positions and all the previous generated target words.</p></blockquote><h4 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h4><p><img loading=lazy src=/image_1688718307045_0.png alt=image.png></p><p>The authors propose a novel architecture, each output word in the decoder is created by considering not only the previous hidden state of the decoder, but also considering a context vector C of the encoder network outputs. This context vector itself is a weighted sum of the hidden states of the encoder network, where the weights are trained and learn the &ldquo;alignment&rdquo; between output words and input words.</p><h4 id=math>Math<a hidden class=anchor aria-hidden=true href=#math>#</a></h4><p>i, is used for the decoder network , and the j is used for the encoder networks</p><p>The hidden states s[i] of the decoder RNN are calculated as a function of s[i-1], y[i-1] and c[i]</p><p><img loading=lazy src=/image_1688719158646_0.png alt=image.png></p><p>The context vector c[i] is calculated as a weighted sum of all the encoder hidden states h[j]</p><p><img loading=lazy src=/image_1688719251116_0.png alt=image.png></p><p>These α weights are an important piece here. These represent the &ldquo;alignment&rdquo; of the decoded word to the encoded sentence. Hence, these are trained to be a function of s[i-1], and h[j]. Essentially, the weights help the model understand how much of the j_th input word is resposible for translating the ith output/decoded state.</p><p><img loading=lazy src=/image_1688719468000_0.png alt=image.png></p><p>Note:</p><blockquote><p>We parametrize the alignment model a as a feedforward neural network which is <strong>jointly</strong> trained with all the other components of the proposed system. the alignment model directly computes a soft alignment, which allows the gradient of the cost function to be backpropagated through.<br>Finally, p(y) is conditioned on previous words, hidden state s[i], and the context vector c[i]</p></blockquote><p><img loading=lazy src=/image_1688719050822_0.png alt=image.png></p><h4 id=the-golden-words>The Golden Words<a hidden class=anchor aria-hidden=true href=#the-golden-words>#</a></h4><blockquote><p>The probability αij , or its associated energy eij , reflects the importance of the annotation hj with respect to the previous hidden state si−1 in deciding the next state si and generating yi. Intuitively, this implements a mechanism of <strong>attention</strong> in the decoder. The decoder decides parts of the source sentence to pay attention to.<br>By letting the decoder have an attention mechanism, we relieve the encoder from the burden of having to encode all information in the source sentence into a fixedlength vector. With this new approach the information can be spread throughout the sequence of annotations, which can be selectively retrieved by the decoder accordingly.</p></blockquote><h4 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h4><p><img loading=lazy src=/image_1688732372869_0.png alt=image.png></p><h2 id=in-the-next-post>In the next post<a hidden class=anchor aria-hidden=true href=#in-the-next-post>#</a></h2><p>Now thats we&rsquo;ve covered some of the basics from RNNs to Attention, we&rsquo;ll cover more advanced topics in the next post.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>A detailed analysis of each influential paper in this domain can facilitate a comprehensive understanding of these models. Recognizing the limitations of each model and how succeeding models strive to address them is integral to this exploration.</p><p>While the completion of Andrej Karpathy&rsquo;s series is anticipated, further exploration of these foundational works will serve to strengthen our understanding of modern language models. Anticipate future posts in this series, which will delve into the realm of Transformers.</p><p>I invite readers to share their insights on these concepts. Which paper do you consider most intriguing?</p><p>If i&rsquo;ve made errors or haven&rsquo;t described something correctly - please do comment, help me learn and correct the article for future readers.</p></div><footer class=post-footer><ul class=post-tags></ul></footer><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kshitij-banerjee.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2023 <a href=https://Kshitij-Banerjee.github.io>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>