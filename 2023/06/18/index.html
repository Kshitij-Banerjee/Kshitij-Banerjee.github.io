<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script type=text/javascript src=https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js></script><meta name=robots content="index, follow"><title>Kshitij's Notes</title><meta name=keywords content><meta name=description content="Introduction This post is inspired by Andrej Karpathy&rsquo;s makemore series. Unfortunately, as of the time of this writing, he hasn&rsquo;t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding."><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2023/06/18/><link crossorigin=anonymous href=/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=author content="Kshitij Banerjee"><meta name=description content="Kshitij Banerjee's notes..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Kshitij Banerjee","url":"https://kshitij-banerjee.github.io/"}</script><meta property="og:title" content><meta property="og:description" content="Introduction This post is inspired by Andrej Karpathy&rsquo;s makemore series. Unfortunately, as of the time of this writing, he hasn&rsquo;t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding."><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2023/06/18/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-06-18T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="Introduction This post is inspired by Andrej Karpathy&rsquo;s makemore series. Unfortunately, as of the time of this writing, he hasn&rsquo;t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":3,"name":"","item":"https://Kshitij-Banerjee.github.io/2023/06/18/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"","name":"","description":"Introduction This post is inspired by Andrej Karpathy\u0026rsquo;s makemore series. Unfortunately, as of the time of this writing, he hasn\u0026rsquo;t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding.","keywords":[],"articleBody":"Introduction This post is inspired by Andrej Karpathy’s makemore series. Unfortunately, as of the time of this writing, he hasn’t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding. The series jumps straight from wave nets, to transformers and GPT and it leaves a lot to be filled.\nSo as I go on my path to understand the history, I’d like this to fill the gap for anyone else who might be going through the same journey. I believe, andrej will eventually complete this series in terms of code, and when it does - it will be a far better source than this post will ever be.\nBut, if you’re like me, and looking for a crash-course on the various papers and the core concepts. Follow along:-\nPapers NOT be going through The following papers are ones, that andrew has explained in a lot of detail in his make more lecture series on YouTube , and I would recommend anyone to go through the series - as its the best explanation I’ve seen so far.\nBigram (one character predicts the next one with a lookup table of counts)\nMLP, following Bengio\nCNN, following DeepMind WaveNet 2016\nPapers intend to deep-dive into: RNN , following Mikolov et al. 2010 Recurrent neural network based language model Backpropagation through time , followed in Mikael Bod ́en 2001 BPTT LSTM , following Graves et al. 2014 Generating Sequences With Recurrent Neural Networks GRU , following Kyunghyun Cho et al. 2014 On the Properties of Neural Machine Translation: Encoder–Decoder Batch Normalisation, following Sergey Ioffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift Attention, following Dzmitry Bahdanau, 2015 Dzmitry Bahdanau, 2015 Layer Normalization, following Jimmy Lei Ba, 2016 Layer Normalization Transformers , following Vaswani et al. 2017 Attention Is All You Need Let’s get started RNN - Recurrent Neural Networks Paper: Mikolov et al. 2010\nSummary\nIn effect, the paper references the MLP paper by Bengio, and cites the shortcoming that the context length is fixed and manually set. (The context length is the number of previous tokens that are fed into the MLP to generate the next word).\nQuoting the author:-\nIt is well known that humans can exploit longer context with great success. Also, cache models provide comple-\nmentary information to neural network models, so it is natural to think about a model that would encode temporal information\nimplicitly for contexts with arbitrary lengths\nThe authors then explain how a simple recurrent neural network works\nWhere\nw(t) is the input word at t\ns(t-1) is the state previously generated by the RNN in its last time-step\nOutput layer y(t) represents probability distribution of next word given previous word w(t) and context. Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.\nBack-propagation through time (BPTT) algorithm is used. (This is covered next)\nNotable lines:\nBased on our experiments, size of hidden layer should reflect amount of training data - for large\namounts of data, large hidden layer is needed\nConvergence is usually achieved after 10-20 epochs.\nregularization of networks to penalize large weights did not provide any significant improvements.\nBackpropagation through time , followed in Mikael Bod ́en 2001\nThe key insight is around how to back-propagate through the recursion caused loop\nThe solution is to “unroll” the model T times, and then follow normal backpropation\nInstead, of keeping separate weight matrix for each time-step, the weight matrix is instead shared across the unfolded layers.\nNote, how weights V and U , remain the same through the unfolding process\nImportant quotes from the paper:\nIt is important to note, however, that after error deltas have been calculated, weights\nare folded back adding up to one big change for each weight. Obviously there is a greater\nmemory requirement (both past errors and activations need to be stored away), the larger\nτ we choose.\nIn practice, a large τ is quite useless due to a “vanishing gradient effect” (see e.g.\n(Bengio et al., 1994)). For each layer the error is backpropagated through the error\ngets smaller and smaller until it diminishes completely. Some have also pointed out that\nthe instability caused by possibly ambiguous deltas (e.g. (Pollack, 1991)) may disrupt\nconvergence. An opposing result has been put forward for certain learning tasks (Bod ́en\net al., 1999).\nNote that batch normalization and layer normalization were probably not present at this time.\nPyTorch\nCode Doc\nInput:\n(N,L,H**in​) when batch_first=True\nN = Batch Size\nL = Sequence Length\nH_in = Hidden Layer\nBidirectional RNN Paper: Mike et al 1997 paper Future input information coming up later than is usually also useful for prediction. With an RNN, this can be partially\nachieved by delaying the output by a certain number of time frames to include future information. While delaying the output by some frames has been used successfully to improve results in a practical speech recogni-\ntion system [12], which was also confirmed by the experiments conducted here, the optimal delay is task dependent and has to be found by the “trial and error” error method on a validation test set.\nTo overcome the limitations of a regular RNN outlined in the previous section, we propose a bidirectional recurrent\nneural network (BRNN) that can be trained using all available input information in the past and future of a specific time frame.\nLSTM - Long Short-term Memory Paper: Graves et al. 2014 Generating Sequences With Recurrent Neural Networks\nSummary\nQuoting the paper to best describe the problem they are addressing\nIn practice however, standard RNNs are unable to\nstore information about past inputs for very long [15]. As well as diminishing\ntheir ability to model long-range structure, this ‘amnesia’ makes them prone to\ninstability when generating sequences. The problem (common to all conditional\ngenerative models) is that if the network’s predictions are only based on the last\nfew inputs, and these inputs were themselves predicted by the network, it has\nlittle opportunity to recover from past mistakes. Having a longer memory has\na stabilising effect, because even if the network cannot make sense of its recent\nhistory, it can look further back in the past to formulate its predictions.\nIn my words, I understand it as follows\nThe forget gate, tries to find how much to forget in the next iteration. The network learns weights, such that for certain inputs x, at hidden states h and a previous cell state c[t-1] - it predicts how to forget in the next iteration\nSimlarly, the input gate learns how much to store in the new cell state at t.\nCombining both, the new c[t] is a Fc[t-1] + I(WX+Wh+b)\nThe paper has some great examples of text generation and handwriting prediction using LSTMs, that I would encourage going through.\nPaper 2 by Google: hasim et al. 2014 hasim et all The recurrent connections in the\nLSTM layer are directly from the cell output units to the cell input\nunits, input gates, output gates and forget gates. The cell output units\nare connected to the output layer of the network.\nPyTorch:\nReference: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\nGRU - Gated Recurrent Neural Networks Paper:\nCho et. al 2014 Learning Phrase representations using Encoder-Decoder On the Properties of Neural Machine Translation: On the Properties of Neural Machine Translation: Encoder–Decoder paper From the Papers:\nIn addition to a novel model architecture, we also\npropose a new type of hidden unit (f in Eq. (1))\nthat has been motivated by the LSTM unit but is\nmuch simpler to compute and implement.1 Fig. 2\nshows the graphical depiction of the proposed hidden unit.\nWe show that the neural machine translation performs\nrelatively well on short sentences without unknown words,\nbut its performance de-grades rapidly as the length of the sentence\nand the number of unknown words increase.\nFurthermore, we find that the pro-posed gated recursive convolutional net-\nwork learns a grammatical structure of a sentence automatically.\nIn my words, they simplified the task to an update gate and a reset gate, instead of the complicated interactions between multiple gates in LSTMs.\nBoth the reset gates and update gates are a function of the input, and the hidden state at t-1\nand the next h state is calculated as\nAs each hidden unit has separate reset and update gates, each hidden unit will learn to capture dependencies over different time scales.\nThose units that learn to capture short-term dependencies will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active.\nBatch Normalisation Paper: Batch Normalisation, following Sergey Ioffe et al. 2015 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift TO BE CONTINUED..\n","wordCount":"1522","inLanguage":"en","datePublished":"2023-06-18T00:00:00Z","dateModified":"2023-06-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2023/06/18/"},"publisher":{"@type":"Organization","name":"Kshitij's Notes","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="Kshitij's Notes (Alt + H)">Kshitij's Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title></h1><div class=post-meta><span title='2023-06-18 00:00:00 +0000 UTC'>June 18, 2023</span></div></header><div class=post-content><h1 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h1><p>This post is inspired by Andrej Karpathy&rsquo;s <a href=https://github.com/karpathy/makemore>makemore</a> series. Unfortunately, as of the time of this writing, he hasn&rsquo;t updated this series in the last 6 months. In short, andrej is teaching us how the modern LLMs came to be, by going through the series of seminal papers that led to its formation - and andrej was kind enough to walk us through the details in his video series. But it has currently left a hole in our understanding. The series jumps straight from wave nets, to transformers and GPT and it leaves a lot to be filled.</p><p>So as I go on my path to understand the history, I&rsquo;d like this to fill the gap for anyone else who might be going through the same journey.
I believe, andrej will eventually complete this series in terms of code, and when it does - it will be a far better source than this post will ever be.<br>But, if you&rsquo;re like me, and looking for a crash-course on the various papers and the core concepts. Follow along:-</p><h2 id=papers-not-be-going-through>Papers NOT be going through<a hidden class=anchor aria-hidden=true href=#papers-not-be-going-through>#</a></h2><p>The following papers are ones, that andrew has explained in a lot of detail in his make more lecture series on <a href="https://www.youtube.com/watch?v=PaCmpygFfXo">YouTube</a> , and I would recommend anyone to go through the series - as its the best explanation I&rsquo;ve seen so far.</p><p>Bigram (one character predicts the next one with a lookup table of counts)</p><p>MLP, following <a href=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>Bengio</a></p><p>CNN, following <a href=https://arxiv.org/abs/1609.03499>DeepMind WaveNet 2016</a></p><h2 id=papers-intend-to-deep-dive-into>Papers intend to deep-dive into:<a hidden class=anchor aria-hidden=true href=#papers-intend-to-deep-dive-into>#</a></h2><p>RNN , following Mikolov et al. 2010 <a href=https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf>Recurrent neural network based language model</a></p><p>Backpropagation through time , followed in Mikael Bod ́en 2001 <a href=https://axon.cs.byu.edu/~martinez/classes/678/Papers/RNN_Intro.pdf>BPTT</a></p><p>LSTM , following Graves et al. 2014 <a href=https://arxiv.org/pdf/1308.0850.pdf>Generating Sequences With Recurrent Neural Networks</a></p><p>GRU , following Kyunghyun Cho et al. 2014 <a href=https://arxiv.org/pdf/1409.1259.pdf>On the Properties of Neural Machine Translation: Encoder–Decoder</a></p><p>Batch Normalisation, following Sergey Ioffe et al. 2015 <a href=https://arxiv.org/pdf/1502.03167.pdf>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><p>Attention, following Dzmitry Bahdanau, 2015 <a href=https://arxiv.org/pdf/1409.0473.pdf>Dzmitry Bahdanau, 2015</a></p><p>Layer Normalization, following Jimmy Lei Ba, 2016 <a href=https://arxiv.org/pdf/1607.06450.pdf>Layer Normalization</a></p><p>Transformers , following Vaswani et al. 2017 <a href=https://arxiv.org/pdf/1706.03762.pdf>Attention Is All You Need</a></p><h2 id=lets-get-started>Let&rsquo;s get started<a hidden class=anchor aria-hidden=true href=#lets-get-started>#</a></h2><h3 id=rnn---recurrent-neural-networks>RNN - Recurrent Neural Networks<a hidden class=anchor aria-hidden=true href=#rnn---recurrent-neural-networks>#</a></h3><p><strong>Paper:</strong> <a href=https://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf>Mikolov et al. 2010</a></p><p><strong>Summary</strong></p><p>In effect, the paper references the MLP paper by <a href=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf>Bengio</a>, and cites the shortcoming that the context length is fixed and manually set. (The context length is the number of previous tokens that are fed into the MLP to generate the next word).</p><p>Quoting the author:-</p><blockquote><p>It is well known that humans can exploit longer context with great success. Also, cache models provide comple-<br>mentary information to neural network models, so it is natural to think about a model that would encode temporal information<br>implicitly for contexts with arbitrary lengths<br>The authors then explain how a simple recurrent neural network works</p></blockquote><p><img loading=lazy src=/image_1687079484371_0.png alt=image.png></p><p><img loading=lazy src=/image_1687079552173_0.png alt=image.png></p><p>Where</p><p>w(t) is the input word at t</p><p>s(t-1) is the state previously generated by the RNN in its last time-step</p><p>Output layer y(t) represents probability distribution of next word given previous word w(t) and context. Consequently, time needed to train optimal network increases faster than just linearly with increased amount of training data: vocabulary growth increases the input and output layer sizes, and also the optimal hidden layer size increases with more training data.</p><p>Back-propagation through time (BPTT) algorithm is used. (This is covered next)</p><p>Notable lines:</p><blockquote><p>Based on our experiments, size of hidden layer should reflect amount of training data - for large<br>amounts of data, large hidden layer is needed<br>Convergence is usually achieved after 10-20 epochs.<br>regularization of networks to penalize large weights did not provide any significant improvements.<br>Backpropagation through time , followed in Mikael Bod ́en 2001</p></blockquote><p>The key insight is around how to back-propagate through the recursion caused loop</p><p>The solution is to &ldquo;unroll&rdquo; the model T times, and then follow normal backpropation</p><p>Instead, of keeping separate weight matrix for each time-step, the weight matrix is instead shared across the unfolded layers.</p><p><img loading=lazy src=/image_1687080413026_0.png alt=image.png></p><p>Note, how weights V and U , remain the same through the unfolding process</p><p>Important quotes from the paper:</p><blockquote><p>It is important to note, however, that after error deltas have been calculated, weights<br>are folded back adding up to one big change for each weight. Obviously there is a greater<br>memory requirement (both past errors and activations need to be stored away), the larger<br>τ we choose.<br>In practice, a large τ is quite useless due to a “vanishing gradient effect” (see e.g.<br>(Bengio et al., 1994)). For each layer the error is backpropagated through the error<br>gets smaller and smaller until it diminishes completely. Some have also pointed out that<br>the instability caused by possibly ambiguous deltas (e.g. (Pollack, 1991)) may disrupt<br>convergence. An opposing result has been put forward for certain learning tasks (Bod ́en<br>et al., 1999).<br>Note that batch normalization and layer normalization were probably not present at this time.</p></blockquote><p>PyTorch</p><p><a href=https://pytorch.org/docs/stable/generated/torch.nn.RNN.html>Code Doc</a></p><p><img loading=lazy src=/image_1687081259085_0.png alt=image.png></p><p>Input:</p><p>(<em>N</em>,<em>L</em>,<em>H**in</em>​) when batch_first=True</p><p>N = Batch Size</p><p>L = Sequence Length</p><p>H_in = Hidden Layer</p><h3 id=bidirectional-rnn>Bidirectional RNN<a hidden class=anchor aria-hidden=true href=#bidirectional-rnn>#</a></h3><p>Paper: Mike et al 1997 <a href=https://deeplearning.cs.cmu.edu/F23/document/readings/Bidirectional%20Recurrent%20Neural%20Networks.pdf>paper</a></p><blockquote><p>Future input information coming up later than is usually also useful for prediction. With an RNN, this can be partially<br>achieved by delaying the output by a certain number of time frames to include future information. While delaying the output by some frames has been used successfully to improve results in a practical speech recogni-<br>tion system [12], which was also confirmed by the experiments conducted here, the optimal delay is task dependent and has to be found by the “trial and error” error method on a validation test set.<br>To overcome the limitations of a regular RNN outlined in the previous section, we propose a bidirectional recurrent<br>neural network (BRNN) that can be trained using all available input information in the past and future of a specific time frame.<br><img loading=lazy src=/image_1687353507326_0.png alt=image.png></p></blockquote><h3 id=lstm---long-short-term-memory>LSTM - Long Short-term Memory<a hidden class=anchor aria-hidden=true href=#lstm---long-short-term-memory>#</a></h3><p><strong>Paper:</strong> Graves et al. 2014 <a href=https://arxiv.org/pdf/1308.0850.pdf>Generating Sequences With Recurrent Neural Networks</a></p><p><strong>Summary</strong></p><p>Quoting the paper to best describe the problem they are addressing</p><blockquote><p>In practice however, standard RNNs are unable to<br>store information about past inputs for very long [15]. As well as diminishing<br>their ability to model long-range structure, this ‘amnesia’ makes them prone to<br>instability when generating sequences. The problem (common to all conditional<br>generative models) is that if the network’s predictions are only based on the last<br>few inputs, and these inputs were themselves predicted by the network, it has<br>little opportunity to recover from past mistakes. Having a longer memory has<br>a stabilising effect, because even if the network cannot make sense of its recent<br>history, it can look further back in the past to formulate its predictions.<br><img loading=lazy src=/image_1687273677960_0.png alt=image.png></p></blockquote><p><img loading=lazy src=/image_1687273755357_0.png alt=image.png></p><p>In my words, I understand it as follows</p><p>The forget gate, tries to find how much to forget in the next iteration. The network learns weights, such that for certain inputs x, at hidden states h and a previous cell state c[t-1] - it predicts how to forget in the next iteration</p><p>Simlarly, the input gate learns how much to store in the new cell state at t.</p><p>Combining both, the new c[t] is a F<em>c[t-1] + I</em>(WX+Wh+b)</p><p>The paper has some great examples of text generation and handwriting prediction using LSTMs, that I would encourage going through.</p><p>Paper 2 by Google: hasim et al. 2014 <a href=https://arxiv.org/pdf/1402.1128.pdf>hasim et all</a></p><blockquote><p>The recurrent connections in the<br>LSTM layer are directly from the cell output units to the cell input<br>units, input gates, output gates and forget gates. The cell output units<br>are connected to the output layer of the network.<br><img loading=lazy src=/image_1687352816805_0.png alt=image.png></p></blockquote><p>PyTorch:</p><p>Reference: <a href=https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html>https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html</a></p><h3 id=gru---gated-recurrent-neural-networks>GRU - Gated Recurrent Neural Networks<a hidden class=anchor aria-hidden=true href=#gru---gated-recurrent-neural-networks>#</a></h3><p>Paper:</p><p><a href=https://arxiv.org/pdf/1406.1078.pdf>Cho et. al 2014</a> <a href=https://arxiv.org/pdf/1406.1078.pdf>Learning Phrase representations using Encoder-Decoder</a></p><p>On the Properties of Neural Machine Translation: <a href=https://arxiv.org/pdf/1409.1259.pdf>On the Properties of Neural Machine Translation: Encoder–Decoder</a></p><a href=https://arxiv.org/pdf/1412.3555.pdf>paper</a><p><strong>From the Papers:</strong></p><blockquote><p>In addition to a novel model architecture, we also<br>propose a new type of hidden unit (f in Eq. (1))<br>that has been motivated by the LSTM unit but is<br>much simpler to compute and implement.1 Fig. 2<br>shows the graphical depiction of the proposed hidden unit.<br>We show that the neural machine translation performs<br>relatively well on short sentences without unknown words,<br>but its performance de-grades rapidly as the length of the sentence<br>and the number of unknown words increase.<br>Furthermore, we find that the pro-posed gated recursive convolutional net-<br>work learns a grammatical structure of a sentence automatically.<br><img loading=lazy src=/image_1687355590275_0.png alt=image.png></p></blockquote><p>In my words, they simplified the task to an update gate and a reset gate, instead of the complicated interactions between multiple gates in LSTMs.</p><p>Both the reset gates and update gates are a function of the input, and the hidden state at t-1</p><p><img loading=lazy src=/image_1687590382787_0.png alt=image.png></p><p><img loading=lazy src=/image_1687590390930_0.png alt=image.png></p><p>and the next h state is calculated as</p><p><img loading=lazy src=/image_1687590493510_0.png alt=image.png></p><p>As each hidden unit has separate reset and update gates, each hidden unit will learn to capture
dependencies over different time scales.<br>Those units that learn to capture short-term dependencies
will tend to have reset gates that are frequently active, but those that capture longer-term dependencies will have update gates that are mostly active.</p><h3 id=batch-normalisation>Batch Normalisation<a hidden class=anchor aria-hidden=true href=#batch-normalisation>#</a></h3><p><strong>Paper:</strong> Batch Normalisation, following Sergey Ioffe et al. 2015 <a href=https://arxiv.org/pdf/1502.03167.pdf>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p><p>TO BE CONTINUED..</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://Kshitij-Banerjee.github.io>Kshitij's Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>