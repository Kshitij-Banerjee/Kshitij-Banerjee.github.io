<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script type=text/javascript src=https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js></script><meta name=robots content="index, follow"><title>Intro to ML | Kshitij's Notes</title><meta name=keywords content="ML,AI,Andrew,Ng,Stanford"><meta name=description content="Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.
I&rsquo;ll update and make these more readable in the future (which possibly means never :D)
Lecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.
The notation “a := b” to denote an assignment operation"><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/><link crossorigin=anonymous href=/assets/css/stylesheet.bccfefac377bc340f06c260aed1bddf49a4354816d7c570d6aac75a997986c95.css integrity="sha256-vM/vrDd7w0DwbCYK7Rvd9JpDVIFtfFcNaqx1qZeYbJU=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta name=author content="Kshitij Banerjee"><meta name=description content="Kshitij Banerjee's notes..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Kshitij Banerjee","url":"https://kshitij-banerjee.github.io/"}</script><meta property="og:title" content="Intro to ML"><meta property="og:description" content="Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.
I&rsquo;ll update and make these more readable in the future (which possibly means never :D)
Lecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.
The notation “a := b” to denote an assignment operation"><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-01-20T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Intro to ML"><meta name=twitter:description content="Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.
I&rsquo;ll update and make these more readable in the future (which possibly means never :D)
Lecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.
The notation “a := b” to denote an assignment operation"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Intro to ML","item":"https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Intro to ML","name":"Intro to ML","description":"Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.\nI\u0026rsquo;ll update and make these more readable in the future (which possibly means never :D)\nLecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.\nThe notation “a := b” to denote an assignment operation","keywords":["ML","AI","Andrew","Ng","Stanford"],"articleBody":"Introduction Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.\nI’ll update and make these more readable in the future (which possibly means never :D)\nLecture Notes: https://cs229.stanford.edu/notes2022fall/main_notes.pdf Notations A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation.\nThe notation “a := b” to denote an assignment operation\nh is called a hypothesis\nThe notation “p(y(i)|x(i); θ)” indicates that this is the distribution of y(i) given x(i) and parameterized by θ.\nLecture 1 - Intro @00:40:31 Supervised learning - Has training examples, and learn model on that\n@01:08:51 - Unsupervised- cocktail party problem, ICA Independent component analysis\n@01:10:48 - Re-inforcement learning - Helicopter example\nLecture 2 - Linear Regression and Gradient Descent @00:06:24 Linear Regression\n$ h_{θ}(x) = θ_0 + θ_1x_1 + θ_2x_2 $ $ J(θ) = \\sum_{n=1}^{n} (h_θ( x^{(i)} ) − y^{(i)})^2 $ @00:21:37 - Gradient descent visualisation\n@00:17:13 Cost Function:\n$ J(θ) = \\sum_{n=1}^{n} (h_θ( x^{(i)} ) − y^{(i)})^2 $ @00:23:54 Optimisation: ø[j] = ø[j] - Learning_Rate * Partial_derivative( J(ø) )\n$ θj := θj − α \\dfrac{∂J(θ)} {∂θj} $ @00:45:00 Stochastic Gradient Descent = Do for each examples\nLecture 3 - Logistic Regression @00:05:54 Locally Weighted Regression\n+ Keep the examples in memory, so takes more memory + @00:12:18 Weight based on nearness to prediction X + W[_i_] = Exp ( -(X[_i_] - X )^2 / 2 ) :- Is e^0=1 when close to the prediction, otherwise 0 if far + J(ø) = SUM( W[_i_] * (y[_i_] - ø * x[_i_])^2 ) @00:46:43 Logistic Regression Sigmoid Function G(x) : + $ H(x) = G( ø^{T} * X ) $ + Where H(x) now gives the probability of the value being 1. + @00:54:20 + Taking Log of P(y|x;ø), we get @00:57:50 + $ Log Likelihood = Log(y) h(x) + Log(1-y) (1-h(x)) $ + @00:58:38 Choose ø that maximises the log likelyhood with Gr.Ascent + @01:07:39 Newton’s method :\nGet the tangent, and use tangent X boundary as next iteration X {{video https://www.youtube.com/watch?v=het9HFqo1TQ\u0026list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU\u0026index=3}}\nLecture 4 - Generalized Linear Model Generalized Linear Models\n@00:05:22\n$ ø[j] = ø[j] - Learning Rate * (y[i] - H_{ø}(x[i]))* x[j] $ Adding Learning_Rate * x[j] =\u003e Moves ø in the direction of x[j]\n+ @00:43:14 Explaining the model + ø - is the learnable parameter ø^{T} X = H(x) is the prediction Produces an exponential family, parameterized by b, a, and T\n@01:14:09 Softmax Lecture 5 - GDA \u0026 Naive Bayes @00:05:17 Generative Learning Algorithms, try to find the P(X|Y) instead of P(Y|X)\nIt also learns P(Y)- Called class prior, before you see anything, what is the chance\nExample:\nIf building classifier for elephants vs dogs. First, looking at elephants, we can build a model of what elephants look like. Then, looking at dogs, we can build a separate model of what dogs look like. Finally, to classify a new animal, we can match the new animal against the elephant model, and match it against the dog model, to see whether the new animal looks more like the elephants or more like the dogs we had seen in the training set. @00:10:09 GDA - Gaussian Discriminant Analysis\nE[z] = Mean\nCov[z] = E[ (z-Mean)(z-Mean)^{T}]\n@00:12:35 Visualization of mean and covariance\nGDA Model: @00:19:33\n+ $ P(Y) = ø^{y} (1 - ø^{1-y}) $ @00:45:16 @00:25:52 Naive Bayes @01:08:08\nGDA vs Logistic Regression @00:53:00\nGDA assumes gaussian, logistic is more general, GDA will do better if assumption is true Lecture 6 - Support Vector Machines @00:47:59 - How SVN finds non-linear decision boundaries\n$ H(X) = G( W^{T}X + b ) $ Functional Margin\n$ J[i] = y[i] * ( W^{T}X + b ) $ Geometric Margin\n$ \\frac{Functional Margin} {||W||} $ Lecture 7 - Kernels Trick @00:15:20 - ø is a linear combination of the examples\n@00:29:21 kernel - trick\nWrite algo in terms of dot-product of Xi, Xj\nMake the features higher dimentional using a kernal function\nSolve and move back to original dimensions\n@00:47:20 visualisation of higher dimension\nRegularisation @01:05:00\n@01:06:40 - Adding C to teh optimisation of W Lecture 8 - Data Splits and Fitting @00:07:41 Fitting / underfit / overfit\nRegularisation: @00:14:30\nAdding Lambda * ||ø||^{2}\nIncentive term for algo to make ø parameters smaller\n@00:38:05 Model complexity tuning to get “just right” @01:05:20 - K-Fold CV\n@01:19:40 Feature selection - adding features greedily\nLecture 9 - Estimation Errors \u0026 ERM Lecture 10 - Decision Trees, Random Forests, Boosting @00:03:45 Decision tree loss function choice\nWe could find a non-linear boundary via higher dimension SVM’s but decision tree would be better here\nGreedy top-down recursive partitioning\n@00:10:54\n$ Loss_{missclassification} = 1 - P^{∆}_{c} $ Where P^{^}_{c} = most common class; and hence 1- becomes miss-classification\nCross entropy loss- @00:16:02, work much better\n@00:31:10 Regression trees - Real values instead of classification\n@00:32:20 - On the leaf, produce the mean of the values in that leaf + @00:35:30 Categorical Features can also be used easily\nEach category makes 2^{z} categories, so need to have less of them @00:38:31 Regularisation | stop at min leaf size, max depth, max nodes etc.\nchange of loss is not great, because some variables if dependent, then first question even if suboptimal is needed for next to be optimal @01:00:27 Bagging - Bootstrapped Aggregation - Avg. of multiple samples\n@01:04:50 Get different samples from training set and train on each and average for outputs\n$ Model = Sum( G_{m}(x) ) /div M $ @01:13:44 Random Forests - At each split, only consider some subset features\n@01:15:39 Boosting - Increase weight of errored samples iteratively\nLecture 11 - Neural Networks Nice recap of logistic regression: @00:03:25\n@00:08:30 - Finding W(weights) and B(bias) by definition loss function\nLoss Function: yLog(y^{y}) + (1-y)log(1-y^{^})\nuse gradient descent function\n@00:12:12 - Neuron = Linear(Wx+b) + Activation(Sigmoid)\n@00:17:08 - Multi classification by layers, where multiple can exist\n@00:25:36 - Softmax, Multi-class, only one exists, sum of output is 1 so a prob over all classes\n@00:35:47 Cross entropy loss function for softmax @00:41:43 - Neural networks intro\n@01:11:17 - Optimising function\n+ y^ is the final output frm NN + @01:13:00 backward propagation Lecture 12 - Back propagation and NN Optimisations @00:06:08 - Loss function and partial derivates for back propagation\n+ @00:17:38 Finally, derivative of cost function comes out as + $ \\frac {\\partial LossFn} {\\partial W^{[3]}} = -\\frac{1}{m} \\sum_{n=1}^{m} (y^{i} - a^{[3]})(a^{[2]})^{T} $ Then we continue to previous layers @00:34:37 Improving the network\nSigmoid has problems because the derivate is very small with large X,\nRelU, has better derivates and helps with the backpropagation\nNormalising Inputs @00:48:20\nif Xs are big, then Wx + B is big, So Z is big, so derivates on sigmoid are very saturated\nNormalise (by reducing X by the means), makes the X smaller, but still variance could be high in Y @00:49:39\nDividing X by Sigma, it becomes more homogeneous around the axis\n+ @01:05:26 Optimisations - Mini batch Gr.Desc + @01:11:48 - Momentum Algorithm + Looks at the past updates, and considers those for future iterations + Lecture 13 - Debugging ML models and Error Analysis @00:09:56 Bias vs Variance Diagnostics\nUnderstand how much of the problem comes from bias vs variance\nHigh Variance model problems\n+ Training error, also increases as training examples increase, because the perfect fit becomes harder with more training data + If there is a huge gap between training and test error, then the model has a high variance + Because, the model is able to fit the training set well, i.e: overfitting (like a higher degree polynomial) + High Bias model problems + Even on training set, the error is high - that means the model is not fitting the data well @00:27:00 Optimisation Algorithm\nIf another algorithm is able to get better results, then possibly the current algorithm is not converging\nMaybe a different cost function is needed Lecture 14 - Expectation Maximization Algorithms @00:01:37 Unsupervised Learning\n@00:02:33 K-means clustering\nInitialise cluster centroids randomly, usually take k random training sets for k clusters Color the point, based on nearest centroid $ C^{j} = arg min ||x^{i} - u_{j}||^{2} $ Update centroid with new values @00:17:26 Anomaly Detection\nTo detect anomaly, first find the Probability density function of the given data P(x)\nThen, if P(x) on the new sample is close to 0, then the new sample has an anomaly\n@00:24:49 Expectation maximisation Algorithm\nAllows us to find a joint distribution that models the data without knowing anything about the distribution of the data.\n$ p(x(i), z(i)) = p(x(i)|z(i))p(z(i)) $ + @00:31:21 2 steps in EM algorithms + Guess the value, by setting W[j] via bayes rule + + EM implements a softer way to assign to classes, and updates with probability instead of hard assigment like K-means did @00:48:17 Jensen’s Inequality Lecture 15 - EM + Factor Analysis Factory Analysis:\nDefinitions online:\n@00:17:33 Factor analysis (FA)\nallows us to simplify a set of complex variables or items using statistical procedures to explore the underlying dimensions that explain the relationships between the multiple variables/items. Factor analysis is one of the unsupervised machine learning algorithms which is used for dimensionality reduction. This algorithm creates factors from the observed variables to represent the common variance i.e. variance due to correlation among the observed variables.\nLecture 16 - Independent Component Analysis \u0026 Re-enforcement Learning Why do we need non-gaussian distributions in ICA\n@00:02:59 It’s important that the training set doesn’t follow a gaussian density for ICA to work\n@00:20:00 - So, we take a sigmoid function as the choice of our cumulative density function, as its derivate is non-gaussian and modes normal speech beter - as it has fatter edges, i.e more outliers wrt to the mean.\n@00:51:40 Re-inforcement Learning\n@00:53:40 Reward Function = R(S)\n@00:54:04 = Credit assign problem : how do we attribute previous actions to the result in reward functions?\n@00:56:10 Markov Decision Process\n@00:56:49 S - set of all states\n@00:57:04 - A set of actions\n@00:57:21 - P_{sa} - Taking action a on s, what is the probability of getting to next state\n@00:58:22 - R = Reward Function\n@00:59:10 - Example illustrating MDP with maze problem\n@01:07:45 - Discount factor\n@01:11:52 Policy / Controller\nLecture 17 - MDPs \u0026 Value/Policy Iteration @00:06:50 - Value function\nV_{pi}s = E[R(s_0) + …)]\n@00:08:29 - What is the expected total payoff, if you start at S and execute pi\n@00:09:20 - Explanation of Value function over the maze example\n@00:11:20 - Bellman Equations\n@00:12:07 Intuition behind it\nImmediate Reward of R(s_{0}) + DiscountFactor * [R(s_{1}) + DF*…]\n@00:19:30 DP formulation = V_{s0} = R(S_{0}) + DiscountFactor * V_{s1}\ns’ is used to denote the next step\n@00:21:05 Illustration of V over the maze\n@00:22:50 - Matrix definition of Value function calculation\n@00:25:20 - Optimal Value Function V^{*}\n[:span] ls-type:: annotation hl-page:: 180 hl-color:: yellow @00:30:09 - PI^{*}(S) = Optimal Policy\n[:span] ls-type:: annotation hl-page:: 180 hl-color:: red @00:35:31 - Value Iteration\n+ @00:42:30 - Backup operator + takes a current estimate of the value function, and maps it to a new estimate.\n+ @00:43:30 - Converges to V* + @00:45:30 - Illustration of the iteration algorithms + @00:50:34 - Policy Iteration + @00:55:10 - Value vs Policy Iteration + Small states - Policy iteration is fine, but large problems the Value iteration is faster + @00:58:40 - How to handle Unknown P_{sa} + Workflow is to find P from data, + Take a random policy and let the problem run, and see how much probability of moving to other directions when taken P for each decision as simple probability + @01:03:40 workflow + Take actions randomly to get P_{sa} + Solve Bellmans equations using value iteration to get V + Update Policy(S) with argmax of V over all decisions + @01:08:20 Exploration vs Exploitation problem - Local optimas + @01:14:19 Lecture 18 - Continuous/Infinite State MDP \u0026 Model Simulation\nValue Function Approximation\nFind a model / simulator that transitions s_t -\u003e s_t+1\nFor example, one may choose to learn a linear model of the form s_{t+1} = A s_{t} + B a_{t},\nTypically a gaussian error is added to make it a stochastic model\n@00:47:04 Fitted Value Iteration\nThe main idea of fitted value iteration is that we are going to approximately carry out value iteration step, over a finite sample of states s(1), . . . , s(n).\nSpecifically, we will use a supervised learning algorithm, to approximate the value function as a linear or non-linear function of the states: @01:01:50 - illustration\n@01:16:27 - Example of how using simulator and\nLecture 19 - Reward Model \u0026 Linear Dynamical System\n@00:06:40 - State action Rewards, makes reward function depend on both state and actions\n@00:11:42 Finite Horizon MDP, replaces Discount factor with Finite horizon time, and it assumes a finite number of steps taken\n@00:16:40 - Non stationary transition probabilities\n@00:23:30 Value iteration in non stationary problems\n@00:30:33 Illustration of the algorithm\n@00:32:30 - Linear Quadratic Regulation\nLecture 20 - RL Debugging\n","wordCount":"2185","inLanguage":"en","datePublished":"2023-01-20T00:00:00Z","dateModified":"2023-02-05T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2023/01/20/intro-to-ml/"},"publisher":{"@type":"Organization","name":"Kshitij's Notes","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="Kshitij's Notes (Alt + H)">Kshitij's Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Intro to ML</h1><div class=post-meta><span title='2023-01-20 00:00:00 +0000 UTC'>January 20, 2023</span></div></header><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><ul><li><p>Note: These are my rough notes, which are auto-synced from my private LogSeq, and is a WIP.</p></li><li><p>I&rsquo;ll update and make these more readable in the future (which possibly means never :D)</p></li></ul><h3 id=lecture-notes>Lecture Notes:<a hidden class=anchor aria-hidden=true href=#lecture-notes>#</a></h3><ul><li><a href=https://cs229.stanford.edu/notes2022fall/main_notes.pdf>https://cs229.stanford.edu/notes2022fall/main_notes.pdf</a></li></ul><p><img loading=lazy src=/andrewngmlcoursenotes_1674652188746_0.pdf alt=AndrewNgMLCourseNotes.pdf></p><h3 id=notations>Notations<a hidden class=anchor aria-hidden=true href=#notations>#</a></h3><ul><li><p>A pair (x^{(i)}, y^{(i)}) is called a training example, the superscript “(i)” in the
notation is simply an index into the training set, and has nothing to do with
exponentiation.</p></li><li><p>The notation “a := b” to denote an assignment operation</p></li><li><p><em>h</em> is called a hypothesis</p></li><li><p>The notation “p(y(i)|x(i); θ)” indicates that this is the distribution of y(i)
given x(i) and parameterized by θ.</p></li></ul><h4 id=lecture-1---intro>Lecture 1 - Intro<a hidden class=anchor aria-hidden=true href=#lecture-1---intro>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/jGwO_UgTS7I style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:40:31 Supervised learning - Has training examples, and learn model on that</p></li><li><p>@01:08:51 - Unsupervised- cocktail party problem, ICA Independent component analysis</p></li><li><p>@01:10:48 - Re-inforcement learning - Helicopter example</p></li></ul><h4 id=lecture-2---linear-regression-and-gradient-descent>Lecture 2 - Linear Regression and Gradient Descent<a hidden class=anchor aria-hidden=true href=#lecture-2---linear-regression-and-gradient-descent>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/4b4MUYve_U8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:06:24 Linear Regression</p><ul><li>$ h_{θ}(x) = θ_0 + θ_1x_1 + θ_2x_2 $</li><li>$ J(θ) = \sum_{n=1}^{n}
(h_θ( x^{(i)} ) − y^{(i)})^2 $</li></ul></li><li><p>@00:21:37 - Gradient descent visualisation</p><ul><li><p>@00:17:13 Cost Function:</p><ul><li>$ J(θ) = \sum_{n=1}^{n}
(h_θ( x^{(i)} ) − y^{(i)})^2 $</li></ul></li><li><p>@00:23:54 Optimisation: ø[<em>j</em>] = ø[<em>j</em>] - Learning_Rate * Partial_derivative( J(ø) )</p><ul><li>$ θj := θj − α \dfrac{∂J(θ)} {∂θj} $</li></ul></li><li><p>@00:45:00 Stochastic Gradient Descent = Do for each examples</p></li></ul></li></ul><p><img loading=lazy src=/image_1674464592581_0.png alt=image.png></p><h4 id=lecture-3---logistic-regression>Lecture 3 - Logistic Regression<a hidden class=anchor aria-hidden=true href=#lecture-3---logistic-regression>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/het9HFqo1TQ style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:05:54 Locally Weighted Regression</p></li></ul><p><img loading=lazy src=/image_1674465035497_0.png alt=image.png></p><pre><code>+ Keep the examples in memory, so takes more memory

+ @00:12:18 Weight based on nearness to prediction X

  + W[_i_] = Exp (  -(X[_i_] - X )^2 / 2 ) :- Is e^0=1 when close to the prediction, otherwise 0 if far

  + J(ø) = SUM( W[_i_] * (y[_i_] - ø * x[_i_])^2 )
</code></pre><ul><li>@00:46:43 Logistic Regression</li></ul><p>Sigmoid Function G(x) : <img loading=lazy src=/image_1674465233772_0.png alt=image.png></p><pre><code>+ $ H(x) = G( ø^{T} * X ) $



  + Where H(x) now gives the probability of the value being 1.

+ @00:54:20
</code></pre><p><img loading=lazy src=/image_1674532901491_0.png alt=image.png></p><pre><code>  + Taking Log of P(y|x;ø), we get @00:57:50

  + $ Log Likelihood = Log(y) h(x) + Log(1-y) (1-h(x)) $



  + @00:58:38 Choose ø that maximises the log likelyhood with Gr.Ascent

  + 
</code></pre><ul><li><p>@01:07:39 Newton&rsquo;s method :</p><ul><li>Get the tangent, and use tangent X boundary as next iteration X</li></ul></li><li><p>{{video <a href="https://www.youtube.com/watch?v=het9HFqo1TQ&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;index=3%7D%7D">https://www.youtube.com/watch?v=het9HFqo1TQ&amp;list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&amp;index=3}}</a></p></li></ul><h4 id=lecture-4---generalized-linear-model>Lecture 4 - Generalized Linear Model<a hidden class=anchor aria-hidden=true href=#lecture-4---generalized-linear-model>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/iZTeva0WSTQ style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>Generalized Linear Models</p><ul><li><p>@00:05:22</p></li><li>$ ø[j] = ø[j] - Learning Rate * (y[i] - H_{ø}(x[i]))* x[j] $</li></ul></li></ul><p><img loading=lazy src=/image_1674465875349_0.png alt=image.png>
Adding Learning_Rate * x[j] => Moves ø in the direction of x[j]</p><pre><code>+ @00:43:14  Explaining the model
</code></pre><p><img loading=lazy src=/image_1674466215288_0.png alt=image.png></p><pre><code>+ ø - is the learnable parameter
</code></pre><p>ø^{T} X = H(x) is the prediction
Produces an exponential family, parameterized by b, a, and T</p><ul><li>@01:14:09 Softmax</li></ul><h4 id=lecture-5---gda--naive-bayes>Lecture 5 - GDA & Naive Bayes<a hidden class=anchor aria-hidden=true href=#lecture-5---gda--naive-bayes>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/nt63k3bfXS0 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:05:17 Generative Learning Algorithms, try to find the P(X|Y) instead of P(Y|X)</p><ul><li><p>It also learns P(Y)- Called class prior, before you see anything, what is the chance</p></li><li><p>Example:</p><ul><li>If building classifier for elephants vs dogs. First, looking at elephants, we can build a
model of what elephants look like. Then, looking at dogs, we can build a
separate model of what dogs look like. Finally, to classify a new animal, we
can match the new animal against the elephant model, and match it against
the dog model, to see whether the new animal looks more like the elephants
or more like the dogs we had seen in the training set.</li></ul></li></ul></li><li><p>@00:10:09 GDA - Gaussian Discriminant Analysis</p><ul><li><p>E[z] = Mean</p></li><li><p>Cov[z] = E[ (z-Mean)(z-Mean)^{T}]</p></li><li><p>@00:12:35 Visualization of mean and covariance</p></li><li><p>GDA Model: @00:19:33</p></li></ul></li></ul><p><img loading=lazy src=/image_1674466717189_0.png alt=image.png></p><pre><code>  + $ P(Y) = ø^{y} (1 - ø^{1-y}) $


</code></pre><p>@00:45:16 <img loading=lazy src=/image_1674466803649_0.png alt=image.png></p><ul><li><p>@00:25:52 Naive Bayes @01:08:08</p></li><li><p>GDA vs Logistic Regression @00:53:00</p><ul><li>GDA assumes gaussian, logistic is more general, GDA will do better if assumption is true</li></ul></li></ul><h4 id=lecture-6---support-vector-machines>Lecture 6 - Support Vector Machines<a hidden class=anchor aria-hidden=true href=#lecture-6---support-vector-machines>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/lDwow4aOrtg style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:47:59 - How SVN finds non-linear decision boundaries</p></li><li>$ H(X) = G( W^{T}X + b ) $</li><li><p>Functional Margin</p><ul><li>$ J[i] = y[i] * ( W^{T}X + b ) $</li></ul></li><li><p>Geometric Margin</p><ul><li>$ \frac{Functional Margin} {||W||} $</li></ul></li></ul><p><img loading=lazy src=/image_1674467376775_0.png alt=image.png></p><h4 id=lecture-7---kernels-trick>Lecture 7 - Kernels Trick<a hidden class=anchor aria-hidden=true href=#lecture-7---kernels-trick>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/8NYoQiRANpg style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:15:20 - ø is a linear combination of the examples</p></li><li><p>@00:29:21 kernel - trick</p><ul><li><p>Write algo in terms of dot-product of Xi, Xj</p></li><li><p>Make the features higher dimentional using a kernal function</p></li><li><p>Solve and move back to original dimensions</p></li><li><p>@00:47:20 visualisation of higher dimension</p></li></ul></li><li><p>Regularisation @01:05:00</p><ul><li>@01:06:40 - Adding C to teh optimisation of W</li></ul></li></ul><h4 id=lecture-8---data-splits-and-fitting>Lecture 8 - Data Splits and Fitting<a hidden class=anchor aria-hidden=true href=#lecture-8---data-splits-and-fitting>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/rjbkWSTjHzM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:07:41 Fitting / underfit / overfit</p></li><li><p>Regularisation: @00:14:30</p><ul><li><p>Adding Lambda * ||ø||^{2}</p></li><li><p>Incentive term for algo to make ø parameters smaller</p></li></ul></li></ul><p><img loading=lazy src=/image_1674470357051_0.png alt=image.png></p><ul><li>@00:38:05 Model complexity tuning to get &ldquo;just right&rdquo;</li></ul><p><img loading=lazy src=/image_1674470485278_0.png alt=image.png></p><ul><li><p>@01:05:20 - K-Fold CV</p></li><li><p>@01:19:40 Feature selection - adding features greedily</p></li><li></li><li></li></ul><h4 id=lecture-9---estimation-errors--erm>Lecture 9 - Estimation Errors & ERM<a hidden class=anchor aria-hidden=true href=#lecture-9---estimation-errors--erm>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/iVOxMcumR4A style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li></ul><h4 id=lecture-10---decision-trees-random-forests-boosting>Lecture 10 - Decision Trees, Random Forests, Boosting<a hidden class=anchor aria-hidden=true href=#lecture-10---decision-trees-random-forests-boosting>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/wr9gUr-eWdA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:03:45 Decision tree loss function choice</p><ul><li><p>We could find a non-linear boundary via higher dimension SVM&rsquo;s but decision tree would be better here</p></li><li><p>Greedy top-down recursive partitioning</p></li><li><p>@00:10:54</p><ul><li>$ Loss_{missclassification} = 1 - P^{∆}_{c} $</li><li><p>Where P^{^}_{c} = most common class; and hence 1- becomes miss-classification</p></li></ul></li><li><p>Cross entropy loss- @00:16:02, work much better</p></li></ul></li></ul><p><img loading=lazy src=/image_1674471183939_0.png alt=image.png></p><ul><li><p>@00:31:10 Regression trees - Real values instead of classification</p><ul><li>@00:32:20 - On the leaf, produce the mean of the values in that leaf</li></ul></li></ul><p><img loading=lazy src=/image_1674471735082_0.png alt=image.png></p><pre><code>+ 
</code></pre><ul><li><p>@00:35:30 Categorical Features can also be used easily</p><ul><li>Each category makes 2^{z} categories, so need to have less of them</li></ul></li><li><p>@00:38:31 Regularisation | stop at min leaf size, max depth, max nodes etc.</p><ul><li>change of loss is not great, because some variables if dependent, then first question even if suboptimal is needed for next to be optimal</li></ul></li><li><p>@01:00:27 Bagging - Bootstrapped Aggregation - Avg. of multiple samples</p><ul><li><p>@01:04:50 Get different samples from training set and train on each and average for outputs</p></li><li>$ Model = Sum( G_{m}(x) ) /div M $</li></ul></li><li><p>@01:13:44 Random Forests - At each split, only consider some subset features</p></li><li><p>@01:15:39 Boosting - Increase weight of errored samples iteratively</p></li></ul><p><img loading=lazy src=/image_1674474275037_0.png alt=image.png></p><h4 id=lecture-11---neural-networks>Lecture 11 - Neural Networks<a hidden class=anchor aria-hidden=true href=#lecture-11---neural-networks>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/MfIjxPh6Pys style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>Nice recap of logistic regression: @00:03:25</p></li><li><p>@00:08:30 - Finding W(weights) and B(bias) by definition loss function</p><ul><li><p>Loss Function: yLog(y^{y}) + (1-y)log(1-y^{^})</p></li><li><p>use gradient descent function</p></li><li><p>@00:12:12 - Neuron = Linear(Wx+b) + Activation(Sigmoid)</p></li></ul></li><li><p>@00:17:08 - Multi classification by layers, where multiple can exist</p></li></ul><p><img loading=lazy src=/image_1674475057522_0.png alt=image.png></p><ul><li><p>@00:25:36 - Softmax, Multi-class, only one exists, sum of output is 1 so a prob over all classes</p><ul><li>@00:35:47 Cross entropy loss function for softmax</li></ul></li></ul><p><img loading=lazy src=/image_1674475728325_0.png alt=image.png></p><ul><li><p>@00:41:43 - Neural networks intro</p></li><li><p>@01:11:17 - Optimising function</p></li></ul><p><img loading=lazy src=/image_1674533245743_0.png alt=image.png></p><pre><code>+ y^ is the final output frm NN

+ @01:13:00  backward propagation
</code></pre><ul><li></li></ul><h4 id=lecture-12---back-propagation-and-nn-optimisations>Lecture 12 - Back propagation and NN Optimisations<a hidden class=anchor aria-hidden=true href=#lecture-12---back-propagation-and-nn-optimisations>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/zUazLXZZA2U style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:06:08 - Loss function and partial derivates for back propagation</p></li></ul><p><img loading=lazy src=/image_1674556024293_0.png alt=image.png></p><pre><code>+ @00:17:38 Finally, derivative of cost function comes out as

+ $ \frac {\partial LossFn} {\partial W^{[3]}} = -\frac{1}{m} \sum_{n=1}^{m} (y^{i} - a^{[3]})(a^{[2]})^{T} $


</code></pre><p>Then we continue to previous layers <img loading=lazy src=/image_1674560524973_0.png alt=image.png></p><ul><li><p>@00:34:37 Improving the network</p><ul><li><p>Sigmoid has problems because the derivate is very small with large X,</p></li><li><p>RelU, has better derivates and helps with the backpropagation</p></li><li><p>Normalising Inputs @00:48:20</p><ul><li><p>if Xs are big, then Wx + B is big, So Z is big, so derivates on sigmoid are very saturated</p></li><li><p>Normalise (by reducing X by the means), makes the X smaller, but still variance could be high in Y @00:49:39</p></li><li><p>Dividing X by Sigma, it becomes more homogeneous around the axis</p></li></ul></li></ul></li></ul><p><img loading=lazy src=/image_1674561947221_0.png alt=image.png></p><pre><code>+ @01:05:26 Optimisations - Mini batch Gr.Desc

+ @01:11:48 - Momentum Algorithm

  + Looks at the past updates, and considers those for future iterations
</code></pre><p><img loading=lazy src=/image_1674564963664_0.png alt=image.png></p><pre><code>+ 
</code></pre><h4 id=lecture-13---debugging-ml-models-and-error-analysis>Lecture 13 - Debugging ML models and Error Analysis<a hidden class=anchor aria-hidden=true href=#lecture-13---debugging-ml-models-and-error-analysis>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/ORrStCArmP4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:09:56 Bias vs Variance Diagnostics</p><ul><li><p>Understand how much of the problem comes from bias vs variance</p></li><li><p>High Variance model problems</p></li></ul></li></ul><p><img loading=lazy src=/image_1674565738786_0.png alt=image.png></p><pre><code>  + Training error, also increases as training examples increase, because the perfect fit becomes harder with more training data

  + If there is a huge gap between training and test error, then the model has a high variance

    + Because, the model is able to fit the training set well, i.e: overfitting (like a higher degree polynomial)

+ High Bias model problems
</code></pre><p><img loading=lazy src=/image_1674565874183_0.png alt=image.png></p><pre><code>  + Even on training set, the error is high - that means the model is not fitting the data well
</code></pre><ul><li><p>@00:27:00 Optimisation Algorithm</p><ul><li><p>If another algorithm is able to get better results, then possibly the current algorithm is not converging</p><ul><li>Maybe a different cost function is needed</li></ul></li><li></li></ul></li><li></li></ul><h4 id=lecture-14---expectation-maximization-algorithms>Lecture 14 - Expectation Maximization Algorithms<a hidden class=anchor aria-hidden=true href=#lecture-14---expectation-maximization-algorithms>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/rVfZHWTwXSA style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>@00:01:37 Unsupervised Learning</p><ul><li><p>@00:02:33 K-means clustering</p><ul><li><ol><li>Initialise cluster centroids randomly, usually take k random training sets for k clusters</li></ol></li></ul></li></ul></li></ul><ol start=2><li><p>Color the point, based on nearest centroid
$ C^{j} = arg min ||x^{i} - u_{j}||^{2} $</p><ul><li><ol start=3><li>Update centroid with new values</li></ol></li></ul></li></ol><ul><li><p>@00:17:26 Anomaly Detection</p><ul><li><p>To detect anomaly, first find the Probability density function of the given data P(x)</p></li><li><p>Then, if P(x) on the new sample is close to 0, then the new sample has an anomaly</p></li></ul></li><li><p>@00:24:49 Expectation maximisation Algorithm</p><ul><li><p>Allows us to find a joint distribution that models the data without knowing anything about the distribution of the data.</p></li><li>$ p(x(i), z(i)) = p(x(i)|z(i))p(z(i)) $</li><li></li></ul></li></ul><pre><code>+ @00:31:21 2 steps in EM algorithms

  + Guess the value, by setting W[j] via bayes rule

  + 

+ EM implements a softer way to assign to classes, and updates with probability instead of hard assigment like K-means did
</code></pre><ul><li>@00:48:17 Jensen&rsquo;s Inequality</li></ul><h4 id=lecture-15---em--factor-analysis>Lecture 15 - EM + Factor Analysis<a hidden class=anchor aria-hidden=true href=#lecture-15---em--factor-analysis>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/tw6cmL5STuY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li><p>Factory Analysis:</p><ul><li><p>Definitions online:</p><ul><li><p>@00:17:33 Factor analysis (FA)</p><ul><li>allows us to simplify a set of complex variables or items using statistical procedures to explore the underlying dimensions that explain the relationships between the multiple variables/items.</li></ul></li><li><p>Factor analysis is <strong>one of the unsupervised machine learning algorithms which is used for dimensionality reduction</strong>. This algorithm creates factors from the observed variables to represent the common variance i.e. variance due to correlation among the observed variables.</p></li></ul></li></ul></li></ul><h4 id=lecture-16---independent-component-analysis--re-enforcement-learning>Lecture 16 - Independent Component Analysis & Re-enforcement Learning<a hidden class=anchor aria-hidden=true href=#lecture-16---independent-component-analysis--re-enforcement-learning>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/YQA9lLdLig8 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><ul><li><p>Why do we need non-gaussian distributions in ICA</p><ul><li><p>@00:02:59 It&rsquo;s important that the training set doesn&rsquo;t follow a gaussian density for ICA to work</p></li><li><p>@00:20:00 - So, we take a sigmoid function as the choice of our cumulative density function, as its derivate is non-gaussian and modes normal speech beter - as it has fatter edges, i.e more outliers wrt to the mean.</p></li></ul></li><li><p>@00:51:40 Re-inforcement Learning</p><ul><li><p>@00:53:40 Reward Function = R(S)</p></li><li><p>@00:54:04 = Credit assign problem : how do we attribute previous actions to the result in reward functions?</p></li></ul></li><li><p>@00:56:10 Markov Decision Process</p><ul><li><p>@00:56:49 S - set of all states</p></li><li><p>@00:57:04 - A set of actions</p></li><li><p>@00:57:21 - P_{sa} - Taking action a on s, what is the probability of getting to next state</p></li><li><p>@00:58:22 - R = Reward Function</p></li><li><p>@00:59:10 - Example illustrating MDP with maze problem</p></li><li><p>@01:07:45 - Discount factor</p></li><li><p>@01:11:52 Policy / Controller</p></li></ul></li></ul></li></ul><h4 id=lecture-17---mdps--valuepolicy-iteration>Lecture 17 - MDPs & Value/Policy Iteration<a hidden class=anchor aria-hidden=true href=#lecture-17---mdps--valuepolicy-iteration>#</a></h4><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/d5gaWTo6kDM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><ul><li><p>@00:06:50 - Value function</p><ul><li><p>V_{pi}s = E[R(s_0) + &mldr;)]</p></li><li><p>@00:08:29 - What is the expected total payoff, if you start at S and execute <em>pi</em></p></li><li><p>@00:09:20 - Explanation of Value function over the maze example</p></li></ul></li><li><p>@00:11:20 - Bellman Equations</p><ul><li><p>@00:12:07 Intuition behind it</p><ul><li><p>Immediate Reward of R(s_{0}) + DiscountFactor * [R(s_{1}) + DF*&mldr;]</p></li><li><p>@00:19:30 DP formulation = V_{s0} = R(S_{0}) + DiscountFactor * V_{s1}</p></li><li><p>s&rsquo; is used to denote the next step</p></li><li><p>@00:21:05 Illustration of V over the maze</p></li><li><p>@00:22:50 - Matrix definition of Value function calculation</p></li></ul></li><li><p>@00:25:20 - Optimal Value Function V^{*}</p><ul><li>[:span]
ls-type:: annotation
hl-page:: 180
hl-color:: yellow</li></ul></li><li><p>@00:30:09 - PI^{*}(S) = Optimal Policy</p><ul><li>[:span]
ls-type:: annotation
hl-page:: 180
hl-color:: red</li></ul></li></ul></li><li><p>@00:35:31 - Value Iteration</p></li></ul></li></ul><p><img loading=lazy src=/image_1675524729839_0.png alt=image.png></p><pre><code>  + @00:42:30 - Backup operator

    + takes a current estimate of
</code></pre><p>the value function, and maps it to a new estimate.</p><pre><code>  + @00:43:30  - Converges to V*

  + @00:45:30 - Illustration of the iteration algorithms

+ @00:50:34 - Policy Iteration
</code></pre><p><img loading=lazy src=/image_1675524850899_0.png alt=image.png></p><pre><code>+ @00:55:10 - Value vs Policy Iteration

  + Small states - Policy iteration is fine, but large problems the Value iteration is faster

+ @00:58:40 - How to handle Unknown P_{sa}

  + Workflow is to find P from data,

  + Take a random policy and let the problem run, and see how much probability of moving to other directions when taken P for each decision as simple probability

+ @01:03:40  workflow

  + Take actions randomly to get P_{sa}

  + Solve Bellmans equations using value iteration to get V

  + Update Policy(S)  with argmax of V over all decisions

+ @01:08:20 Exploration vs Exploitation problem - Local optimas

  + @01:14:19
</code></pre><p>Lecture 18 - Continuous/Infinite State MDP & Model Simulation</p><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/QFu5nuc-S0s style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><ul><li><p>Value Function Approximation</p><ul><li><p>Find a model / simulator that transitions s_t -> s_t+1</p><ul><li><p>For example, one may choose to learn a linear model of the form
s_{t+1} = A s_{t} + B a_{t},</p></li><li><p>Typically a gaussian error is added to make it a stochastic model</p></li></ul></li></ul></li><li><p>@00:47:04 Fitted Value Iteration</p><ul><li><p>The main idea of fitted value iteration is that we are going to approximately carry out value iteration step, over a finite sample of states s(1), . . . , s(n).</p><ul><li>Specifically, we will use a supervised learning algorithm, to approximate the value function as a linear or non-linear function of the states:</li></ul></li><li><p>@01:01:50 - illustration</p></li><li><p>@01:16:27 - Example of how using simulator and</p></li></ul></li><li></li></ul></li></ul><p>Lecture 19 - Reward Model & Linear Dynamical System</p><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/0rt2CsEQv6U style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><ul><li><p>@00:06:40 - State action Rewards, makes reward function depend on both state and actions</p></li><li><p>@00:11:42 Finite Horizon MDP, replaces Discount factor with Finite horizon time, and it assumes a finite number of steps taken</p></li><li><p>@00:16:40 - Non stationary transition probabilities</p></li><li><p>@00:23:30 Value iteration in non stationary problems</p></li><li><p>@00:30:33 Illustration of the algorithm</p></li><li><p>@00:32:30 - Linear Quadratic Regulation</p></li><li></li></ul></li></ul><p>Lecture 20 - RL Debugging</p><ul><li><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/pLhPQynL0tY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></li><li></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/ml/>ML</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/andrew/>Andrew</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ng/>Ng</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/stanford/>Stanford</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://Kshitij-Banerjee.github.io>Kshitij's Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>