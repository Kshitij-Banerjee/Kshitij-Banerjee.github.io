<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><script type=text/javascript src=https://www.maths.nottingham.ac.uk/plp/pmadw/LaTeXMathML.js></script><meta name=robots content="index, follow"><title>Loss Functions in ML | KiloBytes by KB</title>
<meta name=keywords content="ML,machine-learning,AI,loss-functions"><meta name=description content="Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
All losses in keras defined here
Frequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more."><meta name=author content><link rel=canonical href=https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/><link crossorigin=anonymous href=/assets/css/stylesheet.4cb5d59298692facab5b270399a4560592b36cb58936d3c922114e90da87e031.css integrity="sha256-TLXVkphpL6yrWycDmaRWBZKzbLWJNtPJIhFOkNqH4DE=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://Kshitij-Banerjee.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://Kshitij-Banerjee.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://Kshitij-Banerjee.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://Kshitij-Banerjee.github.io/apple-touch-icon.png><link rel=mask-icon href=https://Kshitij-Banerjee.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta name=author content="Kshitij Banerjee"><meta name=description content="Kshitij Banerjee's notes..."><script type=application/ld+json>{"@context":"https://schema.org","@type":"Person","name":"Kshitij Banerjee","url":"https://kshitij-banerjee.github.io/"}</script><meta property="og:title" content="Loss Functions in ML"><meta property="og:description" content="Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
All losses in keras defined here
Frequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more."><meta property="og:type" content="article"><meta property="og:url" content="https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/"><meta property="og:image" content="https://Kshitij-Banerjee.github.io/image_1676730500910_0.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-18T00:00:00+00:00"><meta property="article:modified_time" content="2023-02-18T00:00:00+00:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://Kshitij-Banerjee.github.io/image_1676730500910_0.png"><meta name=twitter:title content="Loss Functions in ML"><meta name=twitter:description content="Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)
All losses in keras defined here
Frequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1
This means, that it penalises a low probability of success exponentially more."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://Kshitij-Banerjee.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Loss Functions in ML","item":"https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Loss Functions in ML","name":"Loss Functions in ML","description":"Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)\nAll losses in keras defined here\nFrequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1\nThis means, that it penalises a low probability of success exponentially more.","keywords":["ML","machine-learning","AI","loss-functions"],"articleBody":"Introduction Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)\nAll losses in keras defined here\nFrequently we see the loss function being expressed as a negative loss, why is that so? Plot: As probabilities only lie between [0-1], the plot is only relevant between X from 0-1\nThis means, that it penalises a low probability of success exponentially more.\nBut since we do LogLoss = - ( y * Log(p(y)) )\nIf the true label is 0, the effect of the log is ignored.\nOnly true labels contribute to the overall loss, and if for the true labels the P(y) value is low, then the loss magnitude is highly penalised\nWhat are logits? In context of deep learning the¬†logits layer means the layer that feeds in to softmax (or other such normalization). The output of the softmax are the probabilities for the classification task and its input is logits layer.\nThe logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.\nWhy do we make the loss functions take the logit values instead of the final classification labels?\nPushing the ‚Äúsoftmax‚Äù activation into the cross-entropy loss layer significantly simplifies the loss computation and makes it more numerically stable.\nFull derivation in this SO post\nBinary Cross Entropy Math:\nThis is just expanded math for using P(0) = { 1 - P(1) }, and becomes same as log loss\nThe hypthosis/P function used is typically sigmoid\nUsed for:\nWhen the output class is one of two values (binary) in nature.\nCode:\nTensor Flow Code\n# Compute cross entropy from probabilities. bce = target * tf.math.log(output + epsilon()) bce += (1 - target) * tf.math.log(1 - output + epsilon()) Categorical cross entropy Math:\nUsed for:\nIf your¬†ùëå vector values are one-hot encoded, use categorical_crossentropy.\nExamples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]\nTF Code Link\nSparse Categorical Cross Entropy Math (same as Categorical Cross Entropy)\nUsed for:\nInteger classes as output\nIntuitively, the sparse categorical just takes the index of the true-value to calculate the loss\nSo when model output is for example¬†[0.1, 0.3, 0.7]¬†and ground truth is¬†3¬†(if indexed from 1) then loss compute only logarithm of¬†0.7. This doesn‚Äôt change the final value, because in the regular version of categorical crossentropy other values are immediately multiplied by zero (because of one-hot encoding characteristic). Thanks to that it computes logarithm once per instance and omits the summation which leads to better performance. The formula might look like this:\n","wordCount":"446","inLanguage":"en","image":"https://Kshitij-Banerjee.github.io/image_1676730500910_0.png","datePublished":"2023-02-18T00:00:00Z","dateModified":"2023-02-18T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://Kshitij-Banerjee.github.io/2023/02/18/loss-functions-in-ml/"},"publisher":{"@type":"Organization","name":"KiloBytes by KB","logo":{"@type":"ImageObject","url":"https://Kshitij-Banerjee.github.io/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://Kshitij-Banerjee.github.io accesskey=h title="KiloBytes by KB (Alt + H)">KiloBytes by KB</a><div class=logo-switches></div></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=post-meta>&lt;span title='2023-02-18 00:00:00 +0000 UTC'>February 18, 2023&lt;/span></div></header><figure class=entry-cover><img loading=lazy src=https://Kshitij-Banerjee.github.io/image_1676730500910_0.png alt></figure><div class=post-content><h3 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h3><hr><p>Loss functions tell the algorithm how far we are from actual truth, and their gradients/derivates help understand how to reduce the overall loss (by changing the parameters being trained on)<br>All losses in keras defined <a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses>here</a></p><h4 id=frequently-we-see-the-loss-function-being-expressed-as-a-negative-loss-why-is-that-so>Frequently we see the loss function being expressed as a negative loss, why is that so?<a hidden class=anchor aria-hidden=true href=#frequently-we-see-the-loss-function-being-expressed-as-a-negative-loss-why-is-that-so>#</a></h4><p>Plot: <img loading=lazy src=/image_1676730500910_0.png alt=image.png></p><p>As probabilities only lie between [0-1], the plot is only relevant between X from 0-1</p><p>This means, that it penalises a low probability of success exponentially more.</p><p>But since we do LogLoss = - ( y * Log(p(y)) )</p><p>If the true label is 0, the effect of the log is ignored.</p><p>Only true labels contribute to the overall loss, and if for the true labels the P(y) value is low, then the loss magnitude is highly penalised</p><h4 id=what-are-logits>What are logits?<a hidden class=anchor aria-hidden=true href=#what-are-logits>#</a></h4><p>In context of deep learning the¬†logits layer means the layer that feeds in to softmax (or other such normalization).
The output of the softmax are the probabilities for the classification task and its input is logits layer.<br>The logits layer typically produces values from -infinity to +infinity and the softmax layer transforms it to values from 0 to 1.<br><strong>Why do we make the loss functions take the logit values instead of the final classification labels?</strong></p><p>Pushing the &ldquo;softmax&rdquo; activation into the cross-entropy loss layer significantly simplifies the loss computation and makes it more <strong>numerically stable.</strong></p><p>Full derivation in this <a href=https://stackoverflow.com/questions/34907657/scale-the-loss-value-according-to-badness-in-caffe/34917052#34917052>SO post</a></p><h4 id=binary-cross-entropyhttpswwwtensorfloworgapi_docspythontfkeraslossesbinarycrossentropy><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy>Binary Cross Entropy</a><a hidden class=anchor aria-hidden=true href=#binary-cross-entropyhttpswwwtensorfloworgapi_docspythontfkeraslossesbinarycrossentropy>#</a></h4><p><strong>Math:</strong></p><p><img loading=lazy src=/image_1676732588575_0.png alt=image.png></p><p>This is just expanded math for using P(0) = { 1 - P(1) }, and becomes same as log loss</p><p>The hypthosis/P function used is typically sigmoid</p><p><strong>Used for:</strong></p><p>When the output class is one of two values (binary) in nature.</p><p><strong>Code:</strong></p><p><a href=https://github.com/keras-team/keras/blob/e6784e4302c7b8cd116b74a784f4b78d60e83c26/keras/backend.py#L5553>Tensor Flow Code</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>		      <span style=color:#75715e># Compute cross entropy from probabilities.</span>
</span></span><span style=display:flex><span>		      bce <span style=color:#f92672>=</span> target <span style=color:#f92672>*</span> tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>log(output <span style=color:#f92672>+</span> epsilon())
</span></span><span style=display:flex><span>		      bce <span style=color:#f92672>+=</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> target) <span style=color:#f92672>*</span> tf<span style=color:#f92672>.</span>math<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> output <span style=color:#f92672>+</span> epsilon())
</span></span></code></pre></div><h4 id=categorical-cross-entropyhttpswwwtensorfloworgapi_docspythontfkeraslossescategoricalcrossentropy><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy>Categorical cross entropy</a><a hidden class=anchor aria-hidden=true href=#categorical-cross-entropyhttpswwwtensorfloworgapi_docspythontfkeraslossescategoricalcrossentropy>#</a></h4><p><strong>Math</strong>:</p><p><strong>Used for:</strong></p><p>If your¬†ùëå vector values are one-hot encoded, use categorical_crossentropy.</p><p>Examples (for a 3-class classification): [1,0,0] , [0,1,0], [0,0,1]</p><p><a href=https://github.com/keras-team/keras/blob/e6784e4302c7b8cd116b74a784f4b78d60e83c26/keras/backend.py#L5553>TF Code Link</a></p><h4 id=sparse-categorical-cross-entropyhttpswwwtensorfloworgapi_docspythontfkeraslossessparsecategoricalcrossentropy><a href=https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy>Sparse Categorical Cross Entropy</a><a hidden class=anchor aria-hidden=true href=#sparse-categorical-cross-entropyhttpswwwtensorfloworgapi_docspythontfkeraslossessparsecategoricalcrossentropy>#</a></h4><p><strong>Math</strong> (same as Categorical Cross Entropy)</p><p><strong>Used for:</strong></p><p>Integer classes as output</p><p>Intuitively, the sparse categorical just takes the index of the true-value to calculate the loss</p><p>So when model output is for example¬†<code>[0.1, 0.3, 0.7]</code>¬†and ground truth is¬†<code>3</code>¬†(if indexed from 1) then loss compute only logarithm of¬†<code>0.7</code>. This doesn&rsquo;t change the final value, because in the regular version of categorical crossentropy other values are immediately multiplied by zero (because of one-hot encoding characteristic). Thanks to that it computes logarithm once per instance and omits the summation which leads to better performance. The formula might look like this:</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://Kshitij-Banerjee.github.io/tags/ml/>ML</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/machine-learning/>machine-learning</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/ai/>AI</a></li><li><a href=https://Kshitij-Banerjee.github.io/tags/loss-functions/>loss-functions</a></li></ul></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//kshitij-banerjee.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2024 <a href=https://Kshitij-Banerjee.github.io>KiloBytes by KB</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>